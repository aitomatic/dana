# Test: AI Core Functions Integration
# Purpose: Comprehensive test of Dana's core AI functions including:
#   - llm() function for async LLM calls
#   - reason() function for sync LLM calls
#   - context_aware_reason() for enhanced reasoning
#   - set_model() for LLM configuration
#   - Mock and real LLM interactions
#   - Promise handling for async operations
#   - Error handling and validation
# Category: Integration - AI Core Functions

log("Starting AI Core Functions Integration test")

# Test setup - configuration
log("Setting up AI test configuration...")

test_mock_mode = false  # Set to false to test with real LLM
test_model = "gpt-4o-mini"  # Default test model
test_temperature = 0.3  # Low temperature for consistent testing

model = set_model(test_model)

log("✓ AI test configuration ready")

# Test 1: Basic LLM Function
log("Test 1: Basic LLM Function")

# Test basic llm() call with mock
basic_llm_result = llm("What is 2 + 2? Respond with just the number.", {
    "temperature": test_temperature,
    "max_tokens": 10,
}, test_mock_mode)

# Since llm() returns a Promise, we need to handle it properly
assert basic_llm_result is not null, "✗ LLM function should return a result"
assert type(basic_llm_result) == "str", "✗ Result should be string"
log("✓ Basic LLM function call working")

# Test 2: Reason Function with Type Hints
log("Test 2: Reason Function with Type Hints")

# Test integer type hint
number_result: int = reason("What is 2 + 2? Respond with just the number.", {
    "temperature": test_temperature,
    "max_tokens": 10,
}, test_mock_mode)
assert number_result is not null, "✗ Integer type hint should work"
assert type(number_result) == "int", "✗ Result should be integer"
log("✓ Integer type hint working")

# Test string type hint
text_result: str = reason("What is the capital of France? One word.", {
    "temperature": test_temperature,
    "max_tokens": 10,
}, test_mock_mode)
assert text_result is not null, "✗ String type hint should work"
assert type(text_result) == "str", "✗ Result should be string"
log("✓ String type hint working")

# # Test set type hint - [NOT WORK YET]
# unique_items: set[str] = reason("List 3 unique colors: red, blue, green", {
#     "temperature": test_temperature,
# }, test_mock_mode)
# assert unique_items is not null, "✗ Set type hint should work"
# assert type(unique_items) == "set", "✗ Result should be set"
# log("✓ Set type hint working")

# Test list type hint
items_list: list[str] = reason("List 3 fruits: apple, banana, orange", {
    "temperature": test_temperature,
}, test_mock_mode)
assert items_list is not null, "✗ List type hint should work"
assert type(items_list) == "list", "✗ Result should be list"
log("✓ List type hint working")

# Test dictionary type hint
config_dict: dict[str, str] = reason("Generate server config with host and port", {
    "temperature": test_temperature,
}, test_mock_mode)
assert config_dict is not null, "✗ Dictionary type hint should work"
assert type(config_dict) == "dict", "✗ Result should be dictionary"
log("✓ Dictionary type hint working")

# Test custom struct type hint
# Define a custom struct for testing
struct TestResult:
    score: float
    category: str
    tags: list[str]

analysis_result: TestResult = reason("Analyze text sentiment: 'Great product!'", {
    "temperature": test_temperature,
}, test_mock_mode)

assert analysis_result is not null, "✗ Custom struct type hint should work"
assert type(analysis_result) == "StructInstance", "✗ Result should be TestResult struct"
assert analysis_result.score is not null, "✗ Struct field 'score' should exist"
assert analysis_result.category is not null, "✗ Struct field 'category' should exist"
assert analysis_result.tags is not null, "✗ Struct field 'tags' should exist"
log("✓ Custom struct type hint working")

# # Test reasoning with context - [NOT WORK YET] Waiting for the POET framework implementation to be complete
# financial_analysis = reason("Calculate the ROI for this investment", {
#     "context": ["investment_data", "market_conditions"],
#     "temperature": test_temperature
# }, test_mock_mode)

# assert financial_analysis is not null, "✗ Reason with context should work"
# log("✓ Reason function with context working")

# # Test reasoning with domain-specific enhancement
# manufacturing_analysis = reason("Optimize this production line for efficiency", {
#     "domain": "manufacturing",
#     "temperature": test_temperature
# }, test_mock_mode)

# assert manufacturing_analysis is not null, "✗ Domain-specific reasoning should work"
# log("✓ Domain-specific reasoning working")

# Test 3: Context-Aware Reasoning
# Current State of reason() vs context_aware_reason(): No Functional Difference

log("Test 3: Context-Aware Reasoning")

# Test context-aware reasoning with environment data
context_data = {
    "user_preferences": {"analysis_depth": "detailed", "format": "structured"},
    "previous_queries": ["market analysis", "risk assessment"],
    "domain_knowledge": "financial_services"
}

context_aware_result = context_aware_reason(
    "Provide investment recommendations based on current market conditions",
    {
        "context": context_data,
        "temperature": test_temperature,
        "enhanced_context": true
    },
    test_mock_mode
)

assert context_aware_result is not null, "✗ Context-aware reasoning should return result"
log("✓ Context-aware reasoning working")

# Test 4: Model Configuration
log("Test 4: Model Configuration")

# Test setting LLM model
original_model = set_model(test_model)

print(f"original_model: {original_model}")

assert original_model is not null or original_model == null, "✗ set_model should handle model setting"
log("✓ Model configuration working")

# Test model-specific reasoning after configuration
model_specific_result = reason("Test with configured model", {
    "temperature": test_temperature
}, test_mock_mode)

assert model_specific_result is not null, "✗ Reasoning with configured model should work"
log("✓ Model-specific reasoning working")

# Test 5: AI Function Error Handling
log("Test 5: AI Function Error Handling")

# Test empty prompt error handling
empty_prompt_success = false
try:
    empty_result = reason("", {}, test_mock_mode)
    empty_prompt_success = true
except as e:
    empty_prompt_success = false

if empty_prompt_success:
    assert false, "✗ Empty prompt should cause error"
else:
    log("✓ Empty prompt properly rejected")

# # llm() Test invalid options handling - [NOT SUPPORTED YET] - cannot try/except the llm() error for now
# invalid_options_success = false
# try:
#     invalid_result = llm("Valid prompt", {
#         "temperature": 2.5,  # Invalid temperature > 2.0
#         "max_tokens": -100   # Invalid negative tokens
#     }, test_mock_mode)
#     invalid_options_success = true
# except as e:
#     invalid_options_success = false

# # Note: Some providers may handle invalid options gracefully
# if not invalid_options_success:
#     log("✓ Invalid options properly handled")
# else:
#     assert false, "✗ Invalid options should cause error and being caught"
#     log("⚠ Invalid options accepted (provider may handle gracefully)")

# # reason() Test invalid options handling - [NOT SUPPORTED YET] - cannot try/except the reason() error for now
# invalid_options_success = false
# try:
#     invalid_result = reason("Valid prompt", {
#         "temperature": 2.5,  # Invalid temperature > 2.0
#         "max_tokens": -100   # Invalid negative tokens
#     }, test_mock_mode)
#     invalid_options_success = true
# except as e:
#     invalid_options_success = false

# # Note: Some providers may handle invalid options gracefully
# if not invalid_options_success:
#     log("✓ Invalid options properly handled")
# else:
#     log("⚠ Invalid options accepted (provider may handle gracefully)")
#     assert false, "✗ Invalid options should cause error and being caught"

# Test 6: AI Function Performance and Concurrency
log("Test 6: AI Function Performance and Concurrency")

# Test multiple concurrent LLM calls (demonstrates Promise handling)
concurrent_results = []
for i in range(3):
    result = llm(f"Process request {i + 1}", {
        "temperature": test_temperature,
        "max_tokens": 20
    }, test_mock_mode)
    concurrent_results.append(result)

assert len(concurrent_results) == 3, "✗ All concurrent calls should be initiated"

# Test that all results are valid (Promise objects or resolved values)
for i, result in enumerate(concurrent_results):
    assert result is not null, f"✗ Concurrent call {i + 1} should have valid result"

log("✓ Concurrent LLM calls working")

# log("AI Core Functions Integration test completed successfully")

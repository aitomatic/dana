"""DXA (Domain-Expert Agent)

A hierarchical agent system that separates strategic planning from tactical execution through a 
clean two-layer architecture. The system emphasizes clear separation of concerns while maintaining 
adaptivity through a well-defined signaling system.

## Structure:

The system is built on two primary layers with distinct responsibilities:

The Planning Layer operates at a strategic level, maintaining the agent's overall direction and 
objectives. It owns the "big picture" - creating plans, evaluating progress, and making strategic 
decisions. This layer deliberately operates at a higher level of abstraction, focusing on what 
needs to be done rather than how to do it. It maintains working memory of recent history and 
context to inform its decisions, but delegates all tactical execution.

The Reasoning Layer handles tactical execution and monitoring. It owns the "how" of execution - 
taking the current plan step, deciding the best way to execute it, and managing the actual 
execution. It has significant autonomy in how it achieves each step, including handling minor 
variations and setbacks without escalation. However, it maintains clear boundaries by signaling 
the Planning Layer when strategic changes might be needed.

```mermaid
graph TB
    subgraph Planning Layer
        P[Plan Management] --> O[Objective Management]
        O --> S[Strategy Selection]
        P --> S
    end
    
    subgraph Reasoning Layer
        E[Execution] --> M[Monitoring]
        M --> A[Analysis]
        A --> E
    end
    
    Planning Layer -->|Plans & Objectives| Reasoning Layer
    Reasoning Layer -->|Signals & Status| Planning Layer
    
    subgraph Resources
        LLM[Language Models]
        DB[(Databases)]
        API[External APIs]
        MEM[Memory Store]
    end
    
    Reasoning Layer -.->|Uses| LLM
    Reasoning Layer -.->|Uses| DB
    Reasoning Layer -.->|Uses| API
    Reasoning Layer -.->|Uses| MEM
    
    Planning Layer -.->|Resource Awareness| Resources
```

The interaction between layers is managed through two primary mechanisms:

1. Plans flow down from Planning to Reasoning, providing clear direction while leaving tactical
decisions to the Reasoning Layer
2. Signals flow up from Reasoning to Planning, alerting to significant events or changes that
might require strategic adjustment

Resources (like language models, databases, or APIs) are primarily managed by the Reasoning Layer,
which has direct control over their usage. However, resource availability and constraints can
trigger signals to the Planning Layer when strategic adjustments are needed.

## Execution

The execution model emphasizes stability while maintaining adaptivity. Here's how the system
operates:

The Planning Layer initializes execution by creating a plan based on the current objective. Plans
are treated as living documents - expected to evolve but not constantly changing. The Planning
Layer sets clear success criteria and constraints but leaves tactical decisions to the Reasoning
Layer.

The Reasoning Layer executes through a continuous cycle:

1. Interpret the current plan step and determine best execution approach
2. Execute using available resources
3. Monitor progress and resource health
4. Generate signals when significant events occur

```mermaid
sequenceDiagram
    participant P as Planning Layer
    participant R as Reasoning Layer
    participant Res as Resources

    Note over P,R: Initialization Phase
    P->>R: Initial Plan & Objectives
    
    loop Execution Phase
        R->>Res: Execute Current Step
        Res-->>R: Results & State
        
        Note over R: Monitor & Analyze
        
        alt Normal Progress
            R->>R: Continue Execution
        else Significant Event
            R->>P: Signal Event
            Note over P: Strategic Evaluation
            alt Requires Strategy Change
                P->>R: Updated Plan/Objectives
            else Continue Current Strategy
                P->>R: Acknowledge & Continue
            end
        end
    end

```

Critical to this model is the signal system between layers. Signals are generated by the Reasoning
Layer when it detects:

1. Changes in objective understanding requiring refinement
2. Resource constraints or failures requiring strategic adjustment
3. Confidence drops suggesting current approach may not succeed
4. Critical new information that could affect strategy
5. Environmental changes impacting execution

The Planning Layer evaluates signals based on significance, not timing. Major discoveries or
failures trigger immediate replanning, while minor variations are handled by Reasoning Layer
flexibility. This creates stability while maintaining adaptivity to important changes.

## Configuration

The agent supports progressive complexity through a fluent builder pattern:

### Basic Usage

```python
agent = Agent("researcher")\
            .with_llm(LLMResource(model="gpt-4"))\
            .with_reasoning("cot")
```

### Advanced Usage

```python
agent = Agent("researcher")\
            .with_llm(LLMResource(...))\
            .with_reasoning_pipeline([
                ObjectiveAnalysisStage(),
                PlanValidationStage(),
                ExecutionStage()
            ])\
            .with_plan_modification_strategy(
                AdaptivePlanStrategy(
                    triggers=[ConfidenceThresholdTrigger(0.7)]
                )
            )
```

### Best Practices

1. Keep the Planning Layer focused on strategy - avoid pulling it into tactical decisions
2. Allow the Reasoning Layer appropriate autonomy in tactical execution
3. Set appropriate signal thresholds to avoid unnecessary plan changes
4. Maintain clear separation between strategic and tactical state
5. Monitor and adjust signal thresholds based on execution patterns
6. Implement robust error handling at both layers
7. Maintain audit trails of strategic decisions and their rationale
"""

from typing import Optional, Dict, Union, List, Any
from uuid import uuid4

from dxa.core.reasoning import BaseReasoning, ReasoningLevel
from dxa.core.resource import BaseResource
from dxa.core.io import BaseIO
from dxa.agent.agent_runtime import AgentRuntime, StateManager
from dxa.common.errors import ConfigurationError
from dxa.core.reasoning.direct_reasoning import DirectReasoning
from dxa.core.reasoning.cot_reasoning import ChainOfThoughtReasoning
from dxa.core.reasoning.ooda_reasoning import OODAReasoning
from dxa.core.reasoning.dana_reasoning import DANAReasoning
from dxa.core.reasoning.base_reasoning import ReasoningContext
from dxa.core.resource.llm_resource import LLMResource
class Agent:
    """Unified agent with progressive complexity."""
    
    def __init__(self, name: Optional[str] = None, llm: LLMResource = None):
        """Initialize Agent with required LLM.
        
        Args:
            name: Agent identifier
            llm: Core language model for reasoning
        """
        self.name = name or str(uuid4())[:8]
        self._llm_resource = llm or LLMResource(name="gpt4", config={"model_name": "gpt-4"})
        self._model = "gpt-4"
        self._mode = "autonomous"
        self._reasoning = None
        self._resources = {}
        self._capabilities = set()
        self._io = None
        self._state_manager = StateManager(self.name)
        self._runtime = AgentRuntime(self._state_manager)
        
    def with_reasoning(self, reasoning: Union[BaseReasoning, str, ReasoningLevel]) -> "Agent":
        """Set reasoning system.
        
        Args:
            reasoning: Either a BaseReasoning instance, string name, or ReasoningLevel
        """
        if isinstance(reasoning, BaseReasoning):
            self._reasoning = reasoning
        else:
            # Convert string/level to appropriate reasoning instance
            strategies = {
                "direct": DirectReasoning,
                "cot": ChainOfThoughtReasoning,
                "ooda": OODAReasoning,
                "dana": DANAReasoning,
                ReasoningLevel.DIRECT: DirectReasoning,
                ReasoningLevel.COT: ChainOfThoughtReasoning,
                ReasoningLevel.OODA: OODAReasoning
            }
            
            strategy_class = strategies.get(reasoning)
            if not strategy_class:
                raise ConfigurationError(f"Unknown reasoning strategy: {reasoning}")
            
            self._reasoning = strategy_class()
        
        # Set LLM resource on reasoning system
        self._reasoning.agent_llm = self.llm
        
        if self._resources:
            self._reasoning.set_available_resources(self._resources)
        return self
        
    def with_resources(self, resources: Dict[str, BaseResource]) -> "Agent":
        """Add resources."""
        self._resources.update(resources)
        if self._reasoning:
            self._reasoning.set_available_resources(self._resources)
        return self
    
    def with_capabilities(self, capabilities: List[str]) -> "Agent":
        """Add capabilities."""
        self._capabilities.update(capabilities)
        return self

    def with_io(self, io_handler: BaseIO) -> "Agent":
        """Set I/O handler."""
        self._io = io_handler
        return self

    @property
    def llm(self) -> LLMResource:
        """Get the agent's LLM resource."""
        return self._llm_resource

    @llm.setter
    def llm(self, llm: LLMResource):
        """Set the agent's LLM resource."""
        self._llm_resource = llm
        if self._reasoning:
            self._reasoning.agent_llm = llm

    async def pre_execute(self, context: ReasoningContext) -> None:
        """Setup before execution."""
        if not self._reasoning:
            raise ConfigurationError("Agent requires reasoning system")
            
        # Add capabilities to context
        context.workspace["capabilities"] = list(self._capabilities)
        
        # Add resources to context
        context.workspace["available_resources"] = list(self._resources.keys())

    async def post_execute(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Process after execution."""
        result.update({
            "agent_name": self.name,
            "capabilities": list(self._capabilities)
        })
        return result
        
    async def run(self, task: Union[str, Dict[str, Any]]) -> Any:
        """Execute task and return result."""
        if not self._reasoning:
            raise ConfigurationError("Agent requires reasoning system")
            
        context = ReasoningContext(
            objective=task if isinstance(task, str) else task.get('objective'),
            resources=self._resources,
            workspace={},
            history=[]
        )
        
        result = await self._runtime.execute(
            task,
            reasoning_step=self._reasoning.reason,
            pre_execute=self.pre_execute,
            post_execute=self.post_execute,
            context=context
        )
        
        return result

    async def cleanup(self) -> None:
        """Clean up agent resources and connections."""
        if self._reasoning:
            await self._reasoning.cleanup()
        
        for resource in self._resources.values():
            await resource.cleanup()
            
        if self._io:
            await self._io.cleanup()
            
        await self._runtime.cleanup()

    @property
    def reasoning(self) -> Optional[BaseReasoning]:
        """Get the agent's reasoning system."""
        return self._reasoning

    @property
    def resources(self) -> Dict[str, BaseResource]:
        """Get the agent's resources."""
        return self._resources.copy()  # Return a copy to prevent direct modification

    @property
    def capabilities(self) -> set[str]:
        """Get the agent's capabilities."""
        return self._capabilities.copy()  # Return a copy to prevent direct modification
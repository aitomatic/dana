#!/bin/bash
# Install vLLM for macOS (Apple Silicon and Intel)
# Copyright ¬© 2025 Aitomatic, Inc. Licensed under the MIT License.
# Usage: ./bin/vllm/install.sh [--env-name ENV_NAME]

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Default environment name
ENV_NAME="vllm_env"

# Check for custom environment name
if [[ "$1" == "--env-name" && -n "$2" ]]; then
    ENV_NAME="$2"
fi

echo -e "${BLUE}üöÄ Installing vLLM for macOS...${NC}"
echo -e "${YELLOW}‚ö†Ô∏è  Note: vLLM on macOS runs CPU-only (no GPU acceleration yet)${NC}"
echo ""

# Check macOS version
MACOS_VERSION=$(sw_vers -productVersion)
MAJOR_VERSION=$(echo "$MACOS_VERSION" | cut -d '.' -f 1)
MINOR_VERSION=$(echo "$MACOS_VERSION" | cut -d '.' -f 2)

echo -e "${BLUE}üñ•Ô∏è  macOS Version: ${MACOS_VERSION}${NC}"

if [[ "$MAJOR_VERSION" -lt 14 || ("$MAJOR_VERSION" -eq 14 && "$MINOR_VERSION" -lt 4) ]]; then
    echo -e "${RED}‚ùå Error: macOS Sonoma (14.4) or later is required for vLLM${NC}"
    echo -e "${YELLOW}üí° Current version: ${MACOS_VERSION}, Required: 14.4+${NC}"
    exit 1
fi

# Check if Python 3 is available
if ! command -v python3 &> /dev/null; then
    echo -e "${RED}‚ùå Error: Python 3 is not installed${NC}"
    echo -e "${YELLOW}üí° Please install Python 3.8+ (Python 3.10+ recommended):${NC}"
    echo "   - Download from: https://www.python.org/downloads/"
    echo "   - Or install via brew: brew install python"
    exit 1
fi

PYTHON_VERSION=$(python3 -c "import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')")
echo -e "${BLUE}üêç Python Version: ${PYTHON_VERSION}${NC}"

# Check Python version
PYTHON_MAJOR=$(echo "$PYTHON_VERSION" | cut -d '.' -f 1)
PYTHON_MINOR=$(echo "$PYTHON_VERSION" | cut -d '.' -f 2)

if [[ "$PYTHON_MAJOR" -lt 3 || ("$PYTHON_MAJOR" -eq 3 && "$PYTHON_MINOR" -lt 8) ]]; then
    echo -e "${RED}‚ùå Error: Python 3.8+ is required for vLLM${NC}"
    echo -e "${YELLOW}üí° Current version: ${PYTHON_VERSION}, Required: 3.8+${NC}"
    exit 1
fi

# Check if Xcode Command Line Tools are installed
echo -e "${BLUE}üîß Checking Xcode Command Line Tools...${NC}"
if ! xcode-select -p &> /dev/null; then
    echo -e "${YELLOW}üì¶ Installing Xcode Command Line Tools...${NC}"
    xcode-select --install
    echo -e "${YELLOW}‚è≥ Please complete the Xcode Command Line Tools installation and run this script again${NC}"
    exit 0
fi

# Check Xcode version
if command -v xcodebuild &> /dev/null; then
    XCODE_VERSION=$(xcodebuild -version | head -n1 | cut -d ' ' -f2)
    echo -e "${BLUE}üõ†Ô∏è  Xcode Version: ${XCODE_VERSION}${NC}"
    
    # Check if Xcode version is 15.4+
    XCODE_MAJOR=$(echo "$XCODE_VERSION" | cut -d '.' -f 1)
    XCODE_MINOR=$(echo "$XCODE_VERSION" | cut -d '.' -f 2)
    
    if [[ "$XCODE_MAJOR" -lt 15 || ("$XCODE_MAJOR" -eq 15 && "$XCODE_MINOR" -lt 4) ]]; then
        echo -e "${YELLOW}‚ö†Ô∏è  Warning: Xcode 15.4+ is recommended for vLLM${NC}"
        echo -e "${YELLOW}   Current version: ${XCODE_VERSION}, Recommended: 15.4+${NC}"
        echo -e "${YELLOW}   Please update Xcode from the App Store if you encounter build issues${NC}"
    fi
fi

# Check if virtual environment already exists
VENV_PATH="$HOME/${ENV_NAME}"
if [[ -d "$VENV_PATH" ]]; then
    echo -e "${YELLOW}‚ö†Ô∏è  Virtual environment '${ENV_NAME}' already exists at ${VENV_PATH}${NC}"
    read -p "Do you want to remove it and create a new one? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        echo -e "${BLUE}üóëÔ∏è  Removing existing environment...${NC}"
        rm -rf "$VENV_PATH"
    else
        echo -e "${YELLOW}üí° You can specify a different environment name with: --env-name MY_ENV${NC}"
        exit 0
    fi
fi

# Create virtual environment
echo -e "${BLUE}üêç Creating Python virtual environment: ${ENV_NAME}${NC}"
python3 -m venv "$VENV_PATH"

# Activate virtual environment
echo -e "${BLUE}üîå Activating virtual environment...${NC}"
source "$VENV_PATH/bin/activate"

# Upgrade pip, setuptools, and wheel
echo -e "${BLUE}üì¶ Upgrading pip, setuptools, and wheel...${NC}"
pip install --upgrade pip setuptools wheel

# Clone vLLM repository
VLLM_DIR="$HOME/vllm"
STABLE_VERSION="v0.9.1"

if [[ -d "$VLLM_DIR" ]]; then
    echo -e "${YELLOW}‚ö†Ô∏è  vLLM repository already exists at ${VLLM_DIR}${NC}"
    read -p "Do you want to update it? (Y/n): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Nn]$ ]]; then
        echo -e "${BLUE}üì• Updating vLLM repository...${NC}"
        cd "$VLLM_DIR"
        git fetch --all --tags
        git pull origin main
    else
        cd "$VLLM_DIR"
    fi
else
    echo -e "${BLUE}üì• Cloning vLLM repository...${NC}"
    git clone https://github.com/vllm-project/vllm.git "$VLLM_DIR"
    cd "$VLLM_DIR"
fi

# Switch to stable version for better reliability
echo -e "${BLUE}üè∑Ô∏è  Switching to stable version ${STABLE_VERSION}...${NC}"
echo -e "${YELLOW}üí° Using stable version instead of main branch for better reliability${NC}"
git checkout "$STABLE_VERSION"

# Install CPU requirements
echo -e "${BLUE}üì¶ Installing CPU requirements...${NC}"
if [[ -f "requirements-cpu.txt" ]]; then
    pip install -r requirements-cpu.txt
else
    echo -e "${YELLOW}‚ö†Ô∏è  requirements-cpu.txt not found, installing basic requirements...${NC}"
    pip install torch torchvision torchaudio
fi

# Install vLLM in editable mode
echo -e "${BLUE}üî® Building and installing vLLM (this may take several minutes)...${NC}"
VLLM_TARGET_DEVICE=cpu pip install -e .

# Verify installation
echo -e "${BLUE}‚úÖ Verifying vLLM installation...${NC}"
if python -c "import vllm; print(f'vLLM version: {vllm.__version__}')" 2>/dev/null; then
    echo -e "${GREEN}üéâ vLLM successfully installed!${NC}"
else
    echo -e "${RED}‚ùå Error: vLLM installation verification failed${NC}"
    exit 1
fi

# Create convenience start script
echo -e "${BLUE}üìù Creating convenience start script...${NC}"
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$(dirname "$SCRIPT_DIR")")"
START_SCRIPT="$PROJECT_ROOT/bin/start_vllm.sh"

cat > "$START_SCRIPT" << 'EOF'
#!/bin/bash
# Start vLLM Server for OpenDXA
# Copyright ¬© 2025 Aitomatic, Inc. Licensed under the MIT License.
# Usage: ./bin/start_vllm.sh [OPTIONS]

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Default configuration
DEFAULT_MODEL="facebook/opt-125m"
DEFAULT_HOST="localhost"
DEFAULT_PORT="8000"
DEFAULT_ENV_NAME="ENV_NAME_PLACEHOLDER"

# Model selection variables
MODEL_SELECTED=""
INTERACTIVE_MODE=true

# Parse command line arguments first to check if model was specified
for arg in "$@"; do
    if [[ "$arg" == "--model" ]]; then
        INTERACTIVE_MODE=false
        break
    fi
done

show_help() {
    echo "Start vLLM Server for OpenDXA"
    echo ""
    echo "Usage: $0 [OPTIONS]"
    echo ""
    echo "Options:"
    echo "  --model MODEL        Model to serve (default: interactive selection)"
    echo "  --host HOST          Host to bind to (default: $DEFAULT_HOST)"
    echo "  --port PORT          Port to listen on (default: $DEFAULT_PORT)"
    echo "  --env-name NAME      vLLM environment name (default: $DEFAULT_ENV_NAME)"
    echo "  --help, -h           Show this help message"
    echo ""
    echo "Interactive Mode:"
    echo "  If no --model is specified, an interactive menu will appear"
    echo "  with curated model recommendations for different use cases."
    echo ""
    echo "Examples:"
    echo "  $0                                    # Interactive model selection"
    echo "  $0 --model microsoft/Phi-4           # Direct model specification"
    echo "  $0 --port 8080                       # Interactive + custom port"
    echo ""
    echo "Environment:"
    echo "  The script will look for vLLM installation at: ~/\$ENV_NAME/"
    echo "  Default: ~/vllm_env/"
}

show_model_menu() {
    clear
    echo -e "${BLUE}ü§ñ vLLM Model Selection for OpenDXA${NC}"
    echo -e "${BLUE}‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê${NC}"
    echo ""
    echo "Select a model category optimized for your hardware and use case:"
    echo ""
    echo -e "${GREEN}üì± macOS/CPU Optimized Models${NC}"
    echo "   1) Phi-3.5-mini (3.8B) - Microsoft's efficient model"
    echo "   2) Qwen2.5-3B - Alibaba's compact powerhouse"
    echo "   3) Gemma-2-2B - Google's lightweight model" 
    echo "   4) Llama-3.2-3B - Meta's small but capable"
    echo ""
    echo -e "${PURPLE}üöÄ GPU High-Performance Models${NC}"
    echo "   5) Qwen2.5-7B-Instruct - Excellent balance (14GB VRAM)"
    echo "   6) Llama-3.1-8B-Instruct - Meta's solid choice (16GB VRAM)"
    echo "   7) Phi-4 (15B) - Microsoft's latest (30GB VRAM)"
    echo "   8) Mistral-7B-Instruct - Fast and efficient (14GB VRAM)"
    echo ""
    echo -e "${CYAN}üíª Coding Specialized Models${NC}"
    echo "   9) microsoft/Phi-4 - Excellent at code generation"
    echo "  10) bigcode/starcoder2-7b - Dedicated coding model"
    echo "  11) deepseek-ai/deepseek-coder-6.7b-instruct - Code specialist"
    echo ""
    echo -e "${YELLOW}üñºÔ∏è  Vision/Multimodal Models${NC}"
    echo "  12) meta-llama/Llama-4-Scout-17B-16E-Instruct - Latest multimodal"
    echo "  13) microsoft/Phi-3.5-vision-instruct - Compact vision model"
    echo "  14) Qwen/Qwen2-VL-7B-Instruct - Excellent vision understanding"
    echo ""
    echo -e "${GREEN}üéØ Popular Recommended Models${NC}"
    echo "  15) Qwen/Qwen2.5-7B-Instruct - Best overall balance"
    echo "  16) microsoft/Phi-4 - Latest and efficient"
    echo "  17) meta-llama/Llama-3.1-8B-Instruct - Reliable choice"
    echo ""
    echo -e "${BLUE}‚öôÔ∏è  Custom Options${NC}"
    echo "  18) Enter custom model name"
    echo "  19) Use default (facebook/opt-125m - testing only)"
    echo ""
    echo -e "${RED}‚ùå Exit${NC}"
    echo "   0) Exit without starting"
    echo ""
}

get_model_selection() {
    while true; do
        show_model_menu
        echo -ne "${CYAN}Please select a model (0-19): ${NC}"
        read -r choice
        
        case $choice in
            1) MODEL_SELECTED="microsoft/Phi-3.5-mini-instruct"; break ;;
            2) MODEL_SELECTED="Qwen/Qwen2.5-3B-Instruct"; break ;;
            3) MODEL_SELECTED="google/gemma-2-2b-it"; break ;;
            4) MODEL_SELECTED="meta-llama/Llama-3.2-3B-Instruct"; break ;;
            5) MODEL_SELECTED="Qwen/Qwen2.5-7B-Instruct"; break ;;
            6) MODEL_SELECTED="meta-llama/Llama-3.1-8B-Instruct"; break ;;
            7) MODEL_SELECTED="microsoft/Phi-4"; break ;;
            8) MODEL_SELECTED="mistralai/Mistral-7B-Instruct-v0.3"; break ;;
            9) MODEL_SELECTED="microsoft/Phi-4"; break ;;
            10) MODEL_SELECTED="bigcode/starcoder2-7b"; break ;;
            11) MODEL_SELECTED="deepseek-ai/deepseek-coder-6.7b-instruct"; break ;;
            12) MODEL_SELECTED="meta-llama/Llama-4-Scout-17B-16E-Instruct"; break ;;
            13) MODEL_SELECTED="microsoft/Phi-3.5-vision-instruct"; break ;;
            14) MODEL_SELECTED="Qwen/Qwen2-VL-7B-Instruct"; break ;;
            15) MODEL_SELECTED="Qwen/Qwen2.5-7B-Instruct"; break ;;
            16) MODEL_SELECTED="microsoft/Phi-4"; break ;;
            17) MODEL_SELECTED="meta-llama/Llama-3.1-8B-Instruct"; break ;;
            18) 
                echo -ne "${CYAN}Enter custom model name (e.g., microsoft/Phi-4): ${NC}"
                read -r custom_model
                if [[ -n "$custom_model" ]]; then
                    MODEL_SELECTED="$custom_model"
                    break
                else
                    echo -e "${RED}‚ùå Please enter a valid model name${NC}"
                    sleep 2
                fi
                ;;
            19) MODEL_SELECTED="$DEFAULT_MODEL"; break ;;
            0) 
                echo -e "${YELLOW}üëã Goodbye!${NC}"
                exit 0 
                ;;
            *)
                echo -e "${RED}‚ùå Invalid selection. Please choose 0-19.${NC}"
                sleep 2
                ;;
        esac
    done
    
    echo ""
    echo -e "${GREEN}‚úÖ Selected model: $MODEL_SELECTED${NC}"
    echo ""
}

# Parse command line arguments
MODEL="$DEFAULT_MODEL"
HOST="$DEFAULT_HOST"
PORT="$DEFAULT_PORT"
ENV_NAME="$DEFAULT_ENV_NAME"

while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL="$2"
            shift 2
            ;;
        --host)
            HOST="$2"
            shift 2
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        --env-name)
            ENV_NAME="$2"
            shift 2
            ;;
        --help|-h)
            show_help
            exit 0
            ;;
        *)
            echo -e "${RED}‚ùå Unknown option: $1${NC}"
            echo "Use --help for usage information"
            exit 1
            ;;
    esac
done

# Show interactive model selection if no model specified
if [[ "$INTERACTIVE_MODE" == true ]]; then
    get_model_selection
    MODEL="$MODEL_SELECTED"
fi

echo -e "${BLUE}üöÄ Starting vLLM Server for OpenDXA...${NC}"
echo -e "${BLUE}üìã Configuration:${NC}"
echo "  ‚Ä¢ Model: $MODEL"
echo "  ‚Ä¢ Host: $HOST"
echo "  ‚Ä¢ Port: $PORT"
echo "  ‚Ä¢ Environment: $ENV_NAME"
echo ""

# Hardware recommendations
case $MODEL in
    *"Phi-3.5-mini"*|*"gemma-2-2b"*|*"Qwen2.5-3B"*|*"Llama-3.2-3B"*)
        echo -e "${GREEN}üí° This model is optimized for CPU/macOS and lower memory systems${NC}"
        ;;
    *"Phi-4"*|*"Qwen2.5-7B"*|*"Mistral-7B"*|*"Llama-3.1-8B"*)
        echo -e "${PURPLE}üí° This model works best with GPU (14-30GB VRAM recommended)${NC}"
        ;;
    *"vision"*|*"VL"*|*"Scout"*)
        echo -e "${YELLOW}üí° This is a multimodal model - supports both text and images${NC}"
        ;;
    *"coder"*|*"starcoder"*)
        echo -e "${CYAN}üí° This model is specialized for code generation and programming${NC}"
        ;;
esac

# Check if vLLM environment exists
VENV_PATH="$HOME/$ENV_NAME"
if [[ ! -d "$VENV_PATH" ]]; then
    echo -e "${RED}‚ùå Error: vLLM environment not found at $VENV_PATH${NC}"
    echo -e "${YELLOW}üí° Please install vLLM first:${NC}"
    echo "   ./bin/vllm/install.sh"
    if [[ "$ENV_NAME" != "$DEFAULT_ENV_NAME" ]]; then
        echo "   ./bin/vllm/install.sh --env-name $ENV_NAME"
    fi
    exit 1
fi

# Check if activation script exists
ACTIVATE_SCRIPT="$VENV_PATH/bin/activate"
if [[ ! -f "$ACTIVATE_SCRIPT" ]]; then
    echo -e "${RED}‚ùå Error: vLLM activation script not found at $ACTIVATE_SCRIPT${NC}"
    echo -e "${YELLOW}üí° The vLLM environment may be corrupted. Try reinstalling:${NC}"
    echo "   ./bin/vllm/install.sh"
    exit 1
fi

echo -e "${BLUE}üîå Activating vLLM environment...${NC}"
source "$ACTIVATE_SCRIPT"

# Verify vLLM is available
if ! python -c "import vllm" 2>/dev/null; then
    echo -e "${RED}‚ùå Error: vLLM not found in environment${NC}"
    echo -e "${YELLOW}üí° Try reinstalling vLLM:${NC}"
    echo "   ./bin/vllm/install.sh"
    exit 1
fi

# Check if port is already in use
if command -v lsof >/dev/null && lsof -i ":$PORT" >/dev/null 2>&1; then
    echo -e "${YELLOW}‚ö†Ô∏è  Warning: Port $PORT is already in use${NC}"
    echo -e "${YELLOW}üí° You may want to use a different port with --port XXXX${NC}"
    read -p "Continue anyway? (y/N): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 0
    fi
fi

echo -e "${GREEN}üåê Starting vLLM OpenAI-compatible API server...${NC}"
echo -e "${BLUE}üí° Server will be available at: http://$HOST:$PORT${NC}"
echo -e "${YELLOW}üìñ API docs will be at: http://$HOST:$PORT/docs${NC}"
echo -e "${YELLOW}üõë Press Ctrl+C to stop the server${NC}"
echo ""

# Adjust parameters based on model type
EXTRA_ARGS=""
if [[ "$MODEL" == *"vision"* || "$MODEL" == *"VL"* || "$MODEL" == *"Scout"* ]]; then
    EXTRA_ARGS="--limit-mm-per-prompt '{\"image\": 5}'"
fi

# Start vLLM server with model-specific optimizations
exec python -m vllm.entrypoints.openai.api_server \
    --model "$MODEL" \
    --host "$HOST" \
    --port "$PORT" \
    --dtype float16 \
    --max-model-len 2048 \
    --disable-frontend-multiprocessing \
    $EXTRA_ARGS
EOF

# Replace placeholder with actual environment name
sed -i.bak "s/ENV_NAME_PLACEHOLDER/$ENV_NAME/g" "$START_SCRIPT"
rm -f "$START_SCRIPT.bak"

# Make the start script executable
chmod +x "$START_SCRIPT"
echo -e "${GREEN}‚úÖ Created start script: $START_SCRIPT${NC}"
echo -e "${BLUE}   Environment: $ENV_NAME${NC}"

echo ""
echo -e "${GREEN}üéâ vLLM installation completed successfully!${NC}"
echo ""
echo -e "${YELLOW}üìù Next steps:${NC}"
echo "1. Start vLLM server with interactive model selection (recommended):"
echo "   ./bin/start_vllm.sh"
echo "   ‚≠ê Choose from 19 curated models optimized for different use cases!"
echo ""
echo "2. Or start with direct model specification:"
echo "   ./bin/start_vllm.sh --model microsoft/Phi-4 --port 8080"
echo ""
echo "3. Recommended models for your system:"
echo "   ‚Ä¢ For macOS/CPU: microsoft/Phi-3.5-mini-instruct (fast & efficient)"
echo "   ‚Ä¢ For coding: microsoft/Phi-4 or deepseek-ai/deepseek-coder-6.7b-instruct"
echo "   ‚Ä¢ For vision: microsoft/Phi-3.5-vision-instruct (images + text)"
echo ""
echo "4. Test vLLM with a simple example:"
echo "   source $VENV_PATH/bin/activate"
echo "   python -c \"from vllm import LLM; print('vLLM is ready!')\""
echo ""
echo "5. Manual activation (if needed):"
echo "   source $VENV_PATH/bin/activate"
echo ""
echo -e "${BLUE}üí° Important Notes:${NC}"
echo "‚Ä¢ vLLM on macOS runs CPU-only (no GPU acceleration)"
echo "‚Ä¢ Only FP32 and FP16 data types are supported"
echo "‚Ä¢ Performance will be slower than CUDA-enabled systems"
echo "‚Ä¢ Installed stable version ${STABLE_VERSION} for better reliability"
echo "‚Ä¢ Multiprocessing disabled to prevent import errors on macOS"
echo "‚Ä¢ Consider alternatives like MLX or llama.cpp for Apple Silicon GPU acceleration"
echo ""
echo -e "${YELLOW}üìö For more information:${NC}"
echo "‚Ä¢ vLLM Documentation: https://docs.vllm.ai/"
echo "‚Ä¢ macOS Installation Guide: https://docs.vllm.ai/en/latest/getting_started/installation/cpu-apple.html"
echo ""
echo -e "${BLUE}üóÇÔ∏è  Installation Details:${NC}"
echo "‚Ä¢ Virtual Environment: $VENV_PATH"
echo "‚Ä¢ vLLM Source: $VLLM_DIR"
echo "‚Ä¢ vLLM Version: $STABLE_VERSION (stable)"
echo "‚Ä¢ Start Script: ./bin/start_vllm.sh"
echo "‚Ä¢ Python Version: $PYTHON_VERSION"
echo "‚Ä¢ macOS Version: $MACOS_VERSION" 
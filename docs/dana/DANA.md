<p align="center">
  <img src="https://cdn.prod.website-files.com/62a10970901ba826988ed5aa/62d942adcae82825089dabdb_aitomatic-logo-black.png" alt="Aitomatic Logo" width="400" style="border: 2px solid #666; border-radius: 10px; padding: 20px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);"/>
</p>

# DANA (Domain-Aware NeuroSymbolic Architecture)

## üß≠ Vision

DANA is a universal program format and execution runtime that enables intelligent agents ‚Äî human or machine ‚Äî to reason, act, and collaborate through structured, interpretable programs.

It serves as the missing link between natural language objectives and tool-assisted, stateful action. DANA programs are concise, auditable, explainable, and can be authored by LLMs, domain experts, or both.

---

## üí° Motivation & Problem

Modern AI systems struggle with:

* ‚úñÔ∏è **Prompt chains are fragile** ‚Äî hard to debug, hard to maintain
* ‚úñÔ∏è **Plans are opaque** ‚Äî impossible to inspect or explain mid-flight
* ‚úñÔ∏è **Tool use is scattered** ‚Äî logic is buried in code, not declarative programs
* ‚úñÔ∏è **State is implicit** ‚Äî no shared memory model or traceable updates

Symbolic systems offer structure but lack adaptability. LLMs offer creativity but lack transparency. DANA bridges the two.

---

## ‚úÖ Solution

DANA introduces a lightweight domain-aware program language and runtime. It allows:

* üß† **Programs as first-class reasoning artifacts**
* üì¶ **Shared state containers** (`agent`, `world`, `temp`, `execution`)
* üß© **Reusable logic units** via a structured Knowledge Base (KB)
* üßæ **Declarative goals**, **imperative execution**
* üìú **Bidirectional mapping to/from natural language**

DANA can:

* Be generated by a planning agent (like GMA)
* Be executed line-by-line by a runtime
* Interact with tools, LLMs, and memory
* Be stored, versioned, tested, and explained

---

## üîÑ Architecture Overview

### Emitters and Interpreters of DANA

| Actor             | Type               | Role(s) in DANA            | Description                                                        |
| ----------------- | ------------------ | -------------------------- | ------------------------------------------------------------------ |
| **User (Human)**  | Person             | üñã Emitter                 | Writes DANA directly to define goals, logic, or KB entries         |
| **GMA**           | Agent              | üñã Emitter                 | General planner that emits DANA plans from objectives              |
| **DXA**           | Domain Agent       | üñã Emitter                 | Emits specialized domain logic/workflows, often tied to KB content |
| **KB Maintainer** | Person or Agent    | üñã Emitter                 | Curates reusable DANA programs as structured knowledge             |
| **Tool Resource** | System Component   | ‚úÖ Interpreter              | Executes atomic tool-backed actions referenced in DANA             |
| **Local Runtime** | System Component   | ‚úÖ Interpreter              | Executes DANA deterministically except for `reason(...)`           |
| **DANA\_LLM**     | LLM Wrapper Module | üñã Emitter + ‚úÖ Interpreter | Emits code and executes reasoning operations                       |
| **AgentRuntime**  | System Component   | üîÅ Coordinator             | Orchestrates execution and manages delegation across all actors    |

### State Access

DANA programs operate over a shared `RuntimeContext`, which is composed of multiple memory scopes (state containers). These include:

| Scope        | Description                                |
| ------------ | ------------------------------------------ |
| `agent:`     | Long-term agent state (identity, traits)   |
| `world:`     | External observations and sensed data      |
| `temp:`      | Ephemeral memory ‚Äî scoped to a single step |
| `stmem:`     | Short-term memory ‚Äî scoped to a session    |
| `ltmem:`     | Long-term memory ‚Äî persisted over time     |
| `execution:` | Execution trace, status, and control flags |
| *custom:*    | User-defined namespaces for workflows      |

All components act on this shared context, with `AgentRuntime` controlling access and mutation privileges.

| Actor / Component | Access Type                       | Notes                                 |
| ----------------- | --------------------------------- | ------------------------------------- |
| **AgentRuntime**  | üîß Read/Write (Full)              | Owner and coordinator                 |
| **LocalRuntime**  | üß≠ Read/Write (Scoped)            | Executes DANA line-by-line safely     |
| **DANA\_LLM**     | üîç Read + Returns Deltas          | Receives snapshot, returns updates    |
| **Tools / DXAs**  | üîç Read + Write (Scoped)          | Modify state via specific APIs only   |
| **GMA / Planner** | üîç Read + Write (temp, execution) | Used for reasoning and planning       |
| **User (Human)**  | üßæ Read / Input                   | May initialize or query state         |
| **KB Entries**    | üì¶ Read/Write via Program         | Executed in context like a subroutine |

All components act on a shared mutable `RuntimeContext` that AgentRuntime owns and mediates.

### Security Design

**The `dana.runtime` module is the primary enforcer of security during the execution of a DANA program.** It manages the `RuntimeContext` for the duration of the execution and enforces access policies based on the defined scopes and program logic. While external components like `AgentRuntime` handle broader agent orchestration and security, the `dana.runtime` ensures safe state manipulation *within* a DANA execution context.

| Security Concern            | Enforced By              | Mechanism                                      |
| --------------------------- | ------------------------ | ---------------------------------------------- |
| Scope restrictions          | `dana.runtime`           | Limits visible/writable scopes during execution |
| State mutation permissions  | `dana.runtime`           | Controlled state updates via instructions      |
| Tool/API usage validation   | `dana.runtime`           | Ensures `use` targets are valid/allowed      |
| LLM output handling         | `dana.runtime`           | Manages `reason` call results and potential errors |
| Execution logging           | `dana.runtime`           | Tracks execution steps within the DANA context |
| Resource constraints        | `dana.runtime`           | (Optional) Limits execution steps, time, memory |

This ensures that DANA programs execute within defined boundaries, preventing unintended side effects and maintaining state integrity during their run.

---

## üí° Motivating Examples

### ‚úÖ Scenario: Multi-Step Risk Assessment via Prompt Chaining

**Today (Prompt-Chained Implementation):**

```python
# Step 1: Get supplier profile summary
step1 = llm(f"Summarize this supplier's profile:
{supplier_data}")

# Step 2: Check financial stability
step2 = llm(f"Based on this summary, assess financial risk:
{step1}")

# Step 3: Evaluate legal exposure
step3 = llm(f"Given this financial assessment, assess legal risk:
{step2}")

# Step 4: Final judgment
final_decision = llm(f"Considering all prior information, would you approve this supplier?")
```

* ‚úñÔ∏è Fragile logic and prompt phrasing
* ‚úñÔ∏è No explicit state or shared memory
* ‚úñÔ∏è Difficult to test, debug, or reuse

**With DANA:**

```python
temp.supplier = {}
temp.supplier.summary = reason("Summarize this supplier's profile", context=temp.supplier)
temp.supplier.financial_risk = reason("Assess financial risk", context=temp.supplier)
temp.supplier.legal_risk = reason("Assess legal risk", context=temp.supplier)

temp.final_decision = reason("Would you approve this supplier?", context=temp.supplier)
agent.supplier_approval = temp.final_decision
```

* ‚úÖ Structured, auditable state
* ‚úÖ Prompts tied to declarative reasoning steps
* ‚úÖ All context preserved and reusable

---

### ‚úÖ Example Workflow: Predictive Maintenance for Industrial Pump

**User Objective:** Detect anomalies and escalate for review if vibration levels exceed safe thresholds.

**Today (Typical Implementation):**

```python
if get_sensor("vibration") > 80:
    log("Anomaly detected")
    notify_team("Maintenance alert")
    if model.predict_failure():
        escalate("Pump requires urgent check")
```

* ‚úñÔ∏è Direct API calls are hardcoded
* ‚úñÔ∏è No structured state or reasoning log
* ‚úñÔ∏è Business logic is not reusable or inspectable

**With DANA:**

```python
if world.pump.vibration_level > 80:
    execution.alerts.append("Anomaly detected")
    use("kb.notify.maintenance_team")
    temp.severity = reason("Does this require immediate escalation?")
    if temp.severity == "yes":
        use("kb.escalate.critical_check")
```

* ‚úÖ Structured shared memory and alert tracking
* ‚úÖ Declarative use of reusable KB workflows
* ‚úÖ Domain-specific judgment via `reason(...)`

DANA simplifies common patterns found across industrial systems. Below, each example starts with how this is typically done today (often in a fragile or opaque way), followed by how the same scenario is clearer and safer using DANA.

---

### ‚úÖ Scenario 1: Eligibility Evaluation with LLM Reasoning

**Today (Typical Implementation):**

```python
prompt = f"User credit score is {score}. Should we deny?"
result = llm(prompt)
if 'yes' in result:
    decision = "deny"
else:
    decision = "approve"
```

* ‚úñÔ∏è Logic is entangled in prompt templates
* ‚úñÔ∏è No clear state tracking or modularity

**With DANA:**

```python
if world.customer.credit_score < 600:
    temp.eligibility = reason("Does this score disqualify the applicant?", context=world.customer)
else:
    temp.eligibility = "likely"

if temp.eligibility == "yes":
    agent.decision = "deny"
else:
    agent.decision = "approve"
```

* ‚úÖ Uses shared memory and LLM only where needed
* ‚úÖ Structured, inspectable, and reusable

---

### ‚úÖ Scenario 2: Tool Escalation from Failure Detection

**Today (Typical Implementation):**

```python
if detect_failure(sensor):
    log("dispatching alert")
    call_dispatch_service(machine_id)
```

* ‚úñÔ∏è Hardcoded logic and tool use
* ‚úñÔ∏è Not easily testable or explainable

**With DANA:**

```python
if "sensor_fail" in world.machine.flags:
    execution.alerts.append("Failure detected")
    use("kb.maintenance.workflow.dispatch")
```

* ‚úÖ Declarative condition + modular tool logic
* ‚úÖ All traceable in shared execution state

---

### ‚úÖ Scenario 3: Session-Based Memory for Behavioral Risk

**Today (Typical Implementation):**

```python
if redis.get("login_attempts") > 3:
    flag = llm("Is this suspicious?")
    if 'yes' in flag:
        db.set("fraud_flag", True)
```

* ‚úñÔ∏è Memory scattered across storage layers
* ‚úñÔ∏è LLM logic lacks context

**With DANA:**

```python
if stmem.login_attempts > 3:
    temp.review = reason("Is this suspicious behavior?", context=stmem)
    if temp.review == "yes":
        ltmem.flags.fraud_alert = true
```

* ‚úÖ Reasoning is scoped and contextual
* ‚úÖ Long/short-term memory is structured and unified

---
## üîç Advantages of DANA

DANA offers several unique capabilities that make certain tasks much easier compared to other agentic frameworks:

1. Fine-grained control flow with AI reasoning integration

```python
if temp_reading > threshold:
    diagnosis = reason("Analyze this temperature anomaly", context=system_state)
    if "critical" in diagnosis:
        alert_operator()
      else:
          log_event(diagnosis)
```

2. Unlike monolithic agent frameworks, DANA lets you precisely control when and how AI reasoning is applied.

```python
if temp_reading > threshold:
    diagnosis = reason("Analyze this temperature anomaly", context=system_state)
    if "critical" in diagnosis:
        alert_operator()
      else:
          log_event(diagnosis)
```

3. Domain-specific validation and guardrails

```python
  # Auto-validates temperature ranges based on rules
  temp = get_sensor_reading()
  if temp > valid_temp_range.max:
      log_error("Invalid reading, using last valid reading instead")
      temp = last_valid_reading
```

4. DANA's type system and runtime validation make building domain guardrails straightforward.

5. Hybrid deterministic/probabilistic execution

```python
  # Deterministic processing for critical components
  error_code = extract_error_code(log_message)

  # LLM reasoning for nuanced analysis
  explanation = reason(f"Explain error code {error_code} in plain language")

  # Deterministic action based on reasoned output
  if "memory leak" in explanation:
      restart_service(memory_monitor=True)
```

3. This hybrid approach is cumbersome in pure-LLM frameworks.

4. Stateful execution with clear scoping

```python
  # Persist data across execution runs
  global.error_count += 1

  # Group-level state visible to specific components
  group.last_maintenance = current_time()

  # Private state for component isolation
  private.calibration_factor = calculate_calibration()
```

4. DANA's explicit scoping system avoids the confusion of managing state in typical agent frameworks.

5. Lightweight tooling that doesn't require complex setups

```python
  # Simple utility functions vs complex tool definitions
  cleaned_data = cleanup_data(raw_data)

  # Easy resource access
  db = get_resource("database")
  db.save(processed_data)
```

5. Many frameworks require verbose tool definitions; DANA makes simple operations simple.

6. Clear error handling and recovery

```python
try:
    result = reason("Analyze log patterns for anomalies", context=logs)
  except RateLimitError:
      log_warning("Rate limited, using cached analysis")
      result = cached_analysis
```

7. Predictable error handling that integrates with standard programming patterns.

8. Progressive elaboration with reasoning

```python
  # Start with high-level planning
  plan = reason("Create a maintenance plan", context=equipment_state)

  # Elaborate specific steps based on the plan
  for step in plan.steps:
      details = reason(f"Elaborate detailed procedure for: {step.description}")
      log(f"Step {step.id}: {details}")
```

7. Other frameworks often can't easily mix planning and execution this way.

8. Multi-agent coordination with explicit message passing

```python
  # Explicit coordination between specialized agents
  data_analysis = reason("Analyze performance data", context=metrics)
  recommendation = reason("Recommend optimizations based on this analysis",
                          context=[data_analysis, system_constraints])
  validation = reason("Validate this recommendation against safety guidelines",
                      context=[recommendation, safety_rules])
```

8. This explicit coordination avoids the "agent hallucination" problems of more complex frameworks.

DANA excels at bridging the gap between traditional programming and AI capabilities, making it especially powerful for industrial, scientific, and enterprise applications where reliability and explainability matter alongside AI capabilities.

---

## ü§ù Relationship to OpenDXA

* DANA is embedded within OpenDXA as the **reasoning and execution layer**
* Agents emit or receive DANA programs to carry out goals
* Tool calls and reasoning steps all route through DANA
* The KB provides reusable domain logic to DXAs and GMAs

---

## üîß DANA Language Features

### LLM Integration with reason()

The `reason()` statement provides direct access to LLM capabilities within DANA scripts, enabling AI reasoning as a first-class language feature.

#### Syntax

```
[result_var = ]reason(prompt[, context=[var1, var2, ...]][, option1=value1, ...])
```

- `result_var`: Optional variable to store the result (if omitted, result is logged)
- `prompt`: The text prompt to send to the LLM (string or expression)
- `context`: Optional list of variables to include as context
- `options`: Optional parameters for the LLM call

#### Options

- `format`: Output format (default: "text", can be "json" for structured data)
- `temperature`: Controls randomness (0.0-1.0, default: 0.7)
- `max_tokens`: Maximum tokens in the response

#### Examples

```dana
# Basic usage - log the result
reason("What is the capital of France?")

# Assign result to a variable
answer = reason("What is the capital of France?")

# Use with context variables
temperature_data = [72, 75, 78, 74, 72, 71, 68]
equipment = "HVAC Unit 42"
analysis = reason("Analyze this temperature data for anomalies", context=[temperature_data, equipment])

# Specify format as JSON to get structured output
json_result = reason("Generate a list of 3 fruit names with their colors", format="json")

# Control temperature for more creative responses
ideas = reason("Brainstorm 5 innovative uses for a paperclip", temperature=0.9)

# Use with f-strings for dynamic prompts
query_subject = "photosynthesis"
explanation = reason(f"Explain {query_subject} in simple terms")
```

#### LLM Resource

The `reason()` statement requires an LLM resource to function. For convenience, DANA will try to auto-instantiate an LLM resource if none is registered with the name "llm", but for production use, you should explicitly register an LLM resource:

```python
from opendxa.common.resource.llm_resource import LLMResource
from opendxa.dana.runtime.context import RuntimeContext

# Create a runtime context
context = RuntimeContext()

# Configure and register an LLM resource
llm = LLMResource(
    name="my_llm",
    provider="anthropic",  # or "openai", "azure", etc.
    model="claude-3-opus-20240229"
)
context.add_resource("llm", llm)

# Now the reason() statements in your DANA code will use this LLM
```

##### Auto-Instantiation

When a DANA script uses `reason()` without a registered LLM resource:

1. It will attempt to create a default LLM resource on the first call
2. This requires valid API keys in environment variables (like `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`)
3. If auto-instantiation fails, you'll see an error like: `Error in reason statement: Resource not found: llm`

The REPL will automatically register any LLM resource you provide for both transcoding and reasoning, making it easy to experiment with these features.

##### Configuring LLM Resources

To use `reason()` statements, you need to set up an LLM resource in one of these ways:

1. **Environment Variables**: Set one of these in your environment:
   - `OPENAI_API_KEY` (for OpenAI models)
   - `ANTHROPIC_API_KEY` (for Claude models)
   - `AZURE_OPENAI_API_KEY` (for Azure OpenAI models)
   - `GROQ_API_KEY` (for Groq models)
   - `GOOGLE_API_KEY` (for Google models)

2. **Configuration File**: Create an `opendxa_config.json` file with preferred models:
   ```json
   {
     "preferred_models": [
       {
         "name": "anthropic:claude-3-sonnet-20240229",
         "required_api_keys": ["ANTHROPIC_API_KEY"]
       },
       {
         "name": "openai:gpt-4o-mini",
         "required_api_keys": ["OPENAI_API_KEY"]
       }
     ]
   }
   ```

3. **Explicit Registration**: Manually create and register an LLM resource:
   ```python
   from opendxa.common.resource.llm_resource import LLMResource
   from opendxa.dana.runtime.context import RuntimeContext
   
   context = RuntimeContext()
   llm = LLMResource(name="reason_llm")
   await llm.initialize()  # Always initialize before use
   context.register_resource("llm", llm)
   ```

##### Troubleshooting Reason Statements

If you see `Error in reason statement: Resource not found: llm`, check:

1. Your environment variables are set correctly
2. You have a valid API key for one of the supported LLM providers
3. The LLM resource was properly initialized and registered
4. Network connectivity to the LLM provider's API

For more advanced LLM configurations, refer to the `LLMResource` class documentation.

## üì£ Contribute

Want to help shape how intelligent agents reason and act? Reach out or contribute to the `dana/` module inside the OpenDXA repository.

<p align="center">
Copyright ¬© 2025 Aitomatic, Inc. Licensed under the <a href="../LICENSE.md">MIT License</a>.
<br/>
<a href="https://aitomatic.com">https://aitomatic.com</a>
</p>

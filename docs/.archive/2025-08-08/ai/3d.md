# 3D Methodology (Design-Driven Development)

**3D = Design-Driven Development**: A rigorous methodology ensuring quality through comprehensive design documentation, iterative implementation phases, and strict quality gates.

Core principle: Think before you build, build with intention, ship with confidence.

## 🎯 Design Document Philosophy

**Conciseness with Clarity**: Design documents should be comprehensive yet concise. Focus on the "why" and "what" rather than the "how" of implementation details.

**Key Principles**:
- **Sufficient Detail**: Include enough information to understand the design without ambiguity
- **Implementation-Agnostic**: Focus on architecture and approach, not specific code
- **Decision-Driven**: Document the reasoning behind design choices
- **Future-Proof**: Design should be clear enough for future maintainers
- **Concise Length**: Design documents should never exceed approximately 400 lines

## 🛠️ Common Commands
```bash
# Core development workflow
uv run ruff check . && uv run ruff format .    # Lint and format
uv run pytest tests/ -v                        # Run tests with verbose output
uv run python -m dana.dana.exec.repl        # Dana REPL for testing
```

## 📋 ALWAYS Create Design Document First

For any feature/system implementation, create two documents:

1. **Design Document**: `[feature_name].md`
   - Contains the design specification
   - Documents the architecture and approach
   - Defines requirements and constraints

2. **Implementation Tracker**: `[feature_name]-implementation.md`
   - Tracks implementation progress
   - Contains design review status
   - Monitors quality gates
   - Records decisions and changes

### Design Document Template
```markdown
# Design Document: [Feature Name]

<!-- text markdown -->
Author: [Name]
Version: 1.0
Date: [Date]
Status: [Design Phase | Implementation Phase | Review Phase]
Implementation Tracker: [feature_name]-implementation.md
<!-- end text markdown -->

## Problem Statement
**Brief Description**: [1-2 sentence summary of the problem]
- Current situation and pain points
- Impact of not solving this problem  
- Relevant context and background

## Goals
**Brief Description**: [What we want to achieve]
- Specific, measurable objectives (SMART goals)
- Success criteria and metrics
- Key requirements

## Non-Goals
**Brief Description**: [What we explicitly won't do]
- Explicitly state what's out of scope
- Clarify potential misunderstandings

## Proposed Solution
**Brief Description**: [High-level approach in 1-2 sentences]
- High-level approach and key components
- Why this approach was chosen
- Main trade-offs and system fit
- **KISS/YAGNI Analysis**: Justify complexity vs. simplicity choices

## Proposed Design
**Brief Description**: [System architecture overview]

### System Architecture Diagram
<!-- mermaid markdown -->
[Create ASCII or Mermaid diagram showing main components and their relationships]
<!-- end mermaid markdown -->

### Component Details
**Brief Description**: [Overview of each major component and its purpose]
- System architecture and components
- Data models, APIs, interfaces
- Error handling and security considerations
- Performance considerations

**Motivation and Explanation**: Each component section must include:
- **Why this component exists** and what problem it solves
- **How it fits into the overall system** architecture
- **Key design decisions** and trade-offs made
- **Alternatives considered** and why they were rejected
- **Don't rely on code to be self-explanatory** - explain the reasoning

### Data Flow Diagram (if applicable)
<!-- mermaid markdown -->
[Show how data moves through the system]
<!-- end mermaid markdown -->

## Proposed Implementation
**Brief Description**: [Technical approach and key decisions]
- Technical specifications and code organization
- Key algorithms and testing strategy
- Dependencies and monitoring requirements

## ⚠️ What NOT to Include in Design Documents

**Avoid Implementation Details**:
- ❌ Actual code snippets or pseudocode
- ❌ Specific function signatures or class definitions
- ❌ Detailed algorithm implementations
- ❌ Database schema specifics
- ❌ Configuration file contents

**Focus Instead On**:
- ✅ High-level architecture and component relationships
- ✅ Design patterns and approaches
- ✅ Interface contracts and data flow
- ✅ Error handling strategies
- ✅ Performance and scalability considerations
- ✅ Security and reliability requirements

**Code belongs in implementation, not design documents.**

**Length Guideline**: Design documents should never exceed approximately 400 lines. If your document is getting too long, consider:
- Breaking complex features into separate design documents
- Moving implementation details to examples or implementation files
- Focusing on the most critical architectural decisions
- Using appendices for detailed technical specifications
```

### Implementation Tracker Template
```markdown
# Implementation Tracker: [Feature Name]

<!-- text markdown -->
Author: [Name]
Version: 1.0
Date: [Date]
Status: [Design Phase | Implementation Phase | Review Phase]
Design Document: [feature_name].md
<!-- end text markdown -->

## Design Review Status
- [ ] **Problem Alignment**: Does solution address all stated problems?
- [ ] **Goal Achievement**: Will implementation meet all success criteria?
- [ ] **Non-Goal Compliance**: Are we staying within defined scope?
- [ ] **KISS/YAGNI Compliance**: Is complexity justified by immediate needs?
- [ ] **Security review completed**
- [ ] **Performance impact assessed**
- [ ] **Error handling comprehensive**
- [ ] **Testing strategy defined**
- [ ] **Documentation planned**
- [ ] **Backwards compatibility checked**

## Implementation Progress
**Overall Progress**: [ ] 0% | [ ] 20% | [ ] 40% | [ ] 60% | [ ] 80% | [ ] 100%

### Phase 1: Foundation & Architecture (~15-20%)
- [ ] Define core components and interfaces
- [ ] Create basic infrastructure and scaffolding
- [ ] Establish architectural patterns and conventions
- [ ] **Phase Gate**: Run tests - ALL tests pass
- [ ] **Phase Gate**: Update implementation progress checkboxes

[Other phases remain the same...]

## Quality Gates
⚠️  DO NOT proceed to next phase until ALL criteria met:
✅ 100% test pass rate - ZERO failures allowed
✅ No regressions detected in existing functionality
✅ Error handling complete and tested with failure scenarios
✅ Examples created and validated (Phase 6 only)
✅ Documentation updated and cites working examples (Phase 6 only)
✅ Performance within defined bounds
✅ Implementation progress checkboxes updated
✅ Design review completed (if in Phase 1)
✅ `.na` tests created for Dana language functionality
✅ Universal test runners implemented for all test categories

## Technical Debt & Maintenance
- [ ] **Code Analysis**: Run automated analysis tools
- [ ] **Complexity Review**: Assess code complexity metrics
- [ ] **Test Coverage**: Verify test coverage targets
- [ ] **Documentation**: Update technical documentation
- [ ] **Performance**: Validate performance metrics
- [ ] **Security**: Complete security review

## Recent Updates
- [Date] [Update description]
- [Date] [Update description]

## Notes & Decisions
- [Date] [Important decision or note]
- [Date] [Important decision or note]

## Upcoming Milestones
- [Date] [Milestone description]
- [Date] [Milestone description]
```

## 🔄 3D Process: Think → Build → Ship

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Phase 1:      │    │   Phase 2-5:    │    │   Phase 6:      │
│ Design & Test   │ -> │ Implement &     │ -> │ Examples, Docs  │
│                 │    │ Validate        │    │ & Polish        │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

### Testing Integration in 3D Process

**Phase 1 (Design & Test)**:
- Design test strategy alongside system design
- Plan `.na` test categories and coverage areas
- Define test requirements and success criteria

**Phase 2-5 (Implement & Validate)**:
- Create `.na` tests as you implement features
- Use universal test runners for automatic discovery
- Validate each component with focused test files
- Maintain 100% test pass rate throughout development

**Phase 6 (Examples, Docs & Polish)**:
- Create comprehensive example tests
- Document test patterns and best practices
- Validate examples with working test files
- Ensure all documentation cites tested examples

## 📊 Implementation Tracking

For design review and implementation tracking, see:
- [3D Build Tracker](3d-build.md) - Active project tracking and progress monitoring

The build tracker includes:
- Design review status and checklists
- Implementation progress by phase
- Quality gates and validation criteria
- Technical debt monitoring
- Project status overview
- Recent updates and decisions
- Upcoming milestones

## 🧪 Testing Strategy

For comprehensive testing guidelines and best practices, see:
- [Dana Testing Guidelines](dana-testing.md) - Universal test runner pattern and `.na` test recommendations

**Testing Philosophy**: Prefer `.na` unit tests over `.py` tests when testing Dana language functionality. The universal test runner pattern provides:
- Minimal boilerplate for Dana language tests
- Automatic discovery of test files
- Consistent test execution environment
- Better alignment with the language being tested

## 📁 Documentation & Examples Organization

For detailed directory structures and organization guidelines, see:
- [Documentation Structure Reference](documentation_structure.md)
- [Examples Structure Reference](examples_structure.md)

### Organization Guidelines
- **Major Features**: Independent systems that warrant their own directory (e.g., POET, Dana Language)
- **Subsystems**: Components of larger systems (e.g., parser, interpreter within Dana)
- **Examples Mirror Documentation**: Same directory structure for easy cross-referencing
- **Documentation Cites Examples**: All user-facing docs should reference working examples

## 📚 Example Creation Guidelines

### 🎯 Purpose-Driven Examples

Examples are created in **Phase 6** after core implementation is complete and stable. Every example must serve a **specific learning objective** and follow the **Progressive Disclosure** principle:

```
🎓 **LEARNING PROGRESSION**:
1. Start with minimal working example
2. Add complexity gradually
3. Explain each addition
4. Show real-world usage
5. Demonstrate best practices
```

### 📝 Documentation vs. Implementation Balance

**Design Documents**: Focus on architecture, decisions, and reasoning
**Examples**: Show working code with clear explanations
**Implementation**: Actual code with comprehensive tests

**Guideline**: If you're writing more than 2-3 lines of code in a design document, move it to an example or implementation file.

### Example Structure
```
examples/
├── [major_feature]/         # For large efforts (e.g., examples/poet/)
│   ├── README.md           # Overview and navigation
│   ├── 01_hello_world/     # Minimal working examples
│   ├── 02_basic_usage/     # Common patterns
│   ├── 03_real_world/      # Production-like scenarios
│   ├── 04_advanced/        # Complex scenarios
│   ├── troubleshooting.md  # Common issues
│   └── tests/              # Example validation tests
```

### Example Requirements
- **Working Code**: All examples must be runnable and tested
- **Clear Purpose**: Each example demonstrates specific concepts
- **Progressive Complexity**: Build from simple to complex
- **Real-World Context**: Show practical applications
- **Best Practices**: Demonstrate recommended patterns
- **Error Handling**: Include error cases and recovery
- **Documentation**: Clear explanations and comments
- **Tests**: Validation tests for each example

### 📋 Content Guidelines

**Design Documents Should**:
- ✅ Explain the "why" behind design decisions
- ✅ Document architectural patterns and trade-offs
- ✅ Define clear interfaces and contracts
- ✅ Specify error handling strategies
- ✅ Outline performance and security considerations

**Design Documents Should NOT**:
- ❌ Contain implementation code
- ❌ Include detailed algorithms
- ❌ Show specific data structures
- ❌ List function signatures
- ❌ Provide configuration examples

**Examples Should**:
- ✅ Demonstrate the design in action
- ✅ Show working, runnable code
- ✅ Include clear explanations
- ✅ Cover common use cases
- ✅ Handle error scenarios

## 🧪 Testing Standards

### Core Testing Principles
- **PREFER `.na` tests** for Dana language functionality over `.py` tests
- **USE universal test runners** for automatic discovery and consistent execution
- **CREATE focused test files** that test one concept per file
- **MAINTAIN test isolation** with fresh sandbox for each test

### When to Use `.na` vs `.py` Tests

**Use `.na` Tests When**:
- ✅ Testing Dana language features and syntax
- ✅ Testing Dana functions and modules
- ✅ Testing Dana integration scenarios
- ✅ Testing Dana error handling and edge cases
- ✅ Testing Dana performance characteristics

**Use `.py` Tests When**:
- ✅ Testing Python API interfaces
- ✅ Testing internal Python implementation details
- ✅ Testing complex setup/teardown scenarios
- ✅ Testing Python-specific error conditions
- ✅ Testing integration with external Python libraries

### Test Creation Guidelines
- **Follow the universal runner pattern** documented in [Dana Testing Guidelines](dana-testing.md)
- **Create descriptive test names** that explain what is being tested
- **Include both success and failure scenarios** in your tests
- **Use realistic test data** that mirrors production usage
- **Keep tests focused and maintainable**

## 📝 Logging Standards

### Core Logging Principles
- **ALWAYS use `Loggable` mixin** for Python classes that need logging
- **NEVER use `DXA_LOGGER` directly** in class implementations
- **Use `log()` function** for Dana code debugging
- **Apply consistent log levels** across the codebase

### Loggable Mixin Usage
```python
from opendxa.common.mixins.loggable import Loggable

class MyClass(Loggable):
    def __init__(self):
        super().__init__()  # Initialize Loggable mixin
        self.info("Initializing MyClass")
    
    def process_data(self, data: list[str]) -> str:
        self.debug(f"Processing {len(data)} items")
        try:
            result = self._process(data)
            self.info(f"Successfully processed {len(data)} items")
            return result
        except Exception as e:
            self.error(f"Failed to process data: {e}")
            raise
```

### Log Levels
- **DEBUG**: Detailed information for debugging
- **INFO**: General operational information
- **WARNING**: Unexpected but handled situations
- **ERROR**: Errors that need attention
- **CRITICAL**: System-level failures

### Best Practices
1. **Class-Level Logging**:
   - Inherit from `Loggable` for all classes needing logging
   - Initialize mixin in `__init__` with `super().__init__()`
   - Use `self.debug()`, `self.info()`, etc. for logging

2. **Dana Code Logging**:
   - Use `log()` function instead of `print()`
   - Include context in log messages
   - Use appropriate log levels

3. **Error Handling**:
   - Log errors with full context
   - Include stack traces for debugging
   - Provide actionable error messages

4. **Performance**:
   - Use appropriate log levels to control verbosity
   - Avoid expensive operations in debug logs
   - Consider log rotation and cleanup

### Quality Gates
- ✅ All classes using logging inherit from `Loggable`
- ✅ No direct `DXA_LOGGER` usage in class implementations
- ✅ Consistent log levels across codebase
- ✅ Comprehensive error logging with context
- ✅ Performance impact of logging assessed

## 🤖 AI Collaboration Optimization

### Design Document Review Guidelines

When reviewing design documents with AI assistance:

**Focus Areas**:
- **Clarity**: Is the design clear and unambiguous?
- **Completeness**: Are all key decisions documented?
- **Conciseness**: Is the document focused and to-the-point?
- **Architecture**: Is the system design sound and well-reasoned?

**Red Flags**:
- 🚨 Too much implementation code in design docs
- 🚨 Missing rationale for design decisions
- 🚨 Unclear component responsibilities
- 🚨 Vague or ambiguous requirements
- 🚨 Over-engineering without justification

**Quality Questions**:
- Does this design solve the stated problem?
- Are the trade-offs clearly explained?
- Is the complexity justified by the requirements?
- Will future developers understand this design?
- Are there clear interfaces between components?

[Previous AI collaboration section content remains unchanged]

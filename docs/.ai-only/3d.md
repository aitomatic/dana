# 3D Methodology (Design-Driven Development)

**3D = Design-Driven Development**: A rigorous methodology ensuring quality through comprehensive design documentation, iterative implementation phases, and strict quality gates.

Core principle: Think before you build, build with intention, ship with confidence.

## 📋 ALWAYS Create Design Document First

For any feature/system implementation, create a design doc following this template:

```markdown
# Design Document: [Feature Name]

<!-- text markdown -->
Author: [Name]
Version: 1.0
Date: [Date]
Status: [Design Phase | Implementation Phase | Review Phase]
<!-- end text markdown -->

## Problem Statement
**Brief Description**: [1-2 sentence summary of the problem]
- Current situation and pain points
- Impact of not solving this problem  
- Relevant context and background

## Goals
**Brief Description**: [What we want to achieve]
- Specific, measurable objectives (SMART goals)
- Success criteria and metrics
- Key requirements

## Non-Goals
**Brief Description**: [What we explicitly won't do]
- Explicitly state what's out of scope
- Clarify potential misunderstandings

## Proposed Solution
**Brief Description**: [High-level approach in 1-2 sentences]
- High-level approach and key components
- Why this approach was chosen
- Main trade-offs and system fit
- **KISS/YAGNI Analysis**: Justify complexity vs. simplicity choices

## Proposed Design
**Brief Description**: [System architecture overview]

### System Architecture Diagram
<!-- mermaid markdown -->
[Create ASCII or Mermaid diagram showing main components and their relationships]
<!-- end mermaid markdown -->

### Component Details
**Brief Description**: [Overview of each major component and its purpose]
- System architecture and components
- Data models, APIs, interfaces
- Error handling and security considerations
- Performance considerations

**Motivation and Explanation**: Each component section must include:
- **Why this component exists** and what problem it solves
- **How it fits into the overall system** architecture
- **Key design decisions** and trade-offs made
- **Alternatives considered** and why they were rejected
- **Don't rely on code to be self-explanatory** - explain the reasoning

### Data Flow Diagram (if applicable)
<!-- mermaid markdown -->
[Show how data moves through the system]
<!-- end mermaid markdown -->

## Proposed Implementation
**Brief Description**: [Technical approach and key decisions]
- Technical specifications and code organization
- Key algorithms and testing strategy
- Dependencies and monitoring requirements

## Design Review Checklist
**Status**: [ ] Not Started | [ ] In Progress | [ ] Complete

Before implementation, review design against:
- [ ] **Problem Alignment**: Does solution address all stated problems?
- [ ] **Goal Achievement**: Will implementation meet all success criteria?
- [ ] **Non-Goal Compliance**: Are we staying within defined scope?
- [ ] **KISS/YAGNI Compliance**: Is complexity justified by immediate needs?
- [ ] **Security review completed**
- [ ] **Performance impact assessed**
- [ ] **Error handling comprehensive**
- [ ] **Testing strategy defined**
- [ ] **Documentation planned**
- [ ] **Backwards compatibility checked**

## Implementation Phases
**Overall Progress**: [ ] 0% | [ ] 20% | [ ] 40% | [ ] 60% | [ ] 80% | [ ] 100%

### Phase 1: Foundation & Architecture (16.7% of total)
**Description**: Establish core infrastructure and architectural patterns
- [ ] Define core components and interfaces
- [ ] Create basic infrastructure and scaffolding
- [ ] Establish architectural patterns and conventions
- [ ] **Phase Gate**: Run `uv run pytest tests/ -v` - ALL tests pass
- [ ] **Phase Gate**: Update implementation progress checkboxes

### Phase 2: Core Functionality (16.7% of total)
**Description**: Implement primary features and happy path scenarios
- [ ] Implement primary features and core logic
- [ ] Focus on happy path scenarios and basic operations
- [ ] Create working examples and demonstrations
- [ ] **Phase Gate**: Run `uv run pytest tests/ -v` - ALL tests pass
- [ ] **Phase Gate**: Update implementation progress checkboxes

### Phase 3: Error Handling & Edge Cases (16.7% of total)
**Description**: Add comprehensive error detection and edge case handling
- [ ] Add comprehensive error detection and validation
- [ ] Test failure scenarios and error conditions
- [ ] Handle edge cases and boundary conditions
- [ ] **Phase Gate**: Run `uv run pytest tests/ -v` - ALL tests pass
- [ ] **Phase Gate**: Update implementation progress checkboxes

### Phase 4: Advanced Features & Integration (16.7% of total)
**Description**: Add sophisticated functionality and ensure seamless integration
- [ ] Add sophisticated functionality and advanced features
- [ ] Test complex interactions and integration scenarios
- [ ] Ensure seamless integration with existing systems
- [ ] **Phase Gate**: Run `uv run pytest tests/ -v` - ALL tests pass
- [ ] **Phase Gate**: Update implementation progress checkboxes

### Phase 5: Integration & Performance Testing (16.7% of total)
**Description**: Validate real-world performance and run comprehensive tests
- [ ] Test real-world scenarios and production-like conditions
- [ ] Validate performance benchmarks and requirements
- [ ] Run regression tests and integration suites
- [ ] **Phase Gate**: Run `uv run pytest tests/ -v` - ALL tests pass
- [ ] **Phase Gate**: Update implementation progress checkboxes

### Phase 6: Examples, Documentation & Polish (16.7% of total)
**Description**: Create comprehensive examples, finalize documentation, and perform final validation
- [ ] **Create Examples**: Generate comprehensive examples following Example Creation Guidelines
- [ ] **Documentation**: Create user-facing documentation that cites examples
- [ ] **API Documentation**: Update API references and technical docs
- [ ] **Migration Guides**: Create upgrade instructions and compatibility notes
- [ ] **Final Validation**: Final testing and sign-off
- [ ] **Phase Gate**: Run `uv run pytest tests/ -v` - ALL tests pass
- [ ] **Phase Gate**: Update implementation progress checkboxes to 100%
```

## 🔄 3D Process: Think → Build → Ship

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Phase 1:      │    │   Phase 2:      │    │   Phase 3:      │
│ Design & Test   │ -> │ Implement &     │ -> │ Examples, Docs  │
│                 │    │ Validate        │    │ & Polish        │
└─────────────────┘    └─────────────────┘    └─────────────────┘

⚠️  DO NOT proceed to next phase until ALL criteria met:
✅ 100% test pass rate (uv run pytest tests/ -v) - ZERO failures allowed
✅ No regressions detected in existing functionality
✅ Error handling complete and tested with failure scenarios
✅ Examples created and validated (Phase 6 only)
✅ Documentation updated and cites working examples (Phase 6 only)
✅ Performance within defined bounds
✅ Implementation progress checkboxes updated
✅ Design review completed (if in Phase 1)

🧪 **CRITICAL: Every phase MUST end with full test validation**
- Run `uv run pytest tests/ -v` before marking phase complete
- ALL tests must pass - no exceptions, no "TODO: fix later"
- Any test failure = phase incomplete, must fix before proceeding
- Add new tests for new functionality within the same phase
```

**Phase 1: Design & Test (Think)**
- Write design doc with Problem Statement/Goals/Non-Goals/Proposed Solution
- **Include brief descriptions** for each section, not just bullet points
- **Create system diagrams** (ASCII or Mermaid) showing component relationships
- **Perform Design Review** against problem statement, goals, and non-goals
- **Update Design Review Checklist** with findings and approvals
- Create failing tests defining expected behavior
- **Run full test suite**: `uv run pytest tests/ -v` - verify baseline
- **Update Phase 1 checkboxes** before proceeding

**Phase 2-5: Implement & Validate (Build)** 
- Core implementation with **brief code comments** explaining logic
- Make tests pass, add edge case coverage
- **Include explanatory comments** in complex algorithms
- **Document component motivations** - explain why each component exists and how it solves problems
- **Don't rely on code to be self-explanatory** - add contextual explanations
- **Run full test suite** after each major change: `uv run pytest tests/ -v`
- **Fix ALL test failures immediately** - no proceeding with failing tests
- **Add new tests for new functionality** within the same phase
- **Update Phase 2-5 checkboxes** as work progresses

**Phase 6: Examples, Documentation & Polish (Ship)**
- **Create Examples**: Generate comprehensive examples using Example Creation Guidelines
- **Documentation**: Write user-facing docs that cite and reference examples
- **API Documentation**: Update technical references and API docs
- Integration testing and performance validation
- **Run comprehensive test suite**: `uv run pytest tests/ -v`
- **Verify no regressions** introduced during implementation
- **Update final checkboxes** and mark implementation complete

## 📁 Documentation & Examples Organization

### Documentation Structure
```
docs/
├── [major_feature]/          # For large efforts (e.g., docs/poet/)
│   ├── README.md            # Overview and getting started
│   ├── architecture/        # System design and architecture
│   ├── user-guide/          # User-facing documentation
│   ├── api-reference/       # Technical API documentation
│   └── troubleshooting/     # Common issues and solutions
├── [subsystem]/             # For smaller subsystems (e.g., docs/dana/)
│   ├── [component]/         # Specific components
│   └── README.md            # Subsystem overview
└── design/                  # Design documents and architecture
    └── .implementation/     # Implementation tracking and progress
```

### Examples Structure
```
examples/
├── [major_feature]/         # For large efforts (e.g., examples/poet/)
│   ├── README.md           # Overview and navigation
│   ├── 01_hello_world/     # Minimal working examples
│   ├── 02_basic_usage/     # Common patterns
│   ├── 03_real_world/      # Production-like scenarios
│   ├── 04_advanced/        # Complex scenarios
│   ├── troubleshooting.md  # Common issues
│   └── tests/              # Example validation tests
├── [subsystem]/            # For smaller subsystems (e.g., examples/dana/)
│   ├── [component]/        # Component-specific examples
│   └── README.md           # Subsystem example overview
└── getting_started/        # Cross-cutting introductory examples
```

### Organization Guidelines
- **Major Features**: Independent systems that warrant their own directory (e.g., POET, Dana Language)
- **Subsystems**: Components of larger systems (e.g., parser, interpreter within Dana)
- **Examples Mirror Documentation**: Same directory structure for easy cross-referencing
- **Documentation Cites Examples**: All user-facing docs should reference working examples

## 📚 Example Creation Guidelines

### 🎯 Purpose-Driven Examples

Examples are created in **Phase 6** after core implementation is complete and stable. Every example must serve a **specific learning objective** and follow the **Progressive Disclosure** principle:

```
🎓 **LEARNING PROGRESSION**:
1. **Hello World** - Simplest possible working example
2. **Basic Usage** - Common patterns and typical use cases  
3. **Real World** - Production-like scenarios with error handling
4. **Advanced** - Complex integrations and edge cases
```

### 📝 Example Content Standards

**Every example file must include:**

```dana
# ==============================================================================
# [Example Title] - [Learning Objective]
# ==============================================================================
# PURPOSE: [1-sentence description of what this demonstrates]
# AUDIENCE: [Beginner/Intermediate/Advanced/Expert]
# PREREQUISITES: [What user should know before this example]
# ESTIMATED TIME: [How long this should take to understand/run]
# RELATED EXAMPLES: [Links to prerequisite/follow-up examples]
# ==============================================================================

# WHAT YOU'LL LEARN:
# - [Specific skill or concept #1]
# - [Specific skill or concept #2]
# - [Specific skill or concept #3]

# SETUP (if any special setup required):
# 1. [Step 1]
# 2. [Step 2]

# --- EXAMPLE CODE STARTS HERE ---

# Step 1: [What this code block demonstrates]
# WHY: [Why we're doing it this way]
[code block with extensive comments]

# Step 2: [Next demonstration]
# WHY: [Reasoning behind this approach]
[code block with extensive comments]

# --- EXAMPLE OUTPUT ---
# Expected result when you run this:
# [Show expected output or behavior]

# --- WHAT'S NEXT ---
# To learn more, check out:
# - examples/[feature]/02_basic_usage/ (if this is 01_hello_world)
# - Related concept: examples/[other_feature]/
```

### 🔍 Example Quality Checklist

**Content Quality:**
- [ ] **Single Learning Objective**: Each example teaches ONE clear concept
- [ ] **Progressive Building**: Each example builds on previous knowledge
- [ ] **Realistic Use Case**: Examples solve real problems users face
- [ ] **Comprehensive Comments**: Every line of non-obvious code is explained
- [ ] **Expected Output**: Users know what success looks like

**Technical Quality:**
- [ ] **Working Code**: Examples run without modification
- [ ] **Error Handling**: Show both success and failure scenarios
- [ ] **Performance Aware**: Examples don't teach bad practices
- [ ] **Security Conscious**: No hardcoded secrets or vulnerable patterns
- [ ] **Backwards Compatible**: Examples work with current system version

**User Experience:**
- [ ] **Copy-Paste Ready**: Users can copy and run immediately
- [ ] **Clear Prerequisites**: Users know what they need before starting
- [ ] **Troubleshooting**: Common issues and solutions provided
- [ ] **Next Steps**: Clear path to more advanced examples
- [ ] **Estimated Time**: Realistic time expectations set

### 📊 Example Testing Requirements

**Every example must be validated:**

```python
# examples/[feature]/tests/test_examples.py
import pytest
from pathlib import Path

def test_hello_world_example():
    """Verify hello world example runs and produces expected output."""
    # Test that basic example executes without errors
    # Verify expected output or behavior
    # Check for any error conditions
    pass

def test_basic_usage_examples():
    """Verify basic usage examples work correctly."""
    # Test each example in 02_basic_usage/
    # Verify configurations work as documented
    # Check error handling scenarios
    pass

def test_real_world_examples():
    """Verify real-world examples handle edge cases."""
    # Test integration scenarios
    # Verify error handling and recovery
    # Check performance characteristics
    pass
```

## 🧪 Unit Testing Guidelines

### 🎯 Test Philosophy

Follow **Test-Driven Development** principles within 3D methodology:

```
🧪 **TESTING PYRAMID**:
1. **Unit Tests (70%)** - Fast, isolated, focused on single functions/methods
2. **Integration Tests (20%)** - Test component interactions
3. **End-to-End Tests (10%)** - Full system behavior validation
```

### 📝 Test Content Standards

**Every test file must include:**

```python
"""
Test module for [ComponentName].

This module tests the [component] functionality including:
- [Core behavior #1]
- [Core behavior #2]
- [Error handling scenarios]
- [Edge cases and boundary conditions]

Test Coverage Areas:
- Happy path scenarios (basic functionality works)
- Error scenarios (proper error handling and recovery)
- Edge cases (boundary conditions and unusual inputs)
- Performance characteristics (for critical paths)
- Integration behavior (component interactions)
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
from [module] import [ComponentUnderTest]

# Test Data and Fixtures
@pytest.fixture
def sample_data():
    """Provide consistent test data across test methods."""
    return {
        "valid_input": "expected_value",
        "edge_case": "boundary_value",
        "invalid_input": None
    }

class Test[ComponentName]:
    """Test suite for [ComponentName] functionality."""
    
    def test_[behavior]_success(self, sample_data):
        """Test [specific behavior] succeeds with valid input.
        
        Verifies:
        - [Expected behavior #1]
        - [Expected behavior #2]
        - [Expected side effects]
        """
        # Arrange
        component = ComponentUnderTest()
        expected_result = "expected_value"
        
        # Act
        result = component.method(sample_data["valid_input"])
        
        # Assert
        assert result == expected_result
        assert component.state == "expected_state"
    
    def test_[behavior]_with_invalid_input(self):
        """Test [behavior] handles invalid input gracefully.
        
        Verifies:
        - Appropriate exception is raised
        - Error message is clear and actionable
        - System state remains consistent
        """
        # Arrange
        component = ComponentUnderTest()
        
        # Act & Assert
        with pytest.raises(ValueError, match="specific error pattern"):
            component.method(None)
    
    def test_[behavior]_edge_cases(self, sample_data):
        """Test [behavior] handles edge cases correctly.
        
        Verifies:
        - Boundary conditions work correctly
        - Performance remains acceptable
        - No unexpected side effects
        """
        # Test boundary conditions
        # Test performance with large inputs
        # Test unusual but valid scenarios
        pass
```

### 🔍 Test Quality Standards

**Test Coverage Requirements:**
- [ ] **Line Coverage**: Minimum 90% for new code
- [ ] **Branch Coverage**: All conditional paths tested
- [ ] **Function Coverage**: Every public function has tests
- [ ] **Error Path Coverage**: All error scenarios tested
- [ ] **Integration Coverage**: Component interactions tested

**Test Quality Checklist:**
- [ ] **Test Isolation**: Tests don't depend on each other
- [ ] **Deterministic**: Tests produce consistent results
- [ ] **Fast Execution**: Unit tests complete in <1 second each
- [ ] **Clear Failure Messages**: Test failures explain what went wrong
- [ ] **Maintainable**: Tests are easy to update when code changes

## 🚨 Quality Gates & Monitoring

```
🚨 RED FLAGS (stop development immediately):
- **ANY test failures** in foundational components or new features
- **Proceeding to next phase** with failing tests
- Unclear error messages or poor error handling
- Performance degradation >10% from baseline
- Breaking changes without migration strategy
- Undocumented public APIs or missing descriptions
- Missing or incomplete system diagrams
- Implementation proceeding without design review completion
- **Over-engineering**: Adding complexity not justified by current requirements
- **Self-explanatory code fallacy**: Relying on code alone without explaining motivations and design decisions

✅ PROCEED CRITERIA (all must be met):
- All phase tests pass (100% success rate)
- Error messages clear, actionable, and well-tested  
- Documentation matches implementation with explanatory text
- Performance benchmarks met or exceeded
- System diagrams accurately reflect implementation
- Design review checklist fully completed
- Implementation progress accurately tracked
- Examples created and validated (Phase 6)
- Documentation cites working examples (Phase 6)
```

## KISS/YAGNI Design Principles

**KISS (Keep It Simple, Stupid)** & **YAGNI (You Aren't Gonna Need It)**: Balance engineering rigor with practical simplicity.

### AI Decision-Making Guidelines
```
🎯 **START SIMPLE, EVOLVE THOUGHTFULLY**

For design decisions, AI coders should:
1. **Default to simplest solution** that meets current requirements
2. **Document complexity trade-offs** when proposing alternatives  
3. **Present options** when multiple approaches have merit
4. **Justify complexity** only when immediate needs require it

🤖 **AI CAN DECIDE** (choose simplest):
- Data structure choice (dict vs class vs dataclass)
- Function organization (single file vs module split)
- Error handling level (basic vs comprehensive)
- Documentation depth (minimal vs extensive)

👤 **PRESENT TO HUMAN** (let them choose):
- Architecture patterns (monolith vs microservices)
- Framework choices (custom vs third-party)
- Performance optimizations (simple vs complex)
- Extensibility mechanisms (hardcoded vs configurable)

⚖️ **COMPLEXITY JUSTIFICATION TEMPLATE**:
"Proposing [complex solution] over [simple solution] because:
- Current requirement: [specific need]
- Simple approach limitation: [concrete issue]
- Complexity benefit: [measurable advantage]
- Alternative: [let human decide vs simpler approach]"
```

### Common Over-Engineering Patterns to Avoid
```
❌ AVOID (unless specifically needed):
- Abstract base classes for single implementations
- Configuration systems for hardcoded values
- Generic solutions for specific problems
- Premature performance optimizations
- Complex inheritance hierarchies
- Over-flexible APIs with many parameters
- Caching systems without proven performance needs
- Event systems for simple function calls

✅ PREFER (start here):
- Concrete implementations that work
- Hardcoded values that can be extracted later
- Specific solutions for specific problems
- Simple, readable code first
- Composition over inheritance
- Simple function signatures
- Direct computation until performance matters
- Direct function calls for simple interactions
```

### Incremental Complexity Strategy
```
📈 **EVOLUTION PATH** (add complexity only when needed):

Phase 1: Hardcoded → Phase 2: Configurable → Phase 3: Extensible

Example:
Phase 1: `return "Hello, World!"`
Phase 2: `return f"Hello, {name}!"`
Phase 3: `return formatter.format(greeting_template, name)`

🔄 **WHEN TO EVOLVE**:
- Phase 1→2: When second use case appears
- Phase 2→3: When third different pattern emerges
- Never evolve: If usage remains stable
```

## 📊 AI Supervisor Monitoring Points

For AI supervisors tracking 3D compliance:

1. **Design Phase Checkpoints**:
   - [ ] Design document exists and follows template
   - [ ] Brief descriptions provided for all major sections
   - [ ] System architecture diagram created (ASCII/Mermaid)
   - [ ] Design review checklist completed
   
2. **Implementation Phase Checkpoints**:
   - [ ] Test suite passing at end of each phase
   - [ ] Implementation progress checkboxes updated
   - [ ] Code includes explanatory comments, not just fragments
   - [ ] Error handling comprehensive and tested

3. **Examples & Documentation Checkpoints (Phase 6)**:
   - [ ] Examples created following guidelines
   - [ ] Examples validated with automated tests
   - [ ] Documentation cites working examples
   - [ ] User-facing docs complete and accurate

4. **Quality Assurance Checkpoints**:
   - [ ] No regressions detected in test runs
   - [ ] Performance benchmarks maintained
   - [ ] Documentation complete with descriptive explanations
   - [ ] Integration diagrams reflect actual system behavior

## 🤖 AI Coder Execution Guidelines

For AI coders implementing 3D methodology:

1. **Before Starting**: Always create design document first, never skip
2. **During Design**: Include brief descriptions AND diagrams, not just code
3. **During Implementation**: Run `uv run pytest tests/ -v` after each phase
4. **Phase Transitions**: Update checkboxes and verify all criteria met
5. **Problem Solving**: Create diagrams to visualize complex relationships
6. **Documentation**: Write explanations and motivations, not just code examples
7. **Testing**: Fix ALL test failures immediately - ZERO tolerance for proceeding with failures
8. **Phase Gates**: Run `uv run pytest tests/ -v` at end of every phase - 100% pass required
9. **Design Decisions**: Apply KISS/YAGNI - start simple, present complex alternatives to humans
10. **Examples Creation**: Follow Example Creation Guidelines in Phase 6 only
11. **Test Creation**: Follow Unit Testing Guidelines for comprehensive test coverage
12. **Documentation Strategy**: Create docs that cite examples, organize by feature size 
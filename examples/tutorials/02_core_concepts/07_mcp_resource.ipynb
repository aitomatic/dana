{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCPResource in OpenDXA\n",
    "\n",
    "This tutorial covers the Model Context Protocol (MCP) resource in OpenDXA, which provides a standardized way to integrate external services and tools into your agents.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "\n",
    "1. What MCP is and how it works\n",
    "2. How to create and use MCP resources\n",
    "3. How to work with different transport types (STDIO and HTTP)\n",
    "4. How to discover and use MCP tools\n",
    "5. Best practices for MCP resource usage\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of OpenDXA's architecture\n",
    "- Familiarity with Python async/await syntax\n",
    "- Understanding of basic resource management concepts\n",
    "\n",
    "## 1. Understanding MCP\n",
    "\n",
    "The Model Context Protocol (MCP) is a standardized way to expose data and functionality to LLM applications. MCP servers can:\n",
    "\n",
    "1. **Expose Data**: Through resources (similar to GET endpoints)\n",
    "2. **Provide Functionality**: Through tools (similar to POST endpoints)\n",
    "3. **Define Interaction Patterns**: Through prompts (reusable templates)\n",
    "\n",
    "Let's start by creating a simple MCP resource. First letâ€™s install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install @modelcontextprotocol/server-filesystem\n",
    "!npm install @modelcontextprotocol/server-brave-search\n",
    "!npm install @modelcontextprotocol/server-sequential-thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool(name='read_file', description='Read the complete contents of a file from the file system. Handles various text encodings and provides detailed error messages if the file cannot be read. Use this tool when you need to examine the contents of a single file. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}}, 'required': ['path'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='read_multiple_files', description=\"Read the contents of multiple files simultaneously. This is more efficient than reading files one by one when you need to analyze or compare multiple files. Each file's content is returned with its path as a reference. Failed reads for individual files won't stop the entire operation. Only works within allowed directories.\", inputSchema={'type': 'object', 'properties': {'paths': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['paths'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='write_file', description='Create a new file or completely overwrite an existing file with new content. Use with caution as it will overwrite existing files without warning. Handles text content with proper encoding. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}, 'content': {'type': 'string'}}, 'required': ['path', 'content'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='edit_file', description='Make line-based edits to a text file. Each edit replaces exact line sequences with new content. Returns a git-style diff showing the changes made. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}, 'edits': {'type': 'array', 'items': {'type': 'object', 'properties': {'oldText': {'type': 'string', 'description': 'Text to search for - must match exactly'}, 'newText': {'type': 'string', 'description': 'Text to replace with'}}, 'required': ['oldText', 'newText'], 'additionalProperties': False}}, 'dryRun': {'type': 'boolean', 'default': False, 'description': 'Preview changes using git-style diff format'}}, 'required': ['path', 'edits'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='create_directory', description='Create a new directory or ensure a directory exists. Can create multiple nested directories in one operation. If the directory already exists, this operation will succeed silently. Perfect for setting up directory structures for projects or ensuring required paths exist. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}}, 'required': ['path'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='list_directory', description='Get a detailed listing of all files and directories in a specified path. Results clearly distinguish between files and directories with [FILE] and [DIR] prefixes. This tool is essential for understanding directory structure and finding specific files within a directory. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}}, 'required': ['path'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='directory_tree', description=\"Get a recursive tree view of files and directories as a JSON structure. Each entry includes 'name', 'type' (file/directory), and 'children' for directories. Files have no children array, while directories always have a children array (which may be empty). The output is formatted with 2-space indentation for readability. Only works within allowed directories.\", inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}}, 'required': ['path'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='move_file', description='Move or rename files and directories. Can move files between directories and rename them in a single operation. If the destination exists, the operation will fail. Works across different directories and can be used for simple renaming within the same directory. Both source and destination must be within allowed directories.', inputSchema={'type': 'object', 'properties': {'source': {'type': 'string'}, 'destination': {'type': 'string'}}, 'required': ['source', 'destination'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='search_files', description=\"Recursively search for files and directories matching a pattern. Searches through all subdirectories from the starting path. The search is case-insensitive and matches partial names. Returns full paths to all matching items. Great for finding files when you don't know their exact location. Only searches within allowed directories.\", inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}, 'pattern': {'type': 'string'}, 'excludePatterns': {'type': 'array', 'items': {'type': 'string'}, 'default': []}}, 'required': ['path', 'pattern'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='get_file_info', description='Retrieve detailed metadata about a file or directory. Returns comprehensive information including size, creation time, last modified time, permissions, and type. This tool is perfect for understanding file characteristics without reading the actual content. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}}, 'required': ['path'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='list_allowed_directories', description='Returns the list of directories that this server is allowed to access. Use this to understand which directories are available before trying to access files.', inputSchema={'type': 'object', 'properties': {}, 'required': []})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:35:25 - [opendxa.common.resource.mcp.mcp_resource.McpResource] ERROR - Tool listing failed: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/ctn/src/aitomatic/opendxa/opendxa/common/resource/mcp/mcp_resource.py\", line 397, in list_tools\n",
      "  |     async with stdio_client(self.server_params) as (read, write):\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/ctn/.pyenv/versions/3.12.7/lib/python3.12/contextlib.py\", line 231, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"/Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/mcp/client/stdio/__init__.py\", line 167, in stdio_client\n",
      "  |     anyio.create_task_group() as tg,\n",
      "  |     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/mcp/client/stdio/__init__.py\", line 146, in stdout_reader\n",
      "    |     await read_stream_writer.send(message)\n",
      "    |   File \"/Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/anyio/streams/memory.py\", line 242, in send\n",
      "    |     self.send_nowait(item)\n",
      "    |   File \"/Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/anyio/streams/memory.py\", line 213, in send_nowait\n",
      "    |     raise BrokenResourceError\n",
      "    | anyio.BrokenResourceError\n",
      "    +------------------------------------\n",
      "22:35:25 - [opendxa.execution.workflow] INFO - Starting workflow execution with 3 nodes\n",
      "22:35:25 - [opendxa.execution.workflow] INFO - Executing workflow node: START\n",
      "22:35:25 - [opendxa.execution.workflow] INFO - Executing workflow node: TASK_0\n",
      "22:35:25 - [opendxa.execution.plan] INFO - Starting plan execution with 3 nodes\n",
      "22:35:25 - [opendxa.execution.plan] INFO - Executing plan node: START\n",
      "22:35:25 - [opendxa.execution.plan] INFO - Executing plan node: TASK\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO - Starting reasoning execution with 3 nodes\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO - Executing reasoning node: START\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO - Executing reasoning node: TASK\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO - \n",
      "Execution Context:\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO - =================\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO - Workflow: NodeType.TASK - query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO -   Objective: Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO -   Plan: NodeType.TASK - query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO -     Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO -     Reasoning: NodeType.TASK - query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO -       Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\n",
      "22:35:25 - [opendxa.execution.reasoning] DEBUG - Reasoning Strategy: ReasoningStrategy.DEFAULT\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO - Prompt:\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO - =======\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO - ['WORKFLOW OVERVIEW:', '- Overall Workflow Objective: Can you reason out what I do for a living from my files? You can use ONE level of subdirs.', '- Overall Plan Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.', '', 'EXECUTION GRAPH:', '1. NodeType.START: Start node', '   - Objective: Start node', '   - Status: ExecutionNodeStatus.SKIPPED', '2. NodeType.TASK: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. [CURRENT]', '   - Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.', '   - Status: ExecutionNodeStatus.IN_PROGRESS', '', '   Plan Sequence:', '   1. NodeType.START: Start node', '      - Objective: Start node', '      - Status: ExecutionNodeStatus.SKIPPED', '   2. NodeType.TASK: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. [CURRENT]', '      - Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.', '      - Status: ExecutionNodeStatus.IN_PROGRESS', '   3. NodeType.END: End node', '      - Objective: End node', '      - Status: ExecutionNodeStatus.NONE', '3. NodeType.END: End node', '   - Objective: End node', '   - Status: ExecutionNodeStatus.NONE', '', 'CURRENT EXECUTION CONTEXT:', '- Workflow: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. (ExecutionNodeStatus.IN_PROGRESS)', '- Plan: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. (ExecutionNodeStatus.IN_PROGRESS)']\n",
      "22:35:25 - [opendxa.execution.reasoning] INFO - Resources: {'filesystem': McpResource(), 'sequential-thinking': McpResource()}\n",
      "22:35:25 - [opendxa.common.resource.llm_resource.LLMResource] INFO - Resource calling iteration 0/10\n",
      "22:35:25 - [opendxa.common.resource.llm_resource.LLMResource] INFO - LLM client initialized successfully for model: deepseek:deepseek-coder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:35:27 - [opendxa.common.resource.llm_resource.LLMResource] INFO - REQUEST TO deepseek:deepseek-coder:\n",
      "--------------------------------------------------------------------------------\n",
      "[{'role': 'system', 'content': \"You are executing a reasoning task. Provide clear, logical analysis and reasoning.\\nYou are operating within a three-layer execution hierarchy: Workflow -> Plan -> Reasoning.\\nThe Workflow layer is typically specified by the human operator\\nThe Plan layer is typically generated dynamically to accomplish the objective\\nThe Reasoning layer is typically a choice of several fundamental strategies, e.g., chain-of-thought, tree-of-thought, reflection, OODA loop, etc.\\n\\nYou are currently in the Reasoning layer. Execute the reasoning task while keeping in mind:\\n 1. The broader workflow context and its objectives\\n 2. The specific plan that this reasoning task is part of\\n 3. The immediate reasoning task requirements\\n\\nHere is your reasoning strategy:\\nReasoning: Following chain-of-thought strategy:\\n  1. Break down the problem into sequential steps\\n  2. Analyze each step carefully and thoroughly\\n  3. Connect the steps logically to form a coherent solution\\n  4. Verify the solution's completeness and correctness\\nRepeat the above process exactly 3 times, or until you are confident that the task is complete and the objective is met.At the end, you must always provide an assessement of whether the objective has been met or not.\"}, {'role': 'user', 'content': 'WORKFLOW OVERVIEW:\\n- Overall Workflow Objective: Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n- Overall Plan Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n\\nEXECUTION GRAPH:\\n1. NodeType.START: Start node\\n   - Objective: Start node\\n   - Status: ExecutionNodeStatus.SKIPPED\\n2. NodeType.TASK: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. [CURRENT]\\n   - Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n   - Status: ExecutionNodeStatus.IN_PROGRESS\\n\\n   Plan Sequence:\\n   1. NodeType.START: Start node\\n      - Objective: Start node\\n      - Status: ExecutionNodeStatus.SKIPPED\\n   2. NodeType.TASK: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. [CURRENT]\\n      - Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n      - Status: ExecutionNodeStatus.IN_PROGRESS\\n   3. NodeType.END: End node\\n      - Objective: End node\\n      - Status: ExecutionNodeStatus.NONE\\n3. NodeType.END: End node\\n   - Objective: End node\\n   - Status: ExecutionNodeStatus.NONE\\n\\nCURRENT EXECUTION CONTEXT:\\n- Workflow: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. (ExecutionNodeStatus.IN_PROGRESS)\\n- Plan: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. (ExecutionNodeStatus.IN_PROGRESS)'}]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/13/25 22:35:28] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.deepseek.com/chat/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1</span> <a href=\"file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py#1038\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1038</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">200 OK\"</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/13/25 22:35:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.deepseek.com/chat/completions\u001b[0m \u001b[32m\"HTTP/1.1\u001b[0m \u001b]8;id=974629;file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=139747;file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py#1038\u001b\\\u001b[2m1038\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m200 OK\"\u001b[0m                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:35:49 - [opendxa.common.resource.llm_resource.LLMResource] INFO - RESPONSE FROM deepseek:deepseek-coder:\n",
      "--------------------------------------------------------------------------------\n",
      "ChatCompletion(id='9be2e7bb-7242-415d-982b-d31b009d89ed', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=\"To reason out what you do for a living from your files, I will follow a structured approach:\\n\\n### Step 1: List the Directories and Files\\nFirst, I need to examine the directory structure and files in your workspace to gather clues about your profession. Since I can only use one level of subdirectories, I will focus on the immediate files and directories in the root or specified path.\\n\\n### Step 2: Analyze File Names and Content\\nNext, I will analyze the names of the files and, if possible, their contents to infer patterns or keywords that might indicate your profession. For example:\\n- File names like `project_plan.docx`, `code_repository`, or `financial_report.xlsx` could hint at specific fields.\\n- Subdirectory names like `clients`, `designs`, or `research` might also provide context.\\n\\n### Step 3: Infer Profession\\nBased on the analysis, I will infer your profession by connecting the dots between the file names, subdirectory names, and any content snippets. For example:\\n- Many code-related files might suggest you're a software developer.\\n- Financial spreadsheets and reports could indicate an accountant or financial analyst.\\n- Design files might point to a graphic designer or architect.\\n\\n### Step 4: Verify the Inference\\nFinally, I will verify the inference by cross-referencing the findings with common professional profiles to ensure the conclusion is logical.\\n\\n### Execution\\nI will now proceed to list the directories and files in the allowed workspace to begin this analysis.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_0_0fe41dc3-25d6-4644-b2ce-34f0ecde3132', function=Function(arguments='{}', name='filesystem__a45531ec__list_allowed_directories'), type='function', index=0)]))], created=1744558528, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_3d5141a69a_prod0225', usage=CompletionUsage(completion_tokens=333, prompt_tokens=3622, total_tokens=3955, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=0), prompt_cache_hit_tokens=0, prompt_cache_miss_tokens=3622))\n",
      "--------------------------------------------------------------------------------\n",
      "22:35:49 - [opendxa.common.resource.llm_resource.LLMResource] INFO - USAGE: prompt_tokens=3622, \n",
      "22:35:49 - [opendxa.common.resource.llm_resource.LLMResource] INFO - LLM is requesting tools, storing tool request message and calling resources\n",
      "22:35:49 - [opendxa.common.resource.llm_resource.LLMResource] INFO - Resource name: filesystem, resource id: a45531ec, function name: list_allowed_directories\n",
      "22:35:50 - [opendxa.common.resource.llm_resource.LLMResource] INFO - Resource calling iteration 1/10\n",
      "22:35:51 - [opendxa.common.resource.llm_resource.LLMResource] INFO - REQUEST TO deepseek:deepseek-coder:\n",
      "--------------------------------------------------------------------------------\n",
      "[{'role': 'system', 'content': \"You are executing a reasoning task. Provide clear, logical analysis and reasoning.\\nYou are operating within a three-layer execution hierarchy: Workflow -> Plan -> Reasoning.\\nThe Workflow layer is typically specified by the human operator\\nThe Plan layer is typically generated dynamically to accomplish the objective\\nThe Reasoning layer is typically a choice of several fundamental strategies, e.g., chain-of-thought, tree-of-thought, reflection, OODA loop, etc.\\n\\nYou are currently in the Reasoning layer. Execute the reasoning task while keeping in mind:\\n 1. The broader workflow context and its objectives\\n 2. The specific plan that this reasoning task is part of\\n 3. The immediate reasoning task requirements\\n\\nHere is your reasoning strategy:\\nReasoning: Following chain-of-thought strategy:\\n  1. Break down the problem into sequential steps\\n  2. Analyze each step carefully and thoroughly\\n  3. Connect the steps logically to form a coherent solution\\n  4. Verify the solution's completeness and correctness\\nRepeat the above process exactly 3 times, or until you are confident that the task is complete and the objective is met.At the end, you must always provide an assessement of whether the objective has been met or not.\"}, {'role': 'user', 'content': 'WORKFLOW OVERVIEW:\\n- Overall Workflow Objective: Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n- Overall Plan Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n\\nEXECUTION GRAPH:\\n1. NodeType.START: Start node\\n   - Objective: Start node\\n   - Status: ExecutionNodeStatus.SKIPPED\\n2. NodeType.TASK: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. [CURRENT]\\n   - Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n   - Status: ExecutionNodeStatus.IN_PROGRESS\\n\\n   Plan Sequence:\\n   1. NodeType.START: Start node\\n      - Objective: Start node\\n      - Status: ExecutionNodeStatus.SKIPPED\\n   2. NodeType.TASK: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. [CURRENT]\\n      - Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n      - Status: ExecutionNodeStatus.IN_PROGRESS\\n   3. NodeType.END: End node\\n      - Objective: End node\\n      - Status: ExecutionNodeStatus.NONE\\n3. NodeType.END: End node\\n   - Objective: End node\\n   - Status: ExecutionNodeStatus.NONE\\n\\nCURRENT EXECUTION CONTEXT:\\n- Workflow: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. (ExecutionNodeStatus.IN_PROGRESS)\\n- Plan: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. (ExecutionNodeStatus.IN_PROGRESS)'}, {'role': 'assistant', 'content': \"To reason out what you do for a living from your files, I will follow a structured approach:\\n\\n### Step 1: List the Directories and Files\\nFirst, I need to examine the directory structure and files in your workspace to gather clues about your profession. Since I can only use one level of subdirectories, I will focus on the immediate files and directories in the root or specified path.\\n\\n### Step 2: Analyze File Names and Content\\nNext, I will analyze the names of the files and, if possible, their contents to infer patterns or keywords that might indicate your profession. For example:\\n- File names like `project_plan.docx`, `code_repository`, or `financial_report.xlsx` could hint at specific fields.\\n- Subdirectory names like `clients`, `designs`, or `research` might also provide context.\\n\\n### Step 3: Infer Profession\\nBased on the analysis, I will infer your profession by connecting the dots between the file names, subdirectory names, and any content snippets. For example:\\n- Many code-related files might suggest you're a software developer.\\n- Financial spreadsheets and reports could indicate an accountant or financial analyst.\\n- Design files might point to a graphic designer or architect.\\n\\n### Step 4: Verify the Inference\\nFinally, I will verify the inference by cross-referencing the findings with common professional profiles to ensure the conclusion is logical.\\n\\n### Execution\\nI will now proceed to list the directories and files in the allowed workspace to begin this analysis.\", 'tool_calls': [ChatCompletionMessageToolCall(id='call_0_0fe41dc3-25d6-4644-b2ce-34f0ecde3132', function=Function(arguments='{}', name='filesystem__a45531ec__list_allowed_directories'), type='function', index=0)]}, {'role': 'tool', 'tool_call_id': 'call_0_0fe41dc3-25d6-4644-b2ce-34f0ecde3132', 'content': 'Allowed directories:\\n/Users/ctn'}]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/13/25 22:35:52] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.deepseek.com/chat/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1</span> <a href=\"file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py#1038\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1038</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">200 OK\"</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/13/25 22:35:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.deepseek.com/chat/completions\u001b[0m \u001b[32m\"HTTP/1.1\u001b[0m \u001b]8;id=449181;file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=852908;file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py#1038\u001b\\\u001b[2m1038\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m200 OK\"\u001b[0m                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:35:56 - [opendxa.common.resource.llm_resource.LLMResource] INFO - RESPONSE FROM deepseek:deepseek-coder:\n",
      "--------------------------------------------------------------------------------\n",
      "ChatCompletion(id='b589f4b0-ca71-498d-a026-a76d86e45f73', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_0_0f928a04-ded9-49a7-a698-2993a3f3b32e', function=Function(arguments='{\"path\":\"/Users/ctn\"}', name='filesystem__a45531ec__list_directory'), type='function', index=0)]))], created=1744558551, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_3d5141a69a_prod0225', usage=CompletionUsage(completion_tokens=30, prompt_tokens=3968, total_tokens=3998, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=3904), prompt_cache_hit_tokens=3904, prompt_cache_miss_tokens=64))\n",
      "--------------------------------------------------------------------------------\n",
      "22:35:56 - [opendxa.common.resource.llm_resource.LLMResource] INFO - USAGE: prompt_tokens=3968, \n",
      "22:35:56 - [opendxa.common.resource.llm_resource.LLMResource] INFO - LLM is requesting tools, storing tool request message and calling resources\n",
      "22:35:56 - [opendxa.common.resource.llm_resource.LLMResource] INFO - Resource name: filesystem, resource id: a45531ec, function name: list_directory\n",
      "22:35:57 - [opendxa.common.resource.llm_resource.LLMResource] INFO - Resource calling iteration 2/10\n",
      "22:35:59 - [opendxa.common.resource.llm_resource.LLMResource] INFO - REQUEST TO deepseek:deepseek-coder:\n",
      "--------------------------------------------------------------------------------\n",
      "[{'role': 'system', 'content': \"You are executing a reasoning task. Provide clear, logical analysis and reasoning.\\nYou are operating within a three-layer execution hierarchy: Workflow -> Plan -> Reasoning.\\nThe Workflow layer is typically specified by the human operator\\nThe Plan layer is typically generated dynamically to accomplish the objective\\nThe Reasoning layer is typically a choice of several fundamental strategies, e.g., chain-of-thought, tree-of-thought, reflection, OODA loop, etc.\\n\\nYou are currently in the Reasoning layer. Execute the reasoning task while keeping in mind:\\n 1. The broader workflow context and its objectives\\n 2. The specific plan that this reasoning task is part of\\n 3. The immediate reasoning task requirements\\n\\nHere is your reasoning strategy:\\nReasoning: Following chain-of-thought strategy:\\n  1. Break down the problem into sequential steps\\n  2. Analyze each step carefully and thoroughly\\n  3. Connect the steps logically to form a coherent solution\\n  4. Verify the solution's completeness and correctness\\nRepeat the above process exactly 3 times, or until you are confident that the task is complete and the objective is met.At the end, you must always provide an assessement of whether the objective has been met or not.\"}, {'role': 'user', 'content': 'WORKFLOW OVERVIEW:\\n- Overall Workflow Objective: Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n- Overall Plan Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n\\nEXECUTION GRAPH:\\n1. NodeType.START: Start node\\n   - Objective: Start node\\n   - Status: ExecutionNodeStatus.SKIPPED\\n2. NodeType.TASK: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. [CURRENT]\\n   - Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n   - Status: ExecutionNodeStatus.IN_PROGRESS\\n\\n   Plan Sequence:\\n   1. NodeType.START: Start node\\n      - Objective: Start node\\n      - Status: ExecutionNodeStatus.SKIPPED\\n   2. NodeType.TASK: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. [CURRENT]\\n      - Objective: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\\n      - Status: ExecutionNodeStatus.IN_PROGRESS\\n   3. NodeType.END: End node\\n      - Objective: End node\\n      - Status: ExecutionNodeStatus.NONE\\n3. NodeType.END: End node\\n   - Objective: End node\\n   - Status: ExecutionNodeStatus.NONE\\n\\nCURRENT EXECUTION CONTEXT:\\n- Workflow: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. (ExecutionNodeStatus.IN_PROGRESS)\\n- Plan: query in Can you reason out what I do for a living from my files? You can use ONE level of subdirs. (ExecutionNodeStatus.IN_PROGRESS)'}, {'role': 'assistant', 'content': \"To reason out what you do for a living from your files, I will follow a structured approach:\\n\\n### Step 1: List the Directories and Files\\nFirst, I need to examine the directory structure and files in your workspace to gather clues about your profession. Since I can only use one level of subdirectories, I will focus on the immediate files and directories in the root or specified path.\\n\\n### Step 2: Analyze File Names and Content\\nNext, I will analyze the names of the files and, if possible, their contents to infer patterns or keywords that might indicate your profession. For example:\\n- File names like `project_plan.docx`, `code_repository`, or `financial_report.xlsx` could hint at specific fields.\\n- Subdirectory names like `clients`, `designs`, or `research` might also provide context.\\n\\n### Step 3: Infer Profession\\nBased on the analysis, I will infer your profession by connecting the dots between the file names, subdirectory names, and any content snippets. For example:\\n- Many code-related files might suggest you're a software developer.\\n- Financial spreadsheets and reports could indicate an accountant or financial analyst.\\n- Design files might point to a graphic designer or architect.\\n\\n### Step 4: Verify the Inference\\nFinally, I will verify the inference by cross-referencing the findings with common professional profiles to ensure the conclusion is logical.\\n\\n### Execution\\nI will now proceed to list the directories and files in the allowed workspace to begin this analysis.\", 'tool_calls': [ChatCompletionMessageToolCall(id='call_0_0fe41dc3-25d6-4644-b2ce-34f0ecde3132', function=Function(arguments='{}', name='filesystem__a45531ec__list_allowed_directories'), type='function', index=0)]}, {'role': 'tool', 'tool_call_id': 'call_0_0fe41dc3-25d6-4644-b2ce-34f0ecde3132', 'content': 'Allowed directories:\\n/Users/ctn'}, {'role': 'assistant', 'content': '', 'tool_calls': [ChatCompletionMessageToolCall(id='call_0_0f928a04-ded9-49a7-a698-2993a3f3b32e', function=Function(arguments='{\"path\":\"/Users/ctn\"}', name='filesystem__a45531ec__list_directory'), type='function', index=0)]}, {'role': 'tool', 'tool_call_id': 'call_0_0f928a04-ded9-49a7-a698-2993a3f3b32e', 'content': '[DIR] .BZG\\n[FILE] .CFUserTextEncoding\\n[FILE] .DS_Store\\n[FILE] .MJKJDeviceGUID\\n[FILE] .MJKJRegInfo_WLHF6G5GDO5A5AUT6EURO6RMAKGWUK6T\\n[DIR] .Trash\\n[DIR] .aider\\n[FILE] .bash_history\\n[FILE] .bash_profile\\n[DIR] .bash_sessions\\n[FILE] .bashrc\\n[DIR] .bridge\\n[DIR] .cache\\n[DIR] .claude\\n[FILE] .claude.json\\n[DIR] .conda\\n[DIR] .config\\n[DIR] .cups\\n[DIR] .cursor\\n[DIR] .cursor-tutor\\n[DIR] .docker\\n[DIR] .dropbox\\n[FILE] .gitconfig\\n[DIR] .gk\\n[DIR] .hashcat\\n[DIR] .ipynb_checkpoints\\n[DIR] .ipython\\n[DIR] .jupyter\\n[DIR] .keras\\n[DIR] .kube\\n[FILE] .lesshst\\n[DIR] .local\\n[DIR] .matplotlib\\n[DIR] .minikube\\n[DIR] .npm\\n[DIR] .ollama\\n[DIR] .oracle_jre_usage\\n[DIR] .platformsh\\n[DIR] .pyenv\\n[FILE] .python_history\\n[DIR] .redhat\\n[DIR] .ssh\\n[DIR] .step\\n[DIR] .streamlit\\n[DIR] .tmp-wework-printers\\n[DIR] .vim\\n[FILE] .viminfo\\n[FILE] .vimrc\\n[DIR] .vs-kubernetes\\n[DIR] .vscode\\n[DIR] .warp\\n[DIR] .zoomus\\n[FILE] .zsh_history\\n[DIR] .zsh_sessions\\n[FILE] .zshrc\\n[FILE] .zshrc.unused\\n[FILE] 190911 FUV Follow-Ups.docx\\n[FILE] 192.168.9.10\\n[FILE] 2019-08-31-Panasonic-Kaiser.pdf\\n[DIR] Applications\\n[DIR] CTN64G\\n[FILE] Cathay-KIX-HAN-2.pdf\\n[DIR] Desktop\\n[DIR] Diagrams\\n[DIR] Documents\\n[DIR] Downloads\\n[DIR] Dropbox\\n[DIR] Dropbox (Old)\\n[DIR] FUV\\n[FILE] HFAI Framework Design Testing.ipynb\\n[DIR] Library\\n[DIR] Movies\\n[DIR] Music\\n[DIR] P-DRIVE\\n[DIR] Pagico Data.pgco\\n[DIR] Panasonic\\n[DIR] Pictures\\n[DIR] Public\\n[DIR] PycharmProjects\\n[DIR] RealID\\n[DIR] Samsung ctnzf3\\n[DIR] Sites\\n[DIR] Spotless\\n[FILE] adatao\\n[FILE] adatao alias\\n[DIR] bin\\n[FILE] cactgent@gmail.com - Google Drive\\n[DIR] catgent\\n[FILE] ctn@cleartransport.org - Google Drive\\n[DIR] downloadtemp\\n[DIR] examples\\n[FILE] google-cloud-cli-437.0.1-darwin-arm.tar.gz\\n[DIR] google-cloud-sdk\\n[DIR] google-minis\\n[FILE] id_ed25519.pub\\n[DIR] miniforge3\\n[DIR] miniforge3.unused\\n[DIR] node_modules\\n[DIR] old\\n[FILE] package-lock.json\\n[FILE] package.json\\n[DIR] pinokio\\n[FILE] poetry-installer-error-qt5n_j0p.log\\n[FILE] requirements.txt\\n[DIR] src\\n[FILE] t\\n[DIR] tests\\n[DIR] tmp\\n[FILE] wip'}]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/13/25 22:35:59] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.deepseek.com/chat/completions</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1</span> <a href=\"file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py#1038\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1038</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">200 OK\"</span>                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/13/25 22:35:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.deepseek.com/chat/completions\u001b[0m \u001b[32m\"HTTP/1.1\u001b[0m \u001b]8;id=924254;file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=324251;file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py#1038\u001b\\\u001b[2m1038\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m200 OK\"\u001b[0m                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:36:32 - [opendxa.common.resource.llm_resource.LLMResource] INFO - RESPONSE FROM deepseek:deepseek-coder:\n",
      "--------------------------------------------------------------------------------\n",
      "ChatCompletion(id='8303791e-285a-4b0b-97ed-ad7614266253', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"From the directory listing, here's the analysis to infer your profession:\\n\\n### Step 1: Key Observations\\n1. **Technical Files and Directories**:\\n   - `.bashrc`, `.bash_profile`, `.gitconfig`, `.vimrc`, `.zshrc`: These are configuration files for development tools like Bash, Git, Vim, and Zsh, indicating familiarity with programming and system administration.\\n   - `PycharmProjects`, `.vscode`, `src`, `tests`, `node_modules`, `package.json`, `requirements.txt`: These suggest active involvement in software development, possibly using Python (Pycharm, requirements.txt) and JavaScript/Node.js (node_modules, package.json).\\n   - `HFAI Framework Design Testing.ipynb`: A Jupyter Notebook file, often used in data science, machine learning, or research.\\n\\n2. **Professional Documents**:\\n   - `190911 FUV Follow-Ups.docx`, `Cathay-KIX-HAN-2.pdf`, `2019-08-31-Panasonic-Kaiser.pdf`: These files hint at professional communication, possibly in roles involving project management, consulting, or technical documentation.\\n\\n3. **Cloud and DevOps**:\\n   - `.docker`, `.kube`, `.minikube`, `google-cloud-sdk`: These indicate experience with containerization (Docker), Kubernetes, and Google Cloud Platform, pointing towards DevOps or cloud engineering.\\n\\n4. **Other Clues**:\\n   - `Dropbox`, `Google Drive`: Collaboration and file-sharing tools, common in remote or hybrid work environments.\\n   - `Diagrams`: Could imply roles requiring visual documentation, such as software architecture or UX design.\\n\\n### Step 2: Inference\\nThe presence of development tools, cloud technologies, and data science-related files strongly suggests a profession in **software development, data science, or DevOps**. The Jupyter Notebook (`HFAI Framework Design Testing.ipynb`) and Python-related files further narrow it down to roles involving **data analysis, machine learning, or AI development**.\\n\\n### Step 3: Verification\\nThe combination of:\\n- Development tools (Git, Vim, Pycharm, VSCode).\\n- Cloud and DevOps tools (Docker, Kubernetes, GCP).\\n- Data science files (Jupyter Notebook).\\n- Professional documents (likely related to projects or clients).\\n\\nThis aligns well with professions like:\\n- **Data Scientist** (focus on AI/ML frameworks).\\n- **Software Engineer** (full-stack or backend, given the variety of tools).\\n- **DevOps Engineer** (given the cloud and containerization tools).\\n\\n### Final Answer\\nBased on the files and directories, you likely work in a **technical role such as Software Engineer, Data Scientist, or DevOps Engineer**, with a focus on Python, cloud technologies, and possibly AI/ML frameworks.\\n\\n**Objective Met**: Yes, the analysis provides a reasonable inference about your profession.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1744558559, model='deepseek-chat', object='chat.completion', service_tier=None, system_fingerprint='fp_3d5141a69a_prod0225', usage=CompletionUsage(completion_tokens=596, prompt_tokens=4854, total_tokens=5450, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=3968), prompt_cache_hit_tokens=3968, prompt_cache_miss_tokens=886))\n",
      "--------------------------------------------------------------------------------\n",
      "22:36:32 - [opendxa.common.resource.llm_resource.LLMResource] INFO - USAGE: prompt_tokens=4854, \n",
      "22:36:32 - [opendxa.common.resource.llm_resource.LLMResource] INFO - LLM is not requesting tools, returning final response\n",
      "22:36:32 - [opendxa.execution.reasoning] INFO - \n",
      "Reasoning Result:\n",
      "22:36:32 - [opendxa.execution.reasoning] INFO - ================\n",
      "22:36:32 - [opendxa.execution.reasoning] INFO - {'choices': [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"From the directory listing, here's the analysis to infer your profession:\\n\\n### Step 1: Key Observations\\n1. **Technical Files and Directories**:\\n   - `.bashrc`, `.bash_profile`, `.gitconfig`, `.vimrc`, `.zshrc`: These are configuration files for development tools like Bash, Git, Vim, and Zsh, indicating familiarity with programming and system administration.\\n   - `PycharmProjects`, `.vscode`, `src`, `tests`, `node_modules`, `package.json`, `requirements.txt`: These suggest active involvement in software development, possibly using Python (Pycharm, requirements.txt) and JavaScript/Node.js (node_modules, package.json).\\n   - `HFAI Framework Design Testing.ipynb`: A Jupyter Notebook file, often used in data science, machine learning, or research.\\n\\n2. **Professional Documents**:\\n   - `190911 FUV Follow-Ups.docx`, `Cathay-KIX-HAN-2.pdf`, `2019-08-31-Panasonic-Kaiser.pdf`: These files hint at professional communication, possibly in roles involving project management, consulting, or technical documentation.\\n\\n3. **Cloud and DevOps**:\\n   - `.docker`, `.kube`, `.minikube`, `google-cloud-sdk`: These indicate experience with containerization (Docker), Kubernetes, and Google Cloud Platform, pointing towards DevOps or cloud engineering.\\n\\n4. **Other Clues**:\\n   - `Dropbox`, `Google Drive`: Collaboration and file-sharing tools, common in remote or hybrid work environments.\\n   - `Diagrams`: Could imply roles requiring visual documentation, such as software architecture or UX design.\\n\\n### Step 2: Inference\\nThe presence of development tools, cloud technologies, and data science-related files strongly suggests a profession in **software development, data science, or DevOps**. The Jupyter Notebook (`HFAI Framework Design Testing.ipynb`) and Python-related files further narrow it down to roles involving **data analysis, machine learning, or AI development**.\\n\\n### Step 3: Verification\\nThe combination of:\\n- Development tools (Git, Vim, Pycharm, VSCode).\\n- Cloud and DevOps tools (Docker, Kubernetes, GCP).\\n- Data science files (Jupyter Notebook).\\n- Professional documents (likely related to projects or clients).\\n\\nThis aligns well with professions like:\\n- **Data Scientist** (focus on AI/ML frameworks).\\n- **Software Engineer** (full-stack or backend, given the variety of tools).\\n- **DevOps Engineer** (given the cloud and containerization tools).\\n\\n### Final Answer\\nBased on the files and directories, you likely work in a **technical role such as Software Engineer, Data Scientist, or DevOps Engineer**, with a focus on Python, cloud technologies, and possibly AI/ML frameworks.\\n\\n**Objective Met**: Yes, the analysis provides a reasonable inference about your profession.\", refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], 'usage': CompletionUsage(completion_tokens=596, prompt_tokens=4854, total_tokens=5450, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetails(audio_tokens=None, cached_tokens=3968), prompt_cache_hit_tokens=3968, prompt_cache_miss_tokens=886), 'model': 'deepseek-chat'}\n",
      "22:36:32 - [opendxa.execution.reasoning] INFO - Executing reasoning node: END\n",
      "22:36:32 - [opendxa.execution.plan] INFO - Executing plan node: END\n",
      "22:36:32 - [opendxa.execution.workflow] INFO - Executing workflow node: END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"From the directory listing, here's the analysis to infer your profession:\\n\"\n",
      " '\\n'\n",
      " '### Step 1: Key Observations\\n'\n",
      " '1. **Technical Files and Directories**:\\n'\n",
      " '   - `.bashrc`, `.bash_profile`, `.gitconfig`, `.vimrc`, `.zshrc`: These are '\n",
      " 'configuration files for development tools like Bash, Git, Vim, and Zsh, '\n",
      " 'indicating familiarity with programming and system administration.\\n'\n",
      " '   - `PycharmProjects`, `.vscode`, `src`, `tests`, `node_modules`, '\n",
      " '`package.json`, `requirements.txt`: These suggest active involvement in '\n",
      " 'software development, possibly using Python (Pycharm, requirements.txt) and '\n",
      " 'JavaScript/Node.js (node_modules, package.json).\\n'\n",
      " '   - `HFAI Framework Design Testing.ipynb`: A Jupyter Notebook file, often '\n",
      " 'used in data science, machine learning, or research.\\n'\n",
      " '\\n'\n",
      " '2. **Professional Documents**:\\n'\n",
      " '   - `190911 FUV Follow-Ups.docx`, `Cathay-KIX-HAN-2.pdf`, '\n",
      " '`2019-08-31-Panasonic-Kaiser.pdf`: These files hint at professional '\n",
      " 'communication, possibly in roles involving project management, consulting, '\n",
      " 'or technical documentation.\\n'\n",
      " '\\n'\n",
      " '3. **Cloud and DevOps**:\\n'\n",
      " '   - `.docker`, `.kube`, `.minikube`, `google-cloud-sdk`: These indicate '\n",
      " 'experience with containerization (Docker), Kubernetes, and Google Cloud '\n",
      " 'Platform, pointing towards DevOps or cloud engineering.\\n'\n",
      " '\\n'\n",
      " '4. **Other Clues**:\\n'\n",
      " '   - `Dropbox`, `Google Drive`: Collaboration and file-sharing tools, common '\n",
      " 'in remote or hybrid work environments.\\n'\n",
      " '   - `Diagrams`: Could imply roles requiring visual documentation, such as '\n",
      " 'software architecture or UX design.\\n'\n",
      " '\\n'\n",
      " '### Step 2: Inference\\n'\n",
      " 'The presence of development tools, cloud technologies, and data '\n",
      " 'science-related files strongly suggests a profession in **software '\n",
      " 'development, data science, or DevOps**. The Jupyter Notebook (`HFAI '\n",
      " 'Framework Design Testing.ipynb`) and Python-related files further narrow it '\n",
      " 'down to roles involving **data analysis, machine learning, or AI '\n",
      " 'development**.\\n'\n",
      " '\\n'\n",
      " '### Step 3: Verification\\n'\n",
      " 'The combination of:\\n'\n",
      " '- Development tools (Git, Vim, Pycharm, VSCode).\\n'\n",
      " '- Cloud and DevOps tools (Docker, Kubernetes, GCP).\\n'\n",
      " '- Data science files (Jupyter Notebook).\\n'\n",
      " '- Professional documents (likely related to projects or clients).\\n'\n",
      " '\\n'\n",
      " 'This aligns well with professions like:\\n'\n",
      " '- **Data Scientist** (focus on AI/ML frameworks).\\n'\n",
      " '- **Software Engineer** (full-stack or backend, given the variety of '\n",
      " 'tools).\\n'\n",
      " '- **DevOps Engineer** (given the cloud and containerization tools).\\n'\n",
      " '\\n'\n",
      " '### Final Answer\\n'\n",
      " 'Based on the files and directories, you likely work in a **technical role '\n",
      " 'such as Software Engineer, Data Scientist, or DevOps Engineer**, with a '\n",
      " 'focus on Python, cloud technologies, and possibly AI/ML frameworks.\\n'\n",
      " '\\n'\n",
      " '**Objective Met**: Yes, the analysis provides a reasonable inference about '\n",
      " 'your profession.')\n"
     ]
    }
   ],
   "source": [
    "from opendxa import McpResource, ReasoningStrategy, DXA_LOGGER\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# From dictionary\n",
    "dirname = \"/Users/ctn/\"\n",
    "filesystem_resource = McpResource.from_config(\"filesystem\", {\n",
    "    \"command\": \"npx\",\n",
    "    \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", dirname]\n",
    "})\n",
    "pprint(await filesystem_resource.list_tools())\n",
    "\n",
    "# search_resource = McpResource.from_config(\"brave-search\", {\n",
    "#    \"command\": \"npx\",\n",
    "#    \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"]\n",
    "# })\n",
    "# pprint(await search_resource.list_tools())\n",
    "\n",
    "sequential_thinking_resource = McpResource.from_config(\"sequential-thinking\", {\n",
    "    \"command\": \"npx\",\n",
    "    \"args\": [\"-y\", \"@modelcontextprotocol/server-sequential-thinking\"]\n",
    "})\n",
    "pprint(await sequential_thinking_resource.list_tools())\n",
    "\n",
    "# response = await filesystem_resource.query({\n",
    "#    \"tool\": \"list_allowed_directories\",\n",
    "#    \"arguments\": {}  # {\"path\": dirname}\n",
    "# })\n",
    "# pprint(f\"Resource response: {response}\")\n",
    "\n",
    "# We can then make that resource available to an Agent.\n",
    "from opendxa import Agent, DXA_LOGGER\n",
    "\n",
    "agent = Agent()\\\n",
    "    .with_model(\"anthropic:claude-3-sonnet-20240229\")\\\n",
    "    .with_model(\"deepseek:deepseek-coder\")\\\n",
    "    .with_resources({\"filesystem\": filesystem_resource})\\\n",
    "    .with_resources({\"sequential-thinking\": sequential_thinking_resource})\\\n",
    "    .with_reasoning(ReasoningStrategy.CHAIN_OF_THOUGHT)\n",
    "DXA_LOGGER.basicConfig(level=DXA_LOGGER.DEBUG)\n",
    "result = agent.ask(\"Can you reason out what I do for a living from my files? You can use ONE level of subdirs.\")\n",
    "\n",
    "pprint(result[\"choices\"][0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Launching our own Python MCP services\n",
    "\n",
    "We can create our own Python MCP services and launch them automatically via an `MCPResource` that is attached to those services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendxa.common.resource.mcp import HttpTransportParams\n",
    "\n",
    "# Example 1: STDIO Transport (Local Server)\n",
    "local_mcp = McpResource(\n",
    "    name=\"local_echo\",\n",
    "    transport_params=StdioTransportParams(\n",
    "        server_script=\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\",\n",
    "        command=\"python\",\n",
    "        args=[\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\"],\n",
    "        env={\"DEBUG\": \"1\"}  # Optional environment variables\n",
    "    )\n",
    ")\n",
    "\n",
    "# Example 2: HTTP Transport (Remote Server)\n",
    "remote_mcp = McpResource(\n",
    "    name=\"remote_echo\",\n",
    "    transport_params=HttpTransportParams(\n",
    "        url=\"https://api.example.com/mcp\",\n",
    "        headers={\"Authorization\": \"Bearer your-token\"},\n",
    "        timeout=5.0,  # Connection timeout in seconds\n",
    "        sse_read_timeout=300.0  # SSE read timeout in seconds\n",
    "    )\n",
    ")\n",
    "\n",
    "# Test both servers\n",
    "local_response = await local_mcp.query({\n",
    "    \"tool\": \"ping\"\n",
    "})\n",
    "print(f\"Local server response: {local_response.content}\")\n",
    "\n",
    "try:\n",
    "    remote_response = await remote_mcp.query({\n",
    "        \"tool\": \"ping\"\n",
    "    })\n",
    "    print(f\"Remote server response: {remote_response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Remote server error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tool Discovery and Usage\n",
    "\n",
    "MCP resources provide a way to discover available tools at runtime. Let's see how to use this feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover available tools\n",
    "tools = await local_mcp.list_tools()\n",
    "print(f\"Found {len(tools)} available tools\\n\")\n",
    "\n",
    "# Print tool details\n",
    "for tool in tools:\n",
    "    print(f\"Tool: {tool.name}\")\n",
    "    print(f\"Description: {tool.description}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param_name, param_details in tool.inputSchema[\"properties\"].items():\n",
    "        print(f\"  - {param_name}: {param_details.get('type')}\")\n",
    "        if param_name in tool.inputSchema.get(\"required\", []):\n",
    "            print(\"    (Required)\")\n",
    "    print()\n",
    "\n",
    "# Example: Using a discovered tool\n",
    "if tools:\n",
    "    tool = tools[0]  # Use the first available tool\n",
    "    print(f\"Testing tool: {tool.name}\")\n",
    "\n",
    "    # Prepare arguments based on the tool's schema\n",
    "    arguments = {}\n",
    "    for param_name, param_details in tool.inputSchema[\"properties\"].items():\n",
    "        if param_name in tool.inputSchema.get(\"required\", []):\n",
    "            # Provide a default value based on the parameter type\n",
    "            param_type = param_details.get(\"type\")\n",
    "            if param_type == \"string\":\n",
    "                arguments[param_name] = \"test\"\n",
    "            elif param_type == \"number\":\n",
    "                arguments[param_name] = 42\n",
    "            elif param_type == \"boolean\":\n",
    "                arguments[param_name] = True\n",
    "            elif param_type == \"array\":\n",
    "                arguments[param_name] = []\n",
    "            elif param_type == \"object\":\n",
    "                arguments[param_name] = {}\n",
    "\n",
    "    # Execute the tool\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": tool.name,\n",
    "        \"arguments\": arguments\n",
    "    })\n",
    "\n",
    "    print(f\"Tool response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Handling\n",
    "\n",
    "MCP resources provide robust error handling. Let's see how to handle different types of errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Invalid tool name\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"nonexistent_tool\",\n",
    "        \"arguments\": {}\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 2: Invalid arguments\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\",\n",
    "        \"arguments\": {\"invalid_param\": \"value\"}\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 3: Missing required arguments\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\"  # Missing required 'message' argument\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Features\n",
    "\n",
    "### 5.1 Environment Variables\n",
    "\n",
    "You can pass environment variables to local MCP servers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MCP resource with environment variables\n",
    "mcp_with_env = McpResource(\n",
    "    name=\"env_mcp\",\n",
    "    transport_params=StdioTransportParams(\n",
    "        server_script=\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\",\n",
    "        command=\"python\",\n",
    "        args=[\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\"],\n",
    "        env={\n",
    "            \"DEBUG\": \"1\",\n",
    "            \"LOG_LEVEL\": \"INFO\",\n",
    "            \"CUSTOM_VAR\": \"custom_value\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Test the server with environment variables\n",
    "response = await mcp_with_env.query({\n",
    "    \"tool\": \"ping\"\n",
    "})\n",
    "print(f\"Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tool Schema Validation\n",
    "\n",
    "MCP automatically validates tool arguments against their schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Valid arguments\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\",\n",
    "        \"arguments\": {\"message\": \"Valid message\"}\n",
    "    })\n",
    "    print(f\"Valid arguments response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 2: Invalid argument type\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\",\n",
    "        \"arguments\": {\"message\": 42}  # Should be a string\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices\n",
    "\n",
    "Here are some best practices for working with MCP resources:\n",
    "\n",
    "1. **Server Design**\n",
    "   - Keep servers focused on specific functionality\n",
    "   - Implement proper error handling\n",
    "   - Use type hints and docstrings\n",
    "   - Follow the single responsibility principle\n",
    "\n",
    "2. **Client Usage**\n",
    "   - Always initialize resources before use\n",
    "   - Implement proper error handling\n",
    "   - Use appropriate timeouts for remote servers\n",
    "   - Validate tool arguments before calling\n",
    "\n",
    "3. **Transport Selection**\n",
    "   - Use STDIO transport for local servers\n",
    "   - Use HTTP transport for remote servers\n",
    "   - Configure appropriate timeouts\n",
    "   - Handle connection errors gracefully\n",
    "\n",
    "4. **Tool Design**\n",
    "   - Keep tools focused and single-purpose\n",
    "   - Provide clear documentation\n",
    "   - Use appropriate parameter types\n",
    "   - Handle edge cases\n",
    "\n",
    "5. **Error Handling**\n",
    "   - Implement proper error handling\n",
    "   - Use appropriate error messages\n",
    "   - Handle timeouts and connection errors\n",
    "   - Validate inputs and outputs\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. Understanding MCP and its features\n",
    "2. Working with different transport types\n",
    "3. Discovering and using MCP tools\n",
    "4. Handling errors and edge cases\n",
    "5. Using advanced features\n",
    "6. Following best practices\n",
    "\n",
    "The MCP resource in OpenDXA provides a powerful and flexible way to integrate external services and tools into your agents. By following the best practices outlined in this tutorial, you can create robust and maintainable MCP-based applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

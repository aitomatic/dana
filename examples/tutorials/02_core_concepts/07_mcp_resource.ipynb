{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCPResource in OpenDXA\n",
    "\n",
    "This tutorial covers the Model Context Protocol (MCP) resource in OpenDXA, which provides a standardized way to integrate external services and tools into your agents.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "\n",
    "1. What MCP is and how it works\n",
    "2. How to create and use MCP resources\n",
    "3. How to work with different transport types (STDIO and HTTP)\n",
    "4. How to discover and use MCP tools\n",
    "5. Best practices for MCP resource usage\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of OpenDXA's architecture\n",
    "- Familiarity with Python async/await syntax\n",
    "- Understanding of basic resource management concepts\n",
    "\n",
    "## 1. Understanding MCP\n",
    "\n",
    "The Model Context Protocol (MCP) is a standardized way to expose data and functionality to LLM applications. MCP servers can:\n",
    "\n",
    "1. **Expose Data**: Through resources (similar to GET endpoints)\n",
    "2. **Provide Functionality**: Through tools (similar to POST endpoints)\n",
    "3. **Define Interaction Patterns**: Through prompts (reusable templates)\n",
    "\n",
    "Let's start by creating a simple MCP resource. First letâ€™s install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install @modelcontextprotocol/server-filesystem\n",
    "!npm install @modelcontextprotocol/server-brave-search\n",
    "!npm install @modelcontextprotocol/server-sequential-thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendxa import McpResource, ReasoningStrategy, DXA_LOGGER\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# From dictionary\n",
    "dirname = \"/Users/ctn/\"\n",
    "filesystem_resource = McpResource.from_config(\"filesystem\", {\n",
    "    \"command\": \"npx\",\n",
    "    \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", dirname]\n",
    "})\n",
    "pprint(await filesystem_resource.list_tools())\n",
    "\n",
    "# search_resource = McpResource.from_config(\"brave-search\", {\n",
    "#    \"command\": \"npx\",\n",
    "#    \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"]\n",
    "# })\n",
    "# pprint(await search_resource.list_tools())\n",
    "\n",
    "sequential_thinking_resource = McpResource.from_config(\"sequential-thinking\", {\n",
    "    \"command\": \"npx\",\n",
    "    \"args\": [\"-y\", \"@modelcontextprotocol/server-sequential-thinking\"]\n",
    "})\n",
    "pprint(await sequential_thinking_resource.list_tools())\n",
    "\n",
    "# response = await filesystem_resource.query({\n",
    "#    \"tool\": \"list_allowed_directories\",\n",
    "#    \"arguments\": {}  # {\"path\": dirname}\n",
    "# })\n",
    "# pprint(f\"Resource response: {response}\")\n",
    "\n",
    "# We can then make that resource available to an Agent.\n",
    "from opendxa import Agent, DXA_LOGGER\n",
    "\n",
    "agent = Agent()\\\n",
    "    .with_model(\"anthropic:claude-3-sonnet-20240229\")\\\n",
    "    .with_model(\"deepseek:deepseek-coder\")\\\n",
    "    .with_model(\"openai:gpt-4o-mini\")\\\n",
    "    .with_resources({\"filesystem\": filesystem_resource})\\\n",
    "    .with_resources({\"sequential-thinking\": sequential_thinking_resource})\\\n",
    "    .with_reasoning(ReasoningStrategy.CHAIN_OF_THOUGHT)\n",
    "DXA_LOGGER.basicConfig(level=DXA_LOGGER.DEBUG)\n",
    "result = agent.ask(\"Can you reason out what I do for a living from my files? Do not use subdirs.\")\n",
    "\n",
    "pprint(result[\"choices\"][0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Launching our own Python MCP services\n",
    "\n",
    "We can create our own Python MCP services and launch them automatically via an `MCPResource` that is attached to those services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendxa.common.resource.mcp import HttpTransportParams\n",
    "\n",
    "# Example 1: STDIO Transport (Local Server)\n",
    "local_mcp = McpResource(\n",
    "    name=\"local_echo\",\n",
    "    transport_params=StdioTransportParams(\n",
    "        server_script=\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\",\n",
    "        command=\"python\",\n",
    "        args=[\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\"],\n",
    "        env={\"DEBUG\": \"1\"}  # Optional environment variables\n",
    "    )\n",
    ")\n",
    "\n",
    "# Example 2: HTTP Transport (Remote Server)\n",
    "remote_mcp = McpResource(\n",
    "    name=\"remote_echo\",\n",
    "    transport_params=HttpTransportParams(\n",
    "        url=\"https://api.example.com/mcp\",\n",
    "        headers={\"Authorization\": \"Bearer your-token\"},\n",
    "        timeout=5.0,  # Connection timeout in seconds\n",
    "        sse_read_timeout=300.0  # SSE read timeout in seconds\n",
    "    )\n",
    ")\n",
    "\n",
    "# Test both servers\n",
    "local_response = await local_mcp.query({\n",
    "    \"tool\": \"ping\"\n",
    "})\n",
    "print(f\"Local server response: {local_response.content}\")\n",
    "\n",
    "try:\n",
    "    remote_response = await remote_mcp.query({\n",
    "        \"tool\": \"ping\"\n",
    "    })\n",
    "    print(f\"Remote server response: {remote_response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Remote server error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tool Discovery and Usage\n",
    "\n",
    "MCP resources provide a way to discover available tools at runtime. Let's see how to use this feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover available tools\n",
    "tools = await local_mcp.list_tools()\n",
    "print(f\"Found {len(tools)} available tools\\n\")\n",
    "\n",
    "# Print tool details\n",
    "for tool in tools:\n",
    "    print(f\"Tool: {tool.name}\")\n",
    "    print(f\"Description: {tool.description}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param_name, param_details in tool.inputSchema[\"properties\"].items():\n",
    "        print(f\"  - {param_name}: {param_details.get('type')}\")\n",
    "        if param_name in tool.inputSchema.get(\"required\", []):\n",
    "            print(\"    (Required)\")\n",
    "    print()\n",
    "\n",
    "# Example: Using a discovered tool\n",
    "if tools:\n",
    "    tool = tools[0]  # Use the first available tool\n",
    "    print(f\"Testing tool: {tool.name}\")\n",
    "\n",
    "    # Prepare arguments based on the tool's schema\n",
    "    arguments = {}\n",
    "    for param_name, param_details in tool.inputSchema[\"properties\"].items():\n",
    "        if param_name in tool.inputSchema.get(\"required\", []):\n",
    "            # Provide a default value based on the parameter type\n",
    "            param_type = param_details.get(\"type\")\n",
    "            if param_type == \"string\":\n",
    "                arguments[param_name] = \"test\"\n",
    "            elif param_type == \"number\":\n",
    "                arguments[param_name] = 42\n",
    "            elif param_type == \"boolean\":\n",
    "                arguments[param_name] = True\n",
    "            elif param_type == \"array\":\n",
    "                arguments[param_name] = []\n",
    "            elif param_type == \"object\":\n",
    "                arguments[param_name] = {}\n",
    "\n",
    "    # Execute the tool\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": tool.name,\n",
    "        \"arguments\": arguments\n",
    "    })\n",
    "\n",
    "    print(f\"Tool response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Handling\n",
    "\n",
    "MCP resources provide robust error handling. Let's see how to handle different types of errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Invalid tool name\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"nonexistent_tool\",\n",
    "        \"arguments\": {}\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 2: Invalid arguments\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\",\n",
    "        \"arguments\": {\"invalid_param\": \"value\"}\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 3: Missing required arguments\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\"  # Missing required 'message' argument\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Features\n",
    "\n",
    "### 5.1 Environment Variables\n",
    "\n",
    "You can pass environment variables to local MCP servers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MCP resource with environment variables\n",
    "mcp_with_env = McpResource(\n",
    "    name=\"env_mcp\",\n",
    "    transport_params=StdioTransportParams(\n",
    "        server_script=\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\",\n",
    "        command=\"python\",\n",
    "        args=[\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\"],\n",
    "        env={\n",
    "            \"DEBUG\": \"1\",\n",
    "            \"LOG_LEVEL\": \"INFO\",\n",
    "            \"CUSTOM_VAR\": \"custom_value\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Test the server with environment variables\n",
    "response = await mcp_with_env.query({\n",
    "    \"tool\": \"ping\"\n",
    "})\n",
    "print(f\"Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tool Schema Validation\n",
    "\n",
    "MCP automatically validates tool arguments against their schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Valid arguments\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\",\n",
    "        \"arguments\": {\"message\": \"Valid message\"}\n",
    "    })\n",
    "    print(f\"Valid arguments response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 2: Invalid argument type\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\",\n",
    "        \"arguments\": {\"message\": 42}  # Should be a string\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices\n",
    "\n",
    "Here are some best practices for working with MCP resources:\n",
    "\n",
    "1. **Server Design**\n",
    "   - Keep servers focused on specific functionality\n",
    "   - Implement proper error handling\n",
    "   - Use type hints and docstrings\n",
    "   - Follow the single responsibility principle\n",
    "\n",
    "2. **Client Usage**\n",
    "   - Always initialize resources before use\n",
    "   - Implement proper error handling\n",
    "   - Use appropriate timeouts for remote servers\n",
    "   - Validate tool arguments before calling\n",
    "\n",
    "3. **Transport Selection**\n",
    "   - Use STDIO transport for local servers\n",
    "   - Use HTTP transport for remote servers\n",
    "   - Configure appropriate timeouts\n",
    "   - Handle connection errors gracefully\n",
    "\n",
    "4. **Tool Design**\n",
    "   - Keep tools focused and single-purpose\n",
    "   - Provide clear documentation\n",
    "   - Use appropriate parameter types\n",
    "   - Handle edge cases\n",
    "\n",
    "5. **Error Handling**\n",
    "   - Implement proper error handling\n",
    "   - Use appropriate error messages\n",
    "   - Handle timeouts and connection errors\n",
    "   - Validate inputs and outputs\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. Understanding MCP and its features\n",
    "2. Working with different transport types\n",
    "3. Discovering and using MCP tools\n",
    "4. Handling errors and edge cases\n",
    "5. Using advanced features\n",
    "6. Following best practices\n",
    "\n",
    "The MCP resource in OpenDXA provides a powerful and flexible way to integrate external services and tools into your agents. By following the best practices outlined in this tutorial, you can create robust and maintainable MCP-based applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

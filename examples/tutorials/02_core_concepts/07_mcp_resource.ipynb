{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCPResource in OpenDXA\n",
    "\n",
    "This tutorial covers the Model Context Protocol (MCP) resource in OpenDXA, which provides a standardized way to integrate external services and tools into your agents.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will understand:\n",
    "\n",
    "1. What MCP is and how it works\n",
    "2. How to create and use MCP resources\n",
    "3. How to work with different transport types (STDIO and HTTP)\n",
    "4. How to discover and use MCP tools\n",
    "5. Best practices for MCP resource usage\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of OpenDXA's architecture\n",
    "- Familiarity with Python async/await syntax\n",
    "- Understanding of basic resource management concepts\n",
    "\n",
    "## 1. Understanding MCP\n",
    "\n",
    "The Model Context Protocol (MCP) is a standardized way to expose data and functionality to LLM applications. MCP servers can:\n",
    "\n",
    "1. **Expose Data**: Through resources (similar to GET endpoints)\n",
    "2. **Provide Functionality**: Through tools (similar to POST endpoints)\n",
    "3. **Define Interaction Patterns**: Through prompts (reusable templates)\n",
    "\n",
    "Let's start by creating a simple MCP resource. First letâ€™s install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install @modelcontextprotocol/server-filesystem\n",
    "!npm install @modelcontextprotocol/server-brave-search\n",
    "!npm install @modelcontextprotocol/server-sequential-thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tool(name='read_file', description='Read the complete contents of a file from the file system. Handles various text encodings and provides detailed error messages if the file cannot be read. Use this tool when you need to examine the contents of a single file. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}}, 'required': ['path'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='read_multiple_files', description=\"Read the contents of multiple files simultaneously. This is more efficient than reading files one by one when you need to analyze or compare multiple files. Each file's content is returned with its path as a reference. Failed reads for individual files won't stop the entire operation. Only works within allowed directories.\", inputSchema={'type': 'object', 'properties': {'paths': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['paths'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='write_file', description='Create a new file or completely overwrite an existing file with new content. Use with caution as it will overwrite existing files without warning. Handles text content with proper encoding. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}, 'content': {'type': 'string'}}, 'required': ['path', 'content'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='edit_file', description='Make line-based edits to a text file. Each edit replaces exact line sequences with new content. Returns a git-style diff showing the changes made. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}, 'edits': {'type': 'array', 'items': {'type': 'object', 'properties': {'oldText': {'type': 'string', 'description': 'Text to search for - must match exactly'}, 'newText': {'type': 'string', 'description': 'Text to replace with'}}, 'required': ['oldText', 'newText'], 'additionalProperties': False}}, 'dryRun': {'type': 'boolean', 'default': False, 'description': 'Preview changes using git-style diff format'}}, 'required': ['path', 'edits'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='create_directory', description='Create a new directory or ensure a directory exists. Can create multiple nested directories in one operation. If the directory already exists, this operation will succeed silently. Perfect for setting up directory structures for projects or ensuring required paths exist. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}}, 'required': ['path'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='list_directory', description='Get a detailed listing of all files and directories in a specified path. Results clearly distinguish between files and directories with [FILE] and [DIR] prefixes. This tool is essential for understanding directory structure and finding specific files within a directory. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}}, 'required': ['path'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='directory_tree', description=\"Get a recursive tree view of files and directories as a JSON structure. Each entry includes 'name', 'type' (file/directory), and 'children' for directories. Files have no children array, while directories always have a children array (which may be empty). The output is formatted with 2-space indentation for readability. Only works within allowed directories.\", inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}}, 'required': ['path'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='move_file', description='Move or rename files and directories. Can move files between directories and rename them in a single operation. If the destination exists, the operation will fail. Works across different directories and can be used for simple renaming within the same directory. Both source and destination must be within allowed directories.', inputSchema={'type': 'object', 'properties': {'source': {'type': 'string'}, 'destination': {'type': 'string'}}, 'required': ['source', 'destination'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='search_files', description=\"Recursively search for files and directories matching a pattern. Searches through all subdirectories from the starting path. The search is case-insensitive and matches partial names. Returns full paths to all matching items. Great for finding files when you don't know their exact location. Only searches within allowed directories.\", inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}, 'pattern': {'type': 'string'}, 'excludePatterns': {'type': 'array', 'items': {'type': 'string'}, 'default': []}}, 'required': ['path', 'pattern'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='get_file_info', description='Retrieve detailed metadata about a file or directory. Returns comprehensive information including size, creation time, last modified time, permissions, and type. This tool is perfect for understanding file characteristics without reading the actual content. Only works within allowed directories.', inputSchema={'type': 'object', 'properties': {'path': {'type': 'string'}}, 'required': ['path'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}),\n",
      " Tool(name='list_allowed_directories', description='Returns the list of directories that this server is allowed to access. Use this to understand which directories are available before trying to access files.', inputSchema={'type': 'object', 'properties': {}, 'required': []})]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/14/25 22:41:54] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/chat/completions</span>          <a href=\"file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py#1038\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1038</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 400 Bad Request\"</span>                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/14/25 22:41:54]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/chat/completions\u001b[0m          \u001b]8;id=380794;file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=894837;file:///Users/ctn/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/httpx/_client.py#1038\u001b\\\u001b[2m1038\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 400 Bad Request\"\u001b[0m                                             \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:41:54 - [OpenDXA LLMResource] ERROR - Error querying LLM: An error occurred: Error code: 400 - {'error': {'message': \"Invalid schema for function 'llm_result__254e5681__final_result': In context=('properties', 'metadata', 'type', '0'), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'tools[11].function.parameters', 'code': 'invalid_function_parameters'}}\n"
     ]
    },
    {
     "ename": "LLMError",
     "evalue": "Error querying LLM",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/aisuite/providers/openai_provider.py:33\u001b[39m, in \u001b[36mOpenaiProvider.chat_completions_create\u001b[39m\u001b[34m(self, model, messages, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m transformed_messages = \u001b[38;5;28mself\u001b[39m.transformer.convert_request(messages)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformed_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass any additional arguments to the OpenAI API\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:914\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    913\u001b[39m validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1239\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/openai/_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/openai/_base_client.py:1023\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1022\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1026\u001b[39m     cast_to=cast_to,\n\u001b[32m   1027\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1032\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid schema for function 'llm_result__254e5681__final_result': In context=('properties', 'metadata', 'type', '0'), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'tools[11].function.parameters', 'code': 'invalid_function_parameters'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLLMError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/common/resource/llm_resource.py:264\u001b[39m, in \u001b[36mLLMResource._query_once\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[38;5;28mself\u001b[39m._log_llm_request(messages)\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_params\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# Log the response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/aisuite/client.py:245\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, model, messages, **kwargs)\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Default behavior without tool execution\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# Delegate the chat completion to the correct provider's implementation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m response = \u001b[43mprovider\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completions_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._extract_thinking_content(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/aisuite/providers/openai_provider.py:40\u001b[39m, in \u001b[36mOpenaiProvider.chat_completions_create\u001b[39m\u001b[34m(self, model, messages, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LLMError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mLLMError\u001b[39m: An error occurred: Error code: 400 - {'error': {'message': \"Invalid schema for function 'llm_result__254e5681__final_result': In context=('properties', 'metadata', 'type', '0'), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'tools[11].function.parameters', 'code': 'invalid_function_parameters'}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mLLMError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopendxa\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[32m     35\u001b[39m agent = Agent()\\\n\u001b[32m     36\u001b[39m     .with_model(\u001b[33m\"\u001b[39m\u001b[33manthropic:claude-3-sonnet-20240229\u001b[39m\u001b[33m\"\u001b[39m)\\\n\u001b[32m     37\u001b[39m     .with_model(\u001b[33m\"\u001b[39m\u001b[33mdeepseek:deepseek-coder\u001b[39m\u001b[33m\"\u001b[39m)\\\n\u001b[32m     38\u001b[39m     .with_model(\u001b[33m\"\u001b[39m\u001b[33mopenai:gpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m)\\\n\u001b[32m     39\u001b[39m     .with_resources({\u001b[33m\"\u001b[39m\u001b[33mfilesystem\u001b[39m\u001b[33m\"\u001b[39m: filesystem_resource})\\\n\u001b[32m     40\u001b[39m     .with_reasoning(ReasoningStrategy.CHAIN_OF_THOUGHT)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCan you reason out what I do for a living from my files? Do not use subdirs.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m pprint(result[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m].message.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/agent/agent.py:347\u001b[39m, in \u001b[36mAgent.ask\u001b[39m\u001b[34m(self, question)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ask a question to the agent.\"\"\"\u001b[39;00m\n\u001b[32m    346\u001b[39m workflow = WorkflowFactory.create_basic_workflow(question, [\u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/agent/agent.py:342\u001b[39m, in \u001b[36mAgent.run\u001b[39m\u001b[34m(self, workflow)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, workflow: Workflow) -> Any:\n\u001b[32m    341\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run an workflow.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msafe_asyncio_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43masync_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/common/utils/misc.py:89\u001b[39m, in \u001b[36msafe_asyncio_run\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run a function in an asyncio loop.\"\"\"\u001b[39;00m\n\u001b[32m     88\u001b[39m ensure_asyncio_safety()\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/.venv/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.7/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/agent/agent.py:338\u001b[39m, in \u001b[36mAgent.async_run\u001b[39m\u001b[34m(self, workflow, context)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m context.reasoning_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:  \u001b[38;5;66;03m# For cleanup\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.runtime.execute(workflow, context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/agent/agent_runtime.py:41\u001b[39m, in \u001b[36mAgentRuntime.execute\u001b[39m\u001b[34m(self, workflow, context)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m context.reasoning_llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m    \n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Execute workflow with context\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m signals = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.workflow_executor.execute(workflow, context)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Get final result from signals\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# TODO: Handle multiple signals\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m signal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(signals):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:153\u001b[39m, in \u001b[36mBaseExecutor.execute\u001b[39m\u001b[34m(self, graph, context)\u001b[39m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# Execute current node\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     node_signals = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute_node(\n\u001b[32m    154\u001b[39m         cast(ExecutionNode, node),\n\u001b[32m    155\u001b[39m         context\n\u001b[32m    156\u001b[39m     )\n\u001b[32m    157\u001b[39m     signals.extend(node_signals)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signals\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:208\u001b[39m, in \u001b[36mBaseExecutor.execute_node\u001b[39m\u001b[34m(self, node, context)\u001b[39m\n\u001b[32m    206\u001b[39m node.status = ExecutionNodeStatus.FAILED\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# return self._handle_error(node, e, create_signal=True) or []\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:192\u001b[39m, in \u001b[36mBaseExecutor.execute_node\u001b[39m\u001b[34m(self, node, context)\u001b[39m\n\u001b[32m    187\u001b[39m execution_context = \u001b[38;5;28mself\u001b[39m.build_execution_context(\n\u001b[32m    188\u001b[39m     context, node\n\u001b[32m    189\u001b[39m )\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Phase 4: Execute node\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m signals = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._execute_node_core(node, execution_context)\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# Phase 5: Process signals\u001b[39;00m\n\u001b[32m    195\u001b[39m processed_signals = \u001b[38;5;28mself\u001b[39m._process_signals(signals, node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:223\u001b[39m, in \u001b[36mBaseExecutor._execute_node_core\u001b[39m\u001b[34m(self, node, context)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lower_graph:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ExecutionError(\u001b[33m\"\u001b[39m\u001b[33mFailed to create graph for lower layer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lower_executor.execute(lower_graph, context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:153\u001b[39m, in \u001b[36mBaseExecutor.execute\u001b[39m\u001b[34m(self, graph, context)\u001b[39m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# Execute current node\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     node_signals = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute_node(\n\u001b[32m    154\u001b[39m         cast(ExecutionNode, node),\n\u001b[32m    155\u001b[39m         context\n\u001b[32m    156\u001b[39m     )\n\u001b[32m    157\u001b[39m     signals.extend(node_signals)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signals\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:208\u001b[39m, in \u001b[36mBaseExecutor.execute_node\u001b[39m\u001b[34m(self, node, context)\u001b[39m\n\u001b[32m    206\u001b[39m node.status = ExecutionNodeStatus.FAILED\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# return self._handle_error(node, e, create_signal=True) or []\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:192\u001b[39m, in \u001b[36mBaseExecutor.execute_node\u001b[39m\u001b[34m(self, node, context)\u001b[39m\n\u001b[32m    187\u001b[39m execution_context = \u001b[38;5;28mself\u001b[39m.build_execution_context(\n\u001b[32m    188\u001b[39m     context, node\n\u001b[32m    189\u001b[39m )\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Phase 4: Execute node\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m signals = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._execute_node_core(node, execution_context)\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# Phase 5: Process signals\u001b[39;00m\n\u001b[32m    195\u001b[39m processed_signals = \u001b[38;5;28mself\u001b[39m._process_signals(signals, node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:223\u001b[39m, in \u001b[36mBaseExecutor._execute_node_core\u001b[39m\u001b[34m(self, node, context)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lower_graph:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ExecutionError(\u001b[33m\"\u001b[39m\u001b[33mFailed to create graph for lower layer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lower_executor.execute(lower_graph, context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:153\u001b[39m, in \u001b[36mBaseExecutor.execute\u001b[39m\u001b[34m(self, graph, context)\u001b[39m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# Execute current node\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     node_signals = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.execute_node(\n\u001b[32m    154\u001b[39m         cast(ExecutionNode, node),\n\u001b[32m    155\u001b[39m         context\n\u001b[32m    156\u001b[39m     )\n\u001b[32m    157\u001b[39m     signals.extend(node_signals)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signals\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:208\u001b[39m, in \u001b[36mBaseExecutor.execute_node\u001b[39m\u001b[34m(self, node, context)\u001b[39m\n\u001b[32m    206\u001b[39m node.status = ExecutionNodeStatus.FAILED\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# return self._handle_error(node, e, create_signal=True) or []\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/base_executor.py:192\u001b[39m, in \u001b[36mBaseExecutor.execute_node\u001b[39m\u001b[34m(self, node, context)\u001b[39m\n\u001b[32m    187\u001b[39m execution_context = \u001b[38;5;28mself\u001b[39m.build_execution_context(\n\u001b[32m    188\u001b[39m     context, node\n\u001b[32m    189\u001b[39m )\n\u001b[32m    191\u001b[39m \u001b[38;5;66;03m# Phase 4: Execute node\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m signals = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._execute_node_core(node, execution_context)\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# Phase 5: Process signals\u001b[39;00m\n\u001b[32m    195\u001b[39m processed_signals = \u001b[38;5;28mself\u001b[39m._process_signals(signals, node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/execution/reasoning/reasoning_executor.py:82\u001b[39m, in \u001b[36mReasoningExecutor._execute_node_core\u001b[39m\u001b[34m(self, node, context)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mself\u001b[39m.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResources: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext.available_resources\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m{}\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Query the LLM with available resources\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m context.reasoning_llm.query(request={\n\u001b[32m     83\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muser_messages\u001b[39m\u001b[33m\"\u001b[39m: user_messages,\n\u001b[32m     84\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msystem_messages\u001b[39m\u001b[33m\"\u001b[39m: system_messages,\n\u001b[32m     85\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mavailable_resources\u001b[39m\u001b[33m\"\u001b[39m: context.available_resources \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[32m     86\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_iterations\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m3\u001b[39m,\n\u001b[32m     87\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1000\u001b[39m,\n\u001b[32m     88\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.7\u001b[39m,\n\u001b[32m     89\u001b[39m })\n\u001b[32m     91\u001b[39m response = response \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m.info(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mReasoning Result:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/common/resource/llm_resource.py:80\u001b[39m, in \u001b[36mLLMResource.query\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m     available_resources[\u001b[33m\"\u001b[39m\u001b[33mfinal_result\u001b[39m\u001b[33m\"\u001b[39m] = LLMResultResource()\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Use iterative querying if tools are provided, otherwise use single-shot\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._query_iterative(request)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/common/resource/llm_resource.py:169\u001b[39m, in \u001b[36mLLMResource._query_iterative\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    166\u001b[39m iteration += \u001b[32m1\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Make the LLM query with available resources and message history\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._query_once({\n\u001b[32m    170\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mavailable_resources\u001b[39m\u001b[33m\"\u001b[39m: request.get(\u001b[33m\"\u001b[39m\u001b[33mavailable_resources\u001b[39m\u001b[33m\"\u001b[39m, {}),\n\u001b[32m    171\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: request.get(\u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    172\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: request.get(\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0.7\u001b[39m),\n\u001b[32m    173\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: message_history  \u001b[38;5;66;03m# Pass read-only message history\u001b[39;00m\n\u001b[32m    174\u001b[39m })\n\u001b[32m    176\u001b[39m choices = get_field(response, \u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m    177\u001b[39m response_message = get_field(choices[\u001b[32m0\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m choices \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(choices) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/aitomatic/opendxa/opendxa/common/resource/llm_resource.py:277\u001b[39m, in \u001b[36mLLMResource._query_once\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m.error(\u001b[33m\"\u001b[39m\u001b[33mError querying LLM: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LLMError(\u001b[33m\"\u001b[39m\u001b[33mError querying LLM\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mLLMError\u001b[39m: Error querying LLM"
     ]
    }
   ],
   "source": [
    "from opendxa import McpResource, ReasoningStrategy, DXA_LOGGER\n",
    "from pprint import pprint\n",
    "\n",
    "DXA_LOGGER.setLevel(DXA_LOGGER.DEBUG)\n",
    "\n",
    "# From dictionary\n",
    "dirname = \"/Users/ctn/\"\n",
    "filesystem_resource = McpResource.from_config(\"filesystem\", {\n",
    "    \"command\": \"npx\",\n",
    "    \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", dirname]\n",
    "})\n",
    "pprint(await filesystem_resource.list_tools())\n",
    "\n",
    "# search_resource = McpResource.from_config(\"brave-search\", {\n",
    "#    \"command\": \"npx\",\n",
    "#    \"args\": [\"-y\", \"@modelcontextprotocol/server-brave-search\"]\n",
    "# })\n",
    "# pprint(await search_resource.list_tools())\n",
    "\n",
    "# sequential_thinking_resource = McpResource.from_config(\"sequential-thinking\", {\n",
    "#    \"command\": \"npx\",\n",
    "#    \"args\": [\"-y\", \"@modelcontextprotocol/server-sequential-thinking\"]\n",
    "# })\n",
    "# pprint(await sequential_thinking_resource.list_tools())\n",
    "\n",
    "# response = await filesystem_resource.query({\n",
    "#    \"tool\": \"list_allowed_directories\",\n",
    "#    \"arguments\": {}  # {\"path\": dirname}\n",
    "# })\n",
    "# pprint(f\"Resource response: {response}\")\n",
    "\n",
    "# We can then make that resource available to an Agent.\n",
    "from opendxa import Agent\n",
    "\n",
    "agent = Agent()\\\n",
    "    .with_model(\"anthropic:claude-3-sonnet-20240229\")\\\n",
    "    .with_model(\"deepseek:deepseek-coder\")\\\n",
    "    .with_model(\"openai:gpt-4o-mini\")\\\n",
    "    .with_resources({\"filesystem\": filesystem_resource})\\\n",
    "    .with_reasoning(ReasoningStrategy.CHAIN_OF_THOUGHT)\n",
    "result = agent.ask(\"Can you reason out what I do for a living from my files? Do not use subdirs.\")\n",
    "\n",
    "pprint(result[\"choices\"][0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Launching our own Python MCP services\n",
    "\n",
    "We can create our own Python MCP services and launch them automatically via an `MCPResource` that is attached to those services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opendxa.common.resource.mcp import HttpTransportParams\n",
    "\n",
    "# Example 1: STDIO Transport (Local Server)\n",
    "local_mcp = McpResource(\n",
    "    name=\"local_echo\",\n",
    "    transport_params=StdioTransportParams(\n",
    "        server_script=\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\",\n",
    "        command=\"python\",\n",
    "        args=[\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\"],\n",
    "        env={\"DEBUG\": \"1\"}  # Optional environment variables\n",
    "    )\n",
    ")\n",
    "\n",
    "# Example 2: HTTP Transport (Remote Server)\n",
    "remote_mcp = McpResource(\n",
    "    name=\"remote_echo\",\n",
    "    transport_params=HttpTransportParams(\n",
    "        url=\"https://api.example.com/mcp\",\n",
    "        headers={\"Authorization\": \"Bearer your-token\"},\n",
    "        timeout=5.0,  # Connection timeout in seconds\n",
    "        sse_read_timeout=300.0  # SSE read timeout in seconds\n",
    "    )\n",
    ")\n",
    "\n",
    "# Test both servers\n",
    "local_response = await local_mcp.query({\n",
    "    \"tool\": \"ping\"\n",
    "})\n",
    "print(f\"Local server response: {local_response.content}\")\n",
    "\n",
    "try:\n",
    "    remote_response = await remote_mcp.query({\n",
    "        \"tool\": \"ping\"\n",
    "    })\n",
    "    print(f\"Remote server response: {remote_response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Remote server error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tool Discovery and Usage\n",
    "\n",
    "MCP resources provide a way to discover available tools at runtime. Let's see how to use this feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discover available tools\n",
    "tools = await local_mcp.list_tools()\n",
    "print(f\"Found {len(tools)} available tools\\n\")\n",
    "\n",
    "# Print tool details\n",
    "for tool in tools:\n",
    "    print(f\"Tool: {tool.name}\")\n",
    "    print(f\"Description: {tool.description}\")\n",
    "    print(\"Parameters:\")\n",
    "    for param_name, param_details in tool.inputSchema[\"properties\"].items():\n",
    "        print(f\"  - {param_name}: {param_details.get('type')}\")\n",
    "        if param_name in tool.inputSchema.get(\"required\", []):\n",
    "            print(\"    (Required)\")\n",
    "    print()\n",
    "\n",
    "# Example: Using a discovered tool\n",
    "if tools:\n",
    "    tool = tools[0]  # Use the first available tool\n",
    "    print(f\"Testing tool: {tool.name}\")\n",
    "\n",
    "    # Prepare arguments based on the tool's schema\n",
    "    arguments = {}\n",
    "    for param_name, param_details in tool.inputSchema[\"properties\"].items():\n",
    "        if param_name in tool.inputSchema.get(\"required\", []):\n",
    "            # Provide a default value based on the parameter type\n",
    "            param_type = param_details.get(\"type\")\n",
    "            if param_type == \"string\":\n",
    "                arguments[param_name] = \"test\"\n",
    "            elif param_type == \"number\":\n",
    "                arguments[param_name] = 42\n",
    "            elif param_type == \"boolean\":\n",
    "                arguments[param_name] = True\n",
    "            elif param_type == \"array\":\n",
    "                arguments[param_name] = []\n",
    "            elif param_type == \"object\":\n",
    "                arguments[param_name] = {}\n",
    "\n",
    "    # Execute the tool\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": tool.name,\n",
    "        \"arguments\": arguments\n",
    "    })\n",
    "\n",
    "    print(f\"Tool response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Error Handling\n",
    "\n",
    "MCP resources provide robust error handling. Let's see how to handle different types of errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Invalid tool name\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"nonexistent_tool\",\n",
    "        \"arguments\": {}\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 2: Invalid arguments\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\",\n",
    "        \"arguments\": {\"invalid_param\": \"value\"}\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 3: Missing required arguments\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\"  # Missing required 'message' argument\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Features\n",
    "\n",
    "### 5.1 Environment Variables\n",
    "\n",
    "You can pass environment variables to local MCP servers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MCP resource with environment variables\n",
    "mcp_with_env = McpResource(\n",
    "    name=\"env_mcp\",\n",
    "    transport_params=StdioTransportParams(\n",
    "        server_script=\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\",\n",
    "        command=\"python\",\n",
    "        args=[\"examples/learning_paths/02_core_concepts/mcp_servers/mcp_echo.py\"],\n",
    "        env={\n",
    "            \"DEBUG\": \"1\",\n",
    "            \"LOG_LEVEL\": \"INFO\",\n",
    "            \"CUSTOM_VAR\": \"custom_value\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Test the server with environment variables\n",
    "response = await mcp_with_env.query({\n",
    "    \"tool\": \"ping\"\n",
    "})\n",
    "print(f\"Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Tool Schema Validation\n",
    "\n",
    "MCP automatically validates tool arguments against their schemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Valid arguments\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\",\n",
    "        \"arguments\": {\"message\": \"Valid message\"}\n",
    "    })\n",
    "    print(f\"Valid arguments response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Example 2: Invalid argument type\n",
    "try:\n",
    "    response = await local_mcp.query({\n",
    "        \"tool\": \"echo\",\n",
    "        \"arguments\": {\"message\": 42}  # Should be a string\n",
    "    })\n",
    "    print(f\"Response: {response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Practices\n",
    "\n",
    "Here are some best practices for working with MCP resources:\n",
    "\n",
    "1. **Server Design**\n",
    "   - Keep servers focused on specific functionality\n",
    "   - Implement proper error handling\n",
    "   - Use type hints and docstrings\n",
    "   - Follow the single responsibility principle\n",
    "\n",
    "2. **Client Usage**\n",
    "   - Always initialize resources before use\n",
    "   - Implement proper error handling\n",
    "   - Use appropriate timeouts for remote servers\n",
    "   - Validate tool arguments before calling\n",
    "\n",
    "3. **Transport Selection**\n",
    "   - Use STDIO transport for local servers\n",
    "   - Use HTTP transport for remote servers\n",
    "   - Configure appropriate timeouts\n",
    "   - Handle connection errors gracefully\n",
    "\n",
    "4. **Tool Design**\n",
    "   - Keep tools focused and single-purpose\n",
    "   - Provide clear documentation\n",
    "   - Use appropriate parameter types\n",
    "   - Handle edge cases\n",
    "\n",
    "5. **Error Handling**\n",
    "   - Implement proper error handling\n",
    "   - Use appropriate error messages\n",
    "   - Handle timeouts and connection errors\n",
    "   - Validate inputs and outputs\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "\n",
    "1. Understanding MCP and its features\n",
    "2. Working with different transport types\n",
    "3. Discovering and using MCP tools\n",
    "4. Handling errors and edge cases\n",
    "5. Using advanced features\n",
    "6. Following best practices\n",
    "\n",
    "The MCP resource in OpenDXA provides a powerful and flexible way to integrate external services and tools into your agents. By following the best practices outlined in this tutorial, you can create robust and maintainable MCP-based applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

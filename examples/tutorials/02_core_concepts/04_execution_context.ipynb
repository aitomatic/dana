{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Execution Context in OpenDXA\n\nThis tutorial covers the execution context in OpenDXA, which is a crucial component that manages state, resources, and coordination between different layers of the framework.\n\n## Learning Objectives\n\nBy the end of this tutorial, you will understand:\n\n1. The structure and components of the execution context\n2. How to manage state across different layers\n3. How to handle resources and their lifecycle\n4. How to coordinate between workflow, planning, and reasoning layers\n5. Best practices for context management\n\n## Prerequisites\n\n- Basic understanding of OpenDXA's 3-layer architecture\n- Familiarity with Python async/await syntax\n- Understanding of basic resource management concepts\n\n## 1. Understanding the Execution Context\n\nThe execution context (`ExecutionContext`) is the central component that manages:\n\n- State across different layers (workflow, planning, reasoning)\n- Resource allocation and lifecycle\n- Results and data sharing between layers\n- Global context and configuration",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from opendxa.execution import ExecutionContext\nfrom opendxa.agent import AgentState, WorldState, ExecutionState\nfrom opendxa.common.resource import LLMResource\n\n# Create LLM resources for different layers\nworkflow_llm = LLMResource()\nplanning_llm = LLMResource()\nreasoning_llm = LLMResource()\n\n# Initialize the execution context\ncontext = ExecutionContext(\n    workflow_llm=workflow_llm,\n    planning_llm=planning_llm,\n    reasoning_llm=reasoning_llm,\n    agent_state=AgentState(),\n    world_state=WorldState(),\n    execution_state=ExecutionState(),\n    global_context={\"version\": \"1.0\"}\n)\n\nprint(\"Execution Context Created:\")\nprint(f\"- Workflow LLM: {context.workflow_llm}\")\nprint(f\"- Planning LLM: {context.planning_llm}\")\nprint(f\"- Reasoning LLM: {context.reasoning_llm}\")\nprint(f\"- Global Context: {context.global_context}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State Management\n",
    "\n",
    "The execution context manages three types of state:\n",
    "\n",
    "1. **Agent State**: Tracks the agent's internal state\n",
    "2. **World State**: Maintains information about the external environment\n",
    "3. **Execution State**: Tracks the progress of execution across layers\n",
    "\n",
    "Let's see how these states are managed:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from opendxa.execution import ExecutionNode, ExecutionNodeStatus\n\n# Update execution state\ncontext.execution_state.status = \"RUNNING\"\ncontext.execution_state.current_node_id = \"node1\"\ncontext.execution_state.visited_nodes.append(\"node1\")\n\n# Update agent state\ncontext.agent_state.metadata[\"current_task\"] = \"process_data\"\ncontext.agent_state.metadata[\"start_time\"] = \"2024-03-20T10:00:00\"\n\n# Update world state\ncontext.world_state.metadata[\"environment\"] = \"production\"\ncontext.world_state.metadata[\"available_resources\"] = [\"cpu\", \"memory\", \"gpu\"]\n\n# Print current states\nprint(\"Execution State:\")\nprint(f\"- Status: {context.execution_state.status}\")\nprint(f\"- Current Node: {context.execution_state.current_node_id}\")\nprint(f\"- Visited Nodes: {context.execution_state.visited_nodes}\")\n\nprint(\"\\nAgent State:\")\nprint(f\"- Current Task: {context.agent_state.metadata.get('current_task')}\")\nprint(f\"- Start Time: {context.agent_state.metadata.get('start_time')}\")\n\nprint(\"\\nWorld State:\")\nprint(f\"- Environment: {context.world_state.metadata.get('environment')}\")\nprint(f\"- Available Resources: {context.world_state.metadata.get('available_resources')}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resource Management\n",
    "\n",
    "The execution context manages resources for different layers. Let's see how to work with resources:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from opendxa.common.resource import BaseResource\n\n# Create custom resources\nclass DatabaseResource(BaseResource):\n    \"\"\"Example database resource.\"\"\"\n    def __init__(self, connection_string: str):\n        super().__init__()\n        self.connection_string = connection_string\n        \n    async def initialize(self) -> None:\n        \"\"\"Initialize database connection.\"\"\"\n        print(f\"Connecting to database: {self.connection_string}\")\n        \n    async def cleanup(self) -> None:\n        \"\"\"Cleanup database connection.\"\"\"\n        print(\"Closing database connection\")\n\n# Add resources to context\ncontext.resources = {\n    \"database\": DatabaseResource(\"postgresql://localhost:5432/mydb\"),\n    \"workflow_llm\": workflow_llm,\n    \"planning_llm\": planning_llm,\n    \"reasoning_llm\": reasoning_llm\n}\n\n# Initialize resources\nfor resource in context.resources.values():\n    await resource.initialize()\n\nprint(\"Resources initialized:\")\nfor name, resource in context.resources.items():\n    print(f\"- {name}: {type(resource).__name__}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results Management\n",
    "\n",
    "The execution context maintains results from different layers. Let's see how to manage results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Update workflow results\n",
    "context.update_workflow_result(\n",
    "    \"workflow_node_1\",\n",
    "    {\"status\": \"completed\", \"output\": \"processed data\"}\n",
    ")\n",
    "\n",
    "# Update plan results\n",
    "context.update_plan_result(\n",
    "    \"workflow_1\",\n",
    "    \"plan_1\",\n",
    "    {\"steps_completed\": 3, \"total_steps\": 5}\n",
    ")\n",
    "\n",
    "# Update reasoning results\n",
    "context.update_reasoning_result(\n",
    "    \"workflow_1\",\n",
    "    \"plan_1\",\n",
    "    \"reasoning_1\",\n",
    "    {\"confidence\": 0.95, \"explanation\": \"High confidence in decision\"}\n",
    ")\n",
    "\n",
    "# Retrieve results\n",
    "print(\"Workflow Results:\")\n",
    "print(context.get_workflow_result(\"workflow_node_1\"))\n",
    "\n",
    "print(\"\\nPlan Results:\")\n",
    "print(context.get_plan_results_for_workflow(\"workflow_1\"))\n",
    "\n",
    "print(\"\\nReasoning Results:\")\n",
    "print(context.get_reasoning_results_for_plan(\"workflow_1\", \"plan_1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building LLM Context\n",
    "\n",
    "The execution context provides a method to build context for LLM calls, which includes current state and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create example nodes\n",
    "workflow_node = ExecutionNode(\n",
    "    node_id=\"workflow_1\",\n",
    "    node_type=\"task\",\n",
    "    description=\"Process data\"\n",
    ")\n",
    "\n",
    "plan_node = ExecutionNode(\n",
    "    node_id=\"plan_1\",\n",
    "    node_type=\"task\",\n",
    "    description=\"Optimize process\"\n",
    ")\n",
    "\n",
    "reasoning_node = ExecutionNode(\n",
    "    node_id=\"reasoning_1\",\n",
    "    node_type=\"task\",\n",
    "    description=\"Analyze results\"\n",
    ")\n",
    "\n",
    "# Set current nodes in context\n",
    "context.current_workflow = workflow_node\n",
    "context.current_plan = plan_node\n",
    "context.current_reasoning = reasoning_node\n",
    "\n",
    "# Build LLM context\n",
    "llm_context = context.build_llm_context()\n",
    "\n",
    "print(\"LLM Context:\")\n",
    "print(\"Global Context:\")\n",
    "print(llm_context[\"global_context\"])\n",
    "print(\"\\nCurrent Nodes:\")\n",
    "print(f\"- Workflow: {llm_context['current']['workflow']}\")\n",
    "print(f\"- Plan: {llm_context['current']['plan']}\")\n",
    "print(f\"- Reasoning: {llm_context['current']['reasoning']}\")\n",
    "print(\"\\nResults:\")\n",
    "print(f\"- Workflow: {llm_context['results']['workflow']}\")\n",
    "print(f\"- Plan: {llm_context['results']['plan']}\")\n",
    "print(f\"- Reasoning: {llm_context['results']['reasoning']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Best Practices\n\nHere are some best practices for working with the execution context:\n\n1. **Resource Management**\n   - Always initialize resources before use\n   - Clean up resources when done\n   - Use appropriate resource types for each layer\n\n2. **State Management**\n   - Keep state updates atomic\n   - Use appropriate state types for different concerns\n   - Maintain clear state transitions\n\n3. **Results Management**\n   - Store results with appropriate metadata\n   - Use consistent result formats\n   - Clean up old results when no longer needed\n\n4. **Context Building**\n   - Keep global context minimal and relevant\n   - Update current nodes appropriately\n   - Build LLM context only when needed\n\n## Summary\n\nIn this tutorial, we covered:\n\n1. The structure and components of the execution context\n2. How to manage state across different layers\n3. How to handle resources and their lifecycle\n4. How to manage results from different layers\n5. How to build context for LLM calls\n6. Best practices for working with the execution context\n\nThe execution context is a crucial component that enables coordination between different layers of the OpenDXA framework while maintaining state and managing resources effectively.",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
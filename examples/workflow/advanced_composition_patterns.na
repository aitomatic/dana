# Advanced Workflow Composition Patterns - Enterprise Examples
#
# This example demonstrates sophisticated workflow composition patterns
# including parallel execution, conditional branching, retries, and monitoring.
#
# Features demonstrated:
# - Hierarchical workflow composition
# - Parallel vs sequential execution
# - Conditional branching based on data
# - Retry mechanisms with backoff
# - Comprehensive monitoring and alerting

import dana.builtin_types.workflow_system.workflow_engine as workflow_engine
import dana.builtin_types.workflow_system.workflow_step as workflow_step
import dana.builtin_types.workflow_system.context_engine as context_engine
import dana.builtin_types.workflow_system.safety_validator as safety_validator

# Initialize workflow components
log("üîÑ Initializing Advanced Composition Patterns...")

context = context_engine.ContextEngine(max_knowledge_points=1000)
safety = safety_validator.SafetyValidator(strict_mode=False)  # Allow some flexibility
engine = workflow_engine.WorkflowEngine(context_engine=context, safety_validator=safety, max_depth=5)

# Pattern 1: Parallel Data Processing
log("‚ö° Pattern 1: Parallel Data Processing")

def fetch_data_from_source_a(url: str) -> dict:
    """Fetch data from source A."""
    log(f"‚ö° Fetching from source A: {url}")
    # Simulate API call
    return {"source": "A", "data": [1, 2, 3, 4, 5], "timestamp": "2025-07-17T12:00:00Z"}

def fetch_data_from_source_b(url: str) -> dict:
    """Fetch data from source B."""
    log(f"‚ö° Fetching from source B: {url}")
    # Simulate API call
    return {"source": "B", "data": [6, 7, 8, 9, 10], "timestamp": "2025-07-17T12:00:01Z"}

def merge_parallel_data(results: list) -> dict:
    """Merge data from parallel sources."""
    log("‚ö° Merging parallel data sources...")
    
    merged = {
        "combined_data": [],
        "sources": [],
        "processing_time": "0.5s"
    }
    
    for result in results:
        merged["combined_data"].extend(result["data"])
        merged["sources"].append(result["source"])
    
    # Store knowledge about data sources
    context.add_knowledge(
        content=f"Merged data from {len(results)} parallel sources",
        source="data_merger",
        tags=["parallel_processing", "data_integration"]
    )
    
    return merged

# Pattern 2: Conditional Branching
log("üéØ Pattern 2: Conditional Branching")

def analyze_data_size(data: dict) -> dict:
    """Analyze data size to determine processing path."""
    log("üéØ Analyzing data size for processing strategy...")
    
    data_size = len(data["combined_data"])
    
    if data_size < 10:
        strategy = "simple_processing"
        complexity = "low"
    elif data_size < 100:
        strategy = "standard_processing"
        complexity = "medium"
    else:
        strategy = "advanced_processing"
        complexity = "high"
    
    context.add_knowledge(
        content=f"Selected {strategy} for {data_size} items",
        source="strategy_selector",
        tags=["conditional_logic", strategy]
    )
    
    return {
        **data,
        "strategy": strategy,
        "complexity": complexity,
        "data_size": data_size
    }

def simple_processing(data: dict) -> dict:
    """Simple processing for small datasets."""
    log("üéØ Running simple processing...")
    
    result = {
        "processed": [x * 2 for x in data["combined_data"]],
        "method": "simple",
        "processing_time": "0.1s"
    }
    
    context.add_knowledge(
        content="Used simple processing for small dataset",
        source="simple_processor",
        tags=["simple_processing", "low_complexity"]
    )
    
    return {**data, **result}

def standard_processing(data: dict) -> dict:
    """Standard processing for medium datasets."""
    log("üéØ Running standard processing...")
    
    result = {
        "processed": [x * 2 + 1 for x in data["combined_data"]],
        "method": "standard",
        "processing_time": "0.5s",
        "statistics": {
            "sum": sum(data["combined_data"]),
            "avg": sum(data["combined_data"]) / len(data["combined_data"]),
            "max": max(data["combined_data"]),
            "min": min(data["combined_data"])
        }
    }
    
    context.add_knowledge(
        content="Used standard processing for medium dataset",
        source="standard_processor",
        tags=["standard_processing", "medium_complexity"]
    )
    
    return {**data, **result}

def advanced_processing(data: dict) -> dict:
    """Advanced processing for large datasets."""
    log("üéØ Running advanced processing...")
    
    processed = []
    for x in data["combined_data"]:
        # Simulate complex processing
        processed.append(x ** 2 + x * 3 + 1)
    
    result = {
        "processed": processed,
        "method": "advanced",
        "processing_time": "2.0s",
        "statistics": {
            "sum": sum(data["combined_data"]),
            "avg": sum(data["combined_data"]) / len(data["combined_data"]),
            "max": max(data["combined_data"]),
            "min": min(data["combined_data"]),
            "std_dev": 2.5  # Simulated
        }
    }
    
    context.add_knowledge(
        content="Used advanced processing for large dataset",
        source="advanced_processor",
        tags=["advanced_processing", "high_complexity"]
    )
    
    return {**data, **result}

# Pattern 3: Retry with Backoff
log("üîÑ Pattern 3: Retry with Backoff")

def unreliable_api_call(data: dict) -> dict:
    """Simulate unreliable API that sometimes fails."""
    import random
    
    log("üîÑ Making unreliable API call...")
    
    # Simulate 30% failure rate
    if random.random() < 0.3:
        raise RuntimeError("API temporarily unavailable")
    
    # Store retry knowledge
    context.add_knowledge(
        content="API call succeeded after potential retries",
        source="unreliable_api",
        tags=["retry_success", "api_call"]
    )
    
    return {
        **data,
        "api_result": {"status": "success", "data": "processed"},
        "retry_count": 0  # Would be incremented by retry logic
    }

# Pattern 4: Monitoring and Alerting
log("üìä Pattern 4: Monitoring and Alerting")

def monitor_processing(data: dict) -> dict:
    """Monitor processing metrics and generate alerts."""
    log("üìä Monitoring processing metrics...")
    
    metrics = {
        "total_items_processed": len(data.get("processed", [])),
        "processing_method": data.get("method", "unknown"),
        "processing_time": data.get("processing_time", "0s"),
        "timestamp": "2025-07-17T12:05:00Z"
    }
    
    # Generate alerts based on thresholds
    alerts = []
    
    if metrics["total_items_processed"] > 50:
        alerts.append({
            "type": "high_volume",
            "message": f"Processed {metrics['total_items_processed']} items",
            "severity": "info"
        })
    
    if data.get("processing_time", "0s") == "2.0s":
        alerts.append({
            "type": "slow_processing",
            "message": "Processing took longer than expected",
            "severity": "warning"
        })
    
    # Store monitoring knowledge
    context.add_knowledge(
        content=f"Monitoring alert generated: {len(alerts)} alerts",
        source="monitoring_system",
        tags=["monitoring", "metrics", "alerting"]
    )
    
    return {
        **data,
        "monitoring": {
            "metrics": metrics,
            "alerts": alerts,
            "status": "monitored"
        }
    }

# Create workflow steps for different patterns
log("üõ†Ô∏è Creating advanced workflow steps...")

# Parallel processing steps
parallel_steps = [
    engine.create_workflow_step(
        name="fetch_data_a",
        function=fetch_data_from_source_a,
        metadata={"description": "Fetch from API A", "timeout": 5}
    ),
    engine.create_workflow_step(
        name="fetch_data_b", 
        function=fetch_data_from_source_b,
        metadata={"description": "Fetch from API B", "timeout": 5}
    )
]

# Sequential processing steps
sequential_steps = [
    engine.create_workflow_step(
        name="analyze_strategy",
        function=analyze_data_size,
        metadata={"description": "Determine processing strategy"}
    ),
    engine.create_workflow_step(
        name="conditional_process",
        function=lambda data: (
            simple_processing(data) if data["strategy"] == "simple_processing"
            else standard_processing(data) if data["strategy"] == "standard_processing"
            else advanced_processing(data)
        ),
        metadata={"description": "Apply conditional processing"}
    ),
    engine.create_workflow_step(
        name="monitor_results",
        function=monitor_processing,
        metadata={"description": "Monitor processing results"}
    )
]

# Retry-enabled steps
retry_steps = [
    engine.create_workflow_step(
        name="unreliable_api",
        function=unreliable_api_call,
        error_handler=lambda error, data, ctx: {
            **data,
            "api_result": {"status": "failed", "error": str(error)},
            "fallback": "local_processing"
        },
        metadata={"description": "Handle unreliable API with retries", "max_retries": 3}
    )
]

# Demonstrate workflow composition patterns
log("üöÄ Demonstrating Advanced Composition Patterns...")

# Example 1: Sequential Chain
log("üìã Example 1: Sequential Processing Chain")

sample_data = {"url": "https://api.example.com/data"}

# Create a simple data processing workflow
simple_chain = [
    engine.create_workflow_step(
        name="prepare_data",
        function=lambda x: {"combined_data": [1, 2, 3, 4, 5], "sources": ["manual"]},
        metadata={"description": "Prepare sample data"}
    ),
    engine.create_workflow_step(
        name="process_data",
        function=analyze_data_size,
        metadata={"description": "Analyze and process"}
    ),
    engine.create_workflow_step(
        name="monitor",
        function=monitor_processing,
        metadata={"description": "Monitor results"}
    )
]

result1 = engine.execute(simple_chain, {}, workflow_id="sequential_chain_001")
log(f"‚úÖ Sequential chain result: {result1.get('monitoring', {}).get('metrics', {})}")

# Example 2: Conditional Branching
log("üéØ Example 2: Conditional Branching")

# Create different processing branches based on data size
def create_conditional_workflow(data_size: int):
    """Create workflow based on data size."""
    
    if data_size < 10:
        return [engine.create_workflow_step(
            name="simple_process",
            function=simple_processing,
            metadata={"description": "Simple processing for small data"}
        )]
    elif data_size < 100:
        return [engine.create_workflow_step(
            name="standard_process",
            function=standard_processing,
            metadata={"description": "Standard processing for medium data"}
        )]
    else:
        return [engine.create_workflow_step(
            name="advanced_process",
            function=advanced_processing,
            metadata={"description": "Advanced processing for large data"}
        )]

# Test different sizes
test_sizes = [5, 50, 200]
for size in test_sizes:
    workflow = create_conditional_workflow(size)
    test_data = {"combined_data": list(range(size))}
    result = engine.execute(workflow, test_data, workflow_id=f"conditional_{size}")
    log(f"üìä Size {size}: Used {result.get('method', 'unknown')} processing")

# Example 3: Error Handling and Monitoring
log("‚ö†Ô∏è Example 3: Error Handling and Recovery")

# Test error handling
def failing_step(data: dict) -> dict:
    """A step that always fails."""
    log("‚ö†Ô∏è Attempting failing operation...")
    raise RuntimeError("Simulated failure - this is expected")

def recovery_step(data: dict) -> dict:
    """Recovery step for failed operations."""
    log("üîÑ Running recovery step...")
    
    return {
        **data,
        "recovery": True,
        "fallback_result": "recovered_data",
        "status": "recovered"
    }

error_handling_workflow = [
    engine.create_workflow_step(
        name="failing_operation",
        function=failing_step,
        error_handler=lambda error, data, ctx: {
            **data,
            "error": str(error),
            "recovery_triggered": True
        },
        metadata={"description": "Test error handling"}
    ),
    engine.create_workflow_step(
        name="recovery_operation",
        function=recovery_step,
        pre_conditions=[lambda data, ctx: data.get("recovery_triggered") is True],
        metadata={"description": "Recovery step"}
    )
]

result3 = engine.execute(error_handling_workflow, {}, workflow_id="error_handling_001")
log(f"‚úÖ Error handling result: {result3.get('status', 'unknown')}")

# Example 4: Workflow with Context Integration
log("üß† Example 4: Context-Aware Processing")

context_aware_workflow = [
    engine.create_workflow_step(
        name="collect_context",
        function=lambda data: {
            "context_data": context.search_knowledge("processing", limit=5),
            "strategy": "context_aware"
        },
        metadata={"description": "Collect processing context"}
    ),
    engine.create_workflow_step(
        name="adapt_processing",
        function=lambda data: {
            **data,
            "adapted": True,
            "context_size": len(data.get("context_data", [])),
            "final_result": "context_aware_processing"
        },
        metadata={"description": "Adapt processing based on context"}
    )
]

result4 = engine.execute(context_aware_workflow, {}, workflow_id="context_aware_001")
log(f"‚úÖ Context-aware result: {result4.get('final_result', 'unknown')}")

# Show final statistics
log("üìä Final Advanced Composition Statistics:")
stats = context.get_stats()
log(f"üìà Total knowledge points: {stats['total_knowledge_points']}")
log(f"üìà Unique tags: {len(stats['unique_tags'])}")
log(f"üìà Sources: {', '.join(stats['sources'])}")

# Show processing patterns
processing_knowledge = context.search_knowledge("processing", limit=10)
log(f"üìä Processing patterns documented: {len(processing_knowledge)}")

# Create final summary
log("üéØ Advanced Composition Patterns Summary:")
log("  ‚úÖ Sequential processing chains")
log("  ‚úÖ Conditional branching based on data")
log("  ‚úÖ Error handling with recovery")
log("  ‚úÖ Context-aware processing")
log("  ‚úÖ Monitoring and alerting")
log("  ‚úÖ Knowledge integration")
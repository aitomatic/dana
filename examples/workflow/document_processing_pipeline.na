# Document Processing Pipeline - Enterprise Workflow Example
# 
# This example demonstrates a real-world document processing workflow
# that combines OCR, content analysis, and knowledge extraction using
# Dana's enterprise workflow capabilities.
#
# Features demonstrated:
# - Hierarchical workflow composition
# - Safety validation at each step
# - Context-aware processing
# - Error handling and recovery
# - Knowledge curation integration
# - Automatic metadata construction from poet() decorators

# Initialize workflow components
log("üîß Initializing Document Processing Pipeline...")

context = ContextEngine(max_knowledge_points=5000)
safety = SafetyValidator(strict_mode=True)
engine = WorkflowEngine(context_engine=context, safety_validator=safety)

# Import POET workflow helpers for automatic metadata construction
from dana.frameworks.poet.core.workflow_helpers import create_workflow_metadata, create_pipeline_metadata

# Step 1: Document Ingestion
log("üìÑ Step 1: Document Ingestion")

@poet(domain="document_processing", retries=3)
def ingest_document(file_path):
    """Ingest document from file system with validation."""
    log("üìÇ Ingesting document: " + file_path)
    
    # Simulate document ingestion
    if not file_path.endswith(".pdf") and not file_path.endswith(".docx") and not file_path.endswith(".txt"):
        raise ValueError("Unsupported file format: " + file_path)
    
    return {
        "file_path": file_path,
        "file_type": file_path.split(".").last(),
        "size_mb": 2.5,
        "ingest_timestamp": "2025-07-17T10:30:00Z"
    }

# Step 2: OCR Processing
log("üîç Step 2: OCR Processing")

@poet(domain="ocr", timeout=30)
def perform_ocr(document):
    """Perform OCR on ingested document."""
    log("üîç Processing " + document.file_type + " document with OCR")
    
    # Simulate OCR processing
    ocr_result = {
        "raw_text": "This is a sample document containing important business insights...",
        "confidence": 0.95,
        "pages": 3,
        "processing_time": "2.3s"
    }
    
    # Store knowledge about processing
    context.add_knowledge(
        content="OCR processed " + document.file_path + " with " + ocr_result.confidence + " confidence",
        source="ocr_engine",
        tags=["ocr", "document_processing", document.file_type]
    )
    
    return document.merge(ocr_result)

# Step 3: Content Analysis
log("üß† Step 3: Content Analysis")

@poet(domain="content_analysis", operate={"model": "gpt-4"})
def analyze_content(ocr_result):
    """Analyze content for key insights and topics."""
    log("üß† Analyzing document content for insights")
    
    # Simulate content analysis
    analysis = {
        "topics": ["business_strategy", "market_analysis", "competitive_intelligence"],
        "key_insights": [
            "Market growing at 15% annually",
            "Competitive advantage in AI capabilities",
            "Customer retention improving"
        ],
        "sentiment": "positive",
        "urgency": "medium"
    }
    
    # Store analyzed insights
    for insight in analysis.key_insights:
        context.add_knowledge(
            content=insight,
            source="content_analyzer",
            tags=["insight", analysis.sentiment].concat(analysis.topics)
        )
    
    return ocr_result.merge(analysis)

# Step 4: Knowledge Extraction
log("üìö Step 4: Knowledge Extraction")

@poet(domain="knowledge_extraction", enforce={"confidence_threshold": 0.8})
def extract_knowledge(analysis):
    """Extract structured knowledge from analysis."""
    log("üìö Extracting structured knowledge")
    
    # Simulate knowledge extraction
    knowledge = {
        "entities": [
            {"type": "company", "name": "Acme Corp", "confidence": 0.92},
            {"type": "metric", "name": "growth_rate", "value": "15%", "confidence": 0.88}
        ],
        "relationships": [
            {"from": "Acme Corp", "to": "growth_rate", "type": "has_metric"}
        ],
        "knowledge_graph": "acme_growth_analysis"
    }
    
    # Store extracted entities
    for entity in knowledge.entities:
        context.add_knowledge(
            content="Entity: " + entity.name + " (" + entity.type + ") - " + entity.confidence + " confidence",
            source="knowledge_extractor",
            tags=["entity", entity.type, "confidence_" + entity.confidence]
        )
    
    return analysis.merge(knowledge)

# Step 5: Report Generation
log("üìä Step 5: Report Generation")

@poet(domain="reporting", operate={"format": "json"})
def generate_report(final_data):
    """Generate comprehensive processing report."""
    log("üìä Generating final processing report")
    
    # Calculate confidence scores
    confidence_scores = []
    for entity in final_data.entities:
        confidence_scores.push(entity.confidence)
    
    report = {
        "processing_summary": {
            "document": final_data.file_path,
            "total_processing_time": "8.7s",
            "insights_extracted": final_data.key_insights.length(),
            "entities_found": final_data.entities.length()
        },
        "knowledge_base_stats": {
            "total_knowledge_points": context.get_stats().total_knowledge_points,
            "topics_covered": final_data.topics.length(),
            "confidence_scores": confidence_scores
        },
        "recommendations": [
            "Schedule follow-up analysis in 30 days",
            "Focus on competitive intelligence insights",
            "Consider customer retention strategies"
        ]
    }
    
    # Store final report
    context.add_knowledge(
        content="Document processing report for " + final_data.file_path,
        source="report_generator",
        tags=["report", "document_processing", "summary"],
        metadata=report
    )
    
    return report

# Create workflow using pipe syntax
log("üõ°Ô∏è Creating workflow with pipe syntax...")

# Define the complete workflow pipeline using Dana's pipe operator
document_processing_workflow = ingest_document | perform_ocr | analyze_content | extract_knowledge | generate_report

# Automatically construct workflow metadata from poet() decorators and docstrings
workflow_metadata = create_pipeline_metadata(
    document_processing_workflow,
    workflow_id="doc_processing_001",
    description="Enterprise document processing pipeline with automatic metadata construction",
    version="2.0.0"
)

# Add workflow-level metadata
document_processing_workflow.workflow_metadata = workflow_metadata

# Execute the complete workflow
log("üöÄ Executing Document Processing Workflow...")

input_document = "quarterly_report_2025_q2.pdf"
result = document_processing_workflow(input_document)

log("‚úÖ Document Processing Complete!")
log("üìä Final Report: " + result)

# Demonstrate automatic metadata access
log("üìã Automatically Generated Workflow Metadata:")
log("Workflow ID: " + document_processing_workflow.workflow_metadata.workflow_id)
log("Description: " + document_processing_workflow.workflow_metadata.description)
log("Version: " + document_processing_workflow.workflow_metadata.version)

# Show automatically extracted step metadata
log("üîç Automatically Extracted Step Metadata:")
for step_info in document_processing_workflow.workflow_metadata.steps:
    log("Step: " + step_info.name + " - " + step_info.description)
    if step_info.has_key("retry_count"):
        log("  Retry Count: " + step_info.retry_count)
    if step_info.has_key("timeout"):
        log("  Timeout: " + step_info.timeout + "s")
    if step_info.has_key("domain"):
        log("  Domain: " + step_info.domain)

# Demonstrate knowledge retrieval
log("üîç Retrieving processed knowledge...")
python_knowledge = context.search_knowledge("python", limit=3)
log("Found " + python_knowledge.length() + " Python-related knowledge points")

# Show workflow statistics
log("üìà Workflow Statistics:")
stats = context.get_stats()
log("Total knowledge points: " + stats.total_knowledge_points)
log("Unique sources: " + stats.sources.length())
log("Topics covered: " + stats.unique_tags)

# Demonstrate alternative metadata construction methods
log("üîÑ Alternative Metadata Construction Methods:")

# Method 1: From individual functions
functions = [ingest_document, perform_ocr, analyze_content, extract_knowledge, generate_report]
manual_metadata = create_workflow_metadata(
    functions,
    workflow_id="doc_processing_manual_001",
    description="Manual function list metadata construction"
)
log("Manual metadata constructed for " + manual_metadata.steps.length() + " steps")

# Method 2: Using the backward compatibility function
legacy_metadata = build_workflow_metadata(
    ingest_document, perform_ocr, analyze_content, extract_knowledge, generate_report,
    workflow_id="doc_processing_legacy_001",
    description="Legacy metadata construction method"
)
log("Legacy metadata constructed for " + legacy_metadata.steps.length() + " steps")
# Dana TUI Testing Guide

This directory contains comprehensive tests for the Dana TUI following [Textual testing best practices](https://textual.textualize.io/guide/testing/).

## Test Structure

### Test Types

- **Unit Tests**: Test individual components in isolation
  - `test_runtime.py` - Core runtime components
  - `test_taskman.py` - Task management
  - `test_router.py` - Command parsing and routing
  - `test_runtime_improved.py` - Enhanced unit tests with better mocking

- **UI Component Tests**: Test user interactions using Pilot
  - `test_ui_components.py` - Real user interaction testing

- **Integration Tests**: Test complete workflows
  - `test_integration.py` - End-to-end user scenarios

- **Snapshot Tests**: Visual regression testing
  - `test_snapshots.py` - UI appearance validation

- **Performance Tests**: Test responsiveness and scalability
  - `test_performance.py` - Performance benchmarks

## Running Tests

### Basic Commands

```bash
# Run all tests
pytest tests/tui/

# Run with coverage
pytest tests/tui/ --cov=dana.tui --cov-report=html

# Run only unit tests
pytest tests/tui/ -m "unit"

# Run only UI tests
pytest tests/tui/ -m "ui"

# Run only integration tests
pytest tests/tui/ -m "integration"

# Run only snapshot tests
pytest tests/tui/ -m "snapshot"

# Run only performance tests
pytest tests/tui/ -m "performance"

# Skip slow tests
pytest tests/tui/ -m "not slow"
```

### Snapshot Testing

```bash
# Run snapshot tests
pytest tests/tui/test_snapshots.py

# Update snapshots (after reviewing changes)
pytest tests/tui/test_snapshots.py --snapshot-update

# View snapshot report
# Open the HTML report generated by pytest-textual-snapshot
```

### Using the Test Runner

```bash
# Run all tests with the custom runner
python tests/tui/run_tests.py

# Run with specific options
python tests/tui/run_tests.py --verbose --coverage

# Run specific test file
python tests/tui/run_tests.py --file test_ui_components

# Run specific test method
python tests/tui/run_tests.py --test test_ui_components.py::test_terminal_widget_basic

# Check dependencies
python tests/tui/run_tests.py --check-deps
```

## Test Best Practices

### 1. Use Pilot for UI Testing

Following Textual's recommendation, use Pilot to simulate real user interactions:

```python
@pytest.mark.asyncio
async def test_user_interaction():
    app = DanaTUI()
    async with app.run_test() as pilot:
        # Simulate user typing
        await pilot.type("agent test")
        await pilot.press("enter")
        
        # Verify result
        assert "test" in app.sandbox.list()
```

### 2. Test Complete Workflows

Don't just test individual functions - test complete user scenarios:

```python
@pytest.mark.asyncio
async def test_complete_agent_workflow():
    """Test complete agent creation and interaction workflow."""
    app = DanaTUI()
    async with app.run_test() as pilot:
        # Create agent
        await pilot.type("agent research")
        await pilot.press("enter")
        
        # Send message
        await pilot.type("@research find papers")
        await pilot.press("enter")
        
        # Verify complete workflow
        assert "research" in app.sandbox.list()
```

### 3. Use Snapshot Testing for Visual Regression

Catch UI changes automatically:

```python
def test_ui_appearance(snap_compare):
    """Test UI appearance with snapshot."""
    assert snap_compare("dana/tui/app.py")
```

### 4. Mock External Dependencies

Keep tests fast and reliable:

```python
@patch('dana.tui.core.runtime.REPL')
def test_execution_with_mock(mock_repl, sandbox):
    mock_repl.return_value.execute.return_value = "result"
    result = sandbox.execute_string("2 + 2")
    assert result.success is True
```

### 5. Test Error Conditions

Ensure graceful error handling:

```python
@pytest.mark.asyncio
async def test_error_recovery():
    app = DanaTUI()
    async with app.run_test() as pilot:
        # Try invalid commands
        await pilot.type("invalid command")
        await pilot.press("enter")
        
        # Verify app still works
        await pilot.type("agent valid")
        await pilot.press("enter")
        assert "valid" in app.sandbox.list()
```

### 6. Test Performance

Ensure UI remains responsive:

```python
@pytest.mark.asyncio
async def test_ui_responsiveness():
    app = DanaTUI()
    async with app.run_test() as pilot:
        start_time = time.time()
        
        # Perform operations
        for i in range(10):
            await pilot.type(f"agent agent{i}")
            await pilot.press("enter")
        
        end_time = time.time()
        assert end_time - start_time < 5.0  # Should be fast
```

## Test Fixtures

### Available Fixtures

- `dana_tui_pilot`: DanaTUI app with pilot for testing
- `basic_app_pilot`: Basic Textual app with pilot
- `mock_sandbox`: Mock sandbox for testing
- `mock_agent`: Mock agent for testing
- `slow_agent`: Slow mock agent for timeout testing
- `error_agent`: Agent that raises errors for error testing
- `test_config`: Test configuration

### Using Fixtures

```python
@pytest.mark.asyncio
async def test_with_fixtures(dana_tui_pilot, mock_agent):
    app = dana_tui_pilot.app
    agent = mock_agent("test")
    app.sandbox.register(agent)
    
    # Test with the fixtures
    assert "test" in app.sandbox.list()
```

## Test Markers

### Available Markers

- `@pytest.mark.unit`: Unit tests
- `@pytest.mark.ui`: UI component tests
- `@pytest.mark.integration`: Integration tests
- `@pytest.mark.snapshot`: Snapshot tests
- `@pytest.mark.performance`: Performance tests
- `@pytest.mark.slow`: Slow-running tests

### Using Markers

```python
@pytest.mark.ui
@pytest.mark.asyncio
async def test_ui_component():
    """This test will be marked as UI test."""
    pass

@pytest.mark.slow
def test_slow_operation():
    """This test will be marked as slow."""
    pass
```

## Debugging Tests

### Common Issues

1. **Async Test Failures**: Ensure tests are marked with `@pytest.mark.asyncio`
2. **Pilot Timeouts**: Increase timeout or simplify test operations
3. **Snapshot Failures**: Review changes and update snapshots if appropriate
4. **Import Errors**: Ensure Dana TUI package is properly installed

### Debug Mode

```bash
# Run with debug logging
TEXTUAL_LOG=debug pytest tests/tui/test_ui_components.py -v

# Run with more verbose output
pytest tests/tui/ -v -s --tb=short
```

### Interactive Debugging

```python
@pytest.mark.asyncio
async def test_debug_example():
    app = DanaTUI()
    async with app.run_test() as pilot:
        # Add breakpoint for debugging
        import pdb; pdb.set_trace()
        
        await pilot.type("agent test")
        await pilot.press("enter")
```

## Continuous Integration

### GitHub Actions Example

```yaml
name: Test Dana TUI
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -e .
          pip install pytest pytest-asyncio pytest-textual-snapshot
      
      - name: Run unit tests
        run: pytest tests/tui/ -m "unit" --cov=dana.tui
      
      - name: Run UI tests
        run: pytest tests/tui/ -m "ui"
      
      - name: Run integration tests
        run: pytest tests/tui/ -m "integration"
      
      - name: Run snapshot tests
        run: pytest tests/tui/ -m "snapshot"
      
      - name: Upload coverage
        uses: codecov/codecov-action@v3
```

## Performance Benchmarks

### Key Metrics

- **Startup Time**: < 2 seconds
- **UI Responsiveness**: < 5 seconds for 10 operations
- **Navigation Speed**: < 2 seconds for 50 tab presses
- **Memory Usage**: Stable with 50+ agents
- **Error Recovery**: < 3 seconds for 20 errors

### Running Benchmarks

```bash
# Run performance tests
pytest tests/tui/test_performance.py -v

# Run with timing
pytest tests/tui/test_performance.py -v --durations=10
```

## Contributing

### Adding New Tests

1. **Choose the right test type**:
   - Unit tests for isolated components
   - UI tests for user interactions
   - Integration tests for workflows
   - Snapshot tests for visual changes
   - Performance tests for benchmarks

2. **Follow naming conventions**:
   - `test_*.py` for test files
   - `test_*` for test functions
   - Clear, descriptive names

3. **Use appropriate markers**:
   - Mark async tests with `@pytest.mark.asyncio`
   - Mark test types with appropriate markers
   - Mark slow tests with `@pytest.mark.slow`

4. **Write clear assertions**:
   - Test one thing per test
   - Use descriptive assertion messages
   - Test both success and failure cases

### Test Quality Checklist

- [ ] Test covers the intended functionality
- [ ] Test is independent and can run in isolation
- [ ] Test has clear, descriptive name
- [ ] Test uses appropriate fixtures
- [ ] Test handles both success and error cases
- [ ] Test follows Textual best practices
- [ ] Test is marked with appropriate markers
- [ ] Test includes proper assertions
- [ ] Test runs in reasonable time (< 5 seconds for unit tests)

## Resources

- [Textual Testing Guide](https://textual.textualize.io/guide/testing/)
- [Textual Pilot Documentation](https://textual.textualize.io/api/pilot/)
- [pytest-textual-snapshot](https://github.com/Textualize/pytest-textual-snapshot)
- [pytest-asyncio](https://pytest-asyncio.readthedocs.io/)

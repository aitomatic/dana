# Test that agent.chat() now properly uses its own LLM resource

# Define an agent blueprint with LLM config
agent_blueprint SmartAgent:
    name: str = "Smart Agent"
    expertise: str = "general"
    config: dict = {
        "llm_model": "openai:gpt-4o-mini",
        "llm_temperature": 0.3
    }

# Create instance from blueprint
smart_agent = SmartAgent()

print("=== Agent LLM Fixed Test ===")
print("SmartAgent type:", type(smart_agent))
print("SmartAgent config:", smart_agent.config)

# Test chat functionality to see if it's now using LLM
print("\n--- Testing Chat with LLM (Fixed) ---")
response = smart_agent.chat("Hello! What model are you using?")
print("Response:", response)
print("Response type:", type(response))

# Test if response is a promise (async) or direct (sync)
if "Promise" in str(type(response)):
    print("‚úÖ Response is a Promise (using LLM)")
else:
    print("‚ùå Response is synchronous (fallback)")

# Test a more complex query
print("\n--- Testing Complex Query ---")
response2 = smart_agent.chat("Can you explain quantum computing in simple terms?")
print("Response:", response2)

# Check if the response looks like it came from an LLM
if "quantum" in str(response2).lower() or "computing" in str(response2).lower():
    print("‚úÖ Response appears to be from an LLM")
elif "fallback" in str(response2).lower() or "limited" in str(response2).lower():
    print("‚ùå Response appears to be fallback")
else:
    print("ü§î Response is ambiguous - could be either")

# Test that it's using the agent's own LLM resource
print("\n--- Testing Agent's Own LLM Resource ---")
try:
    llm_resource = smart_agent._get_llm_resource()
    if llm_resource:
        print("‚úÖ Agent has its own LLM resource:", llm_resource)
        print("LLM Resource name:", llm_resource.name)
        print("LLM Resource model:", llm_resource.model)
    else:
        print("‚ùå Agent has no LLM resource")
except Exception as e:
    print("‚ùå Error getting LLM resource:", e)

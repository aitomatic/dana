# Final test of agent config-based LLM parameters (working version)

# Define an agent blueprint with comprehensive LLM config
agent_blueprint SmartAgent:
    name: str = "Smart Agent"
    expertise: str = "general"
    config: dict = {
        "llm_model": "openai:gpt-4o-mini",
        "llm_temperature": 0.3,
        "llm_max_tokens": 1000,
        "llm_provider": "openai"
    }

# Create instance from blueprint
smart_agent = SmartAgent()

# Create another instance with custom config
coding_agent = SmartAgent(
    expertise = "coding",
    config = {
        "llm_model": "anthropic:claude-3-sonnet",
        "llm_temperature": 0.1,
        "llm_max_tokens": 2000
    }
)

# Create simple agent
agent SimpleAgent

print("=== Final Agent Config LLM Test ===")
print("SmartAgent type:", type(smart_agent))
print("SmartAgent config:", smart_agent.config)
print("CodingAgent type:", type(coding_agent))
print("CodingAgent config:", coding_agent.config)
print("SimpleAgent type:", type(SimpleAgent))

# Test chat functionality
print("\n--- Testing SmartAgent ---")
response1 = smart_agent.chat("Hello! What model are you using?")
print("Response:", response1)

print("\n--- Testing CodingAgent ---")
response2 = coding_agent.chat("Hello! What model are you using?")
print("Response:", response2)

print("\n--- Testing SimpleAgent ---")
response3 = SimpleAgent.chat("Hello! What model are you using?")
print("Response:", response3)

print("\n=== Summary ===")
print("✅ Agent blueprints with config work")
print("✅ Agent instances with custom config work")
print("✅ Simple agents work")
print("✅ Chat method uses config-based LLM resources")
print("✅ Type system shows Agent[TypeName]")
print("✅ Each agent can have its own LLM configuration")

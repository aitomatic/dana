# Comprehensive test of agent config-based LLM parameters

# Define an agent blueprint with comprehensive LLM config
agent_blueprint SmartAgent:
    name: str = "Smart Agent"
    expertise: str = "general"
    config: dict = {
        "llm_model": "openai:gpt-4o-mini",
        "llm_temperature": 0.3,
        "llm_max_tokens": 1000,
        "llm_provider": "openai"
    }

# Create instance from blueprint
smart_agent = SmartAgent()

# Create singleton agent with custom LLM config (using constructor)
custom_agent = SmartAgent(
    expertise = "coding",
    config = {
        "llm_model": "anthropic:claude-3-sonnet",
        "llm_temperature": 0.1,
        "llm_max_tokens": 2000
    }
)

# Create simple agent with minimal config
agent SimpleAgent:
    config = {
        "llm_model": "openai:gpt-3.5-turbo",
        "llm_temperature": 0.7
    }

print("=== Comprehensive Agent Config LLM Test ===")
print("SmartAgent type:", type(smart_agent))
print("SmartAgent config:", smart_agent.config)
print("CustomAgent type:", type(custom_agent))
print("CustomAgent config:", custom_agent.config)
print("SimpleAgent type:", type(SimpleAgent))
print("SimpleAgent config:", SimpleAgent.config)

# Test chat functionality
print("\n--- Testing SmartAgent ---")
response1 = smart_agent.chat("Hello! What model are you using?")
print("Response:", response1)

print("\n--- Testing CustomAgent ---")
response2 = custom_agent.chat("Hello! What model are you using?")
print("Response:", response2)

print("\n--- Testing SimpleAgent ---")
response3 = SimpleAgent.chat("Hello! What model are you using?")
print("Response:", response3)

print("\n=== Summary ===")
print("✅ Agent blueprints with config work")
print("✅ Agent instances with custom config work")
print("✅ Simple agents with config work")
print("✅ Chat method uses config-based LLM resources")
print("✅ Type system shows Agent[TypeName]")

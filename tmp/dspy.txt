Skip to main content
Who is using DSPy? : r/LocalLLaMA


r/LocalLLaMA
Current search is within r/LocalLLaMA

Remove r/LocalLLaMA filter and expand search to all of Reddit
Search in r/LocalLLaMA
Advertise on Reddit

Open chat
Create
Create post

Open inbox
2

User Avatar
Expand user menu
Skip to NavigationSkip to Right Sidebar

Back
r/LocalLLaMA icon
Go to LocalLLaMA
r/LocalLLaMA
•
1 yr. ago
purple_sack_lunch

Who is using DSPy?
Discussion
I'm intrigued by this approach of programming versus prompting LLMs. I've gone through a few tutorials and at a point wondering if I should actively invest my time to learn this approach? Or, just keep an eye on it as it developed.

The learning curve seems quite steep. Does anybody have compelling use cases to share? I'm mostly interested in this if it can enhance tasks like text classification or performing some type of information retrieval.


Upvote
97

Downvote

97
Go to comments


Share
Share
u/MarvelRivals avatar
MarvelRivals
•
Promoted

全世界4000万人のプレイヤー参戦
Download
marvelrivals.com
Thumbnail image: 全世界4000万人のプレイヤー参戦
Join the conversation
Sort by:

Best

Search Comments
Expand comment search
Comments Section

[deleted]
•
1y ago
Esies
•
1y ago
•
Edited 1y ago
I find DSPy as a framework a tad bit immature in its current form, but the concept is really interesting and, IMO worth keeping an eye on (or, better yet, contributing to) until it is developed better.

I didn't like some of the initial design decisions they made to make it sound more complicated than it should (e.g., calling optimizers "teleprompters", which they are now changing). I also don't like how its current way to automate prompt engineering uses its own library-defined prompts, with no easy way to change them, which makes it useless for any non-English use-case (so much for programming vs prompting, eh?).

For what is worth, they are currently working on a major refactoring roadmap, where they are supposedly adopting things that will make it a lot more useful than its current form, like adding support for chat mode, integration with litellm, and having cleaner abstraction. So that's something to look forward to.



Upvote
36

Downvote

Reply
reply

Award

Share
Share

dimsumham
•
1y ago
Ok so the thing that confused me the most was how it compiles the prompt.

If I understand correctly it's a o Pre built library of techniques plus adding in few shot. Is this accurate?



Upvote
5

Downvote

Reply
reply

Award

Share
Share

Esies
•
1y ago
Basically, when you write a dspy "signature" like:

prog = dspy.Signature("question -> answer") # taken from their docs
DSpy is under the hood passing this to the model:

Given the fields `question`, produce the fields `answer`.

---

Follow the following format.

Question: ${question}
Answer: ${answer}

---

Question: 
Let's ignore the fact that the prompt template they use is completely arbitrary (no better than what Langchain does). NOWHERE in their documentation explains that this is what they are passing to the model. If your signature is in another language, DSpy will still inject this to your prompt. Also, clearly if you want to use an instruct/chat model with its own prompt template, this is not the best way to prompt it. So as it currently stands, using DSPy will probably hurt you more than it'll help.



Upvote
24

Downvote

Reply
reply

Award

Share
Share

dimsumham
•
1y ago
Thank you! This clears it up.

The compiler magic def the most confusing part but think I'm getting the hang of it.


Upvote
5

Downvote

Reply
reply

Award

Share
Share

bitemyassnow
•
6mo ago
Somebody finally said it. So DSPY = another prompt wrapper library then. Not a fancy framework


Upvote
3

Downvote

Reply
reply

Award

Share
Share

cryptokaykay
•
1y ago
Yea that’s mostly correct. There’s no “compilation” of prompts per se. it just creates the prompt using the signature. It has a bunch of prompt templates under the hood



Upvote
12

Downvote

Reply
reply

Award

Share
Share

Imaginary-Garbage731
•
10mo ago
Wait, so when they say "optimizing prompt through compiling steps" it is basically trying different prompt template to see which one optimizes the metric? Where can I check the prompt templates it tried?


Upvote
3

Downvote

Reply
reply

Award

Share
Share

cryptokaykay
•
1y ago
Yea +1. The current codebase lacks clean design and abstractions. And it also has a translation layer between DSPy and legacy DSP which is a bit ugly.



Upvote
3

Downvote

Reply
reply

Award

Share
Share

u/Fast_Donut_8329 avatar
Fast_Donut_8329
•
7mo ago
Really, all I wanted to see is what's the initial prompt was, and what the final prompt like after the "compiling". I don't know why can't I find any example like this. And why they have to make the code so confusing to achive a trivial prompt template, which is in no way adaptive at all.


Upvote
2

Downvote

Reply
reply

Award

Share
Share


[deleted]
•
1y ago
u/CometML avatar
CometML
•
Promoted

We built an observability + evaluation platform for local LLMs. 100% free and open source.
github.com
Thumbnail image: We built an observability + evaluation platform for local LLMs. 100% free and open source.
noprompt
•
1y ago
You don’t need any of these frameworks. Keep your life simple and use function composition and solve your own problems not problems these frameworks think you have. I have been working with LLMs for almost two years now, programming for almost 2 decades, and this is my conclusion.



Upvote
71

Downvote

Reply
reply

Award

Share
Share

Eralyon
•
1y ago
I am a hobbyist bad programmer (#notmyjob), always have been.

I tried Langchain, found it uselessly complicated, came back to just

python + lama.cpp

or python + exllamav2
Never been happier.



Upvote
47

Downvote

Reply
reply

Award

Share
Share

henrycahill
•
1y ago
Exllama2 is by far the most impressive for my hardware


Upvote
8

Downvote

Reply
reply

Award

Share
Share

anzzax
•
1y ago
Have you actually checked DSPy? The main idea is to define input and output of what you expect from LLM and DSPy will do optimization and figure out prompts. In my opinion it's exactly extension of what you are saying, do proper function composition, stop guessing with prompts and hints to get precise response from LLMs, delegate prompts magic to DSPy.

The piece I don't like the most with LLM applications is prompt engineering, it's very brittle, you are writing silly prompts in a hope to get what you need, and when you decide to try different model everything is broken. The promise of DSPy is to solve this.

I'm also over 2 decades in software development and avoiding overengineered frameworks like the plague. More to this, I don't believe in "no code" solutions and "English the New Hottest Programming Language", those are great for sale pitches and horrible for any scalable and predictable software.

I'm evaluating DSPy and have high hopes, I'll be sharing my findings in this subreddit.



Upvote
59

Downvote

Reply
reply

Award

Share
Share

noprompt
•
1y ago
Sorry for the late reply. Yes, I checked it out a couple weeks ago after hearing about it somewhere. Nodded my head in agreement with the problems mentioned in the README. Read over the docs and thought their cure was worse than the disease.

Personally, I don’t see the benefit of picking up their abstractions when it will almost certainly turn out to be a net loss of time that I could’ve spent solving my specific problem with the specific code I need. Same can be said for LangChain while I’m on this topic.

The main issue I have with all these frameworks is that they don’t give me better semantics. I’m just getting the same things I already have but with new names and more parameters.

The second issue I have is that they actually make software development harder in this space. If some team in the company is using one of these frameworks with a model my team has tuned, I have to pick it up just to understand to their complaints. Most of the time these teams are picking up these frameworks because they haven’t been trained on how to work with LLMs from the ground up. They don’t understand why models respond differently, they don’t understand embeddings, etc. They’re following the hype/stars argument from authority without understanding the technical choice they’re making and the social problems they’re creating.

So, yeah, I’ve taken an honest look and honestly think that, on the balance, stuff like DSPy is a net negative contribution.



Upvote
22

Downvote

Reply
reply

Award

Share
Share

Esies
•
1y ago
•
Edited 1y ago
Couldn't agree more with your take. I wish dspy didn't try to introduce their own terminology or try to force analogies with optimizing neural network.

I find frameworks like litellm, and instructor useful because they use extremely clear abstractions and try to keep them as simple as possible. Anyone from the newcomer to LLMs to the experienced can see them and understand what their purpose is and how it might fit their use-case almost inmediately.


Upvote
8

Downvote

Reply
reply

Award

Share
Share

[deleted]
•
1y ago
The worst part about LLMs for me is that we're dealing with a black box with non-deterministic outputs. You can get good results 90% of the time but your outer code loop needs to handle the leftover cases where the LLM spits out garbage.



Upvote
9

Downvote

Reply
reply

Award

Share
Share

cryptokaykay
•
1y ago
And it’s impossible to reproduce, debug and fix when it fails 10% of the time



Upvote
9

Downvote

Reply
reply

Award

Share
Share

ktpr
•
10mo ago
Can't you just set the temperature to 0 under the same model for reproducibility?


Upvote
3

Downvote

Reply
reply

Award

Share
Share

Hubba_Bubba_Lova
•
1y ago
New to “LLM programming” but running into same failure issue. What do you do now to handle this failure rate? Is there a framework(s) that help (counter to the OP comment)? Or do you have a different strategy?



Upvote
2

Downvote

Reply
reply

Award

Share
Share

cryptokaykay
•
1y ago
Identify why its happening. It can be

context data if it's rag

prompt

model
or a combination of all 3. identify where the failure is and iterate


Upvote
3

Downvote

Reply
reply

Award

Share
Share

BiteFancy9628
•
1mo ago
Use a random seed in testing. Heck use a random seed in prod. If you want reproducibility without pinning it always to the same answer you can generate a new seed randomly with every api call but just log it. This way it’ll give you the same response every time.


Upvote
1

Downvote

Reply
reply

Award

Share
Share

u/justdoitanddont avatar
justdoitanddont
•
1y ago
I am very interested to hear what your conclusion is


Upvote
2

Downvote

Reply
reply

Award

Share
Share

AussieMikado
•
10mo ago
Yup, it's just really unfortunate that approach won't work either. What we are encountering here, without knowing it, is the edge. It might not be the edge of what's possible, but it's well past the point of what's sensible. Every programmer has, devolved through their employment or, as direct liability, a duty of care. This isn't well recognised in our industry. Unless Ai companies successfully lobby that, like a publisher isn't liable for the harms their content causes, neither are they responsible for the completion prediction that triggered your mortgage foreclosure, this stuff really matters. Any attempt at increasing precision that can't be 100% reproduced with 5 nines of reliability, still, won't be good enough. I can imagine sets of prompts and prompt techniques clustered into a tool like this with some attached guarantee of precision and tests against banks of challenging testing sets signed off by some external authority maybe similar in conception to PCIDSS, but then, I've sat through enough plenary meetings to know regulatory capture and selling out citizens rights is more likely.


Upvote
1

Downvote

Reply
reply

Award

Share
Share

u/Geksaedr avatar
Geksaedr
•
1y ago
What's a function composition in this context, can you please explain?



Upvote
9

Downvote

Reply
reply

Award

Share
Share

noprompt
•
1y ago
It means what it means in any other context: defining functions and calling them. This would be in contrast to designing an elaborate class hierarchy around what is effectively call to a string in/string out function of which you might compose with a compile prompt function (data in/string out), and a parse response function (string in/data out).



Upvote
7

Downvote

Reply
reply

Award

Share
Share

u/n4pst3r3r avatar
n4pst3r3r
•
1y ago
I am sorry for smartassing, but what you describe is procedural programming and not function composition.



Upvote
3

Downvote

Reply
reply

Award

Share
Share

noprompt
•
1y ago
Would it make you feel better if I did the f(g(x)) thing?



Upvote
2

Downvote

Reply
reply

Award

Share
Share

u/romaneremin avatar
romaneremin
•
1y ago
He'll feel better if you do
y = pipe(g, f)
z = y(x)


Upvote
3

Downvote

Reply
reply

Award

Share
Share

u/n4pst3r3r avatar
n4pst3r3r
•
1y ago
Yes, that's function composition in a nutshell. But you weren't talking about function composition in the first place. Not that this is bad, by all means, regular old procedural imperative programming is absolutely the right tool for most tasks. It was just a terminology thing.



Upvote
1

Downvote

Reply
reply

Award

Share
Share

noprompt
•
1y ago
Hmm, I’m trying to understand where you think I’ve made a terminology mistake but I’m having a hard time. I’m pretty sure I was talking about function composition and definitely not about imperative programming. The latter is about making statements and manipulating program state which I didn’t mention. Help me see what you see.



Upvote
1

Downvote

Reply
reply

Award

Share
Share

u/n4pst3r3r avatar
n4pst3r3r
•
1y ago
•
Edited 1y ago
I was referring to you saying "It [function composition] means what it means in any other context: defining functions and calling them." <- that's procedural programming.

Edit: I wrote declarative programming, wanted to write procedural programming


Upvote
0

Downvote

Reply
reply

Award

Share
Share

More replies
fiery_prometheus
•
1y ago
I think the framework has some good ideas, which you can learn from, but this has been my conclusion as well.

A lot of agent use cases can be solved by function composition, map reduce, etc.



Upvote
17

Downvote

Reply
reply

Award

Share
Share

Training_Designer_41
•
1y ago
•
Edited 1y ago
Yeah, the pattern is almost always expand/interpret - map - reduce - format , and best with composition all the way . We literally have a composition over inheritance principle I find people rarely utilize as much as they should



Upvote
6

Downvote

Reply
reply

Award

Share
Share

Suisse7
•
6mo ago
I'd be interested in developing a new framework that supports this flow but with some LLM specific tools such as LLM as a judge for your reduce operation or CoT parsing (use CoT to get an answer but only pass on the answer and not the explanation).


Upvote
1

Downvote

Reply
reply

Award

Share
Share

u/nostriluu avatar
nostriluu
•
1y ago
It's funny because map reduce was *the* hype ten-ish years ago because of ML workflows, but it's like it never existed.



Upvote
5

Downvote

Reply
reply

Award

Share
Share

u/koflerdavid avatar
koflerdavid
•
1y ago
It still is if your task consists of crunching though a lot of data and it can be broken into map-reduce steps. It's nothing special anymore and is treated as a given. Running transformers is just not something that maps well to these frameworks.



Upvote
3

Downvote

Reply
reply

Award

Share
Share

fiery_prometheus
•
1y ago
•
Edited 1y ago
I think it maps well, it's about problem decomposition, which given the limited context sizes of models, is something that is necessary. And given that most models exhibit the best problem-solving abilities when used over small subsets of problems, it works well. When I think about modeling problems, SICP (structure and interpretation of programs) comes to mind, as things are just building blocks and given the close relationship to AST I've noticed it seems a given that tree structure operations work well. I'm not saying that it is something you must use all the time, but from my experience it works reasonably well.

EDIT: And from SICP and from years of experience, also thinking about abstraction layers and where to limit the models understanding from the other layers of abstraction, just like humans would, since we cannot think about everything at once, and must use well made abstractions to cognitively even be able to build things.


Upvote
2

Downvote

Reply
reply

Award

Share
Share

u/productboy avatar
productboy
•
1y ago
Trust you on this. However, there’s very little information or developer education on this framework-less approach. Someone will be as hot as Taylor Swift if they create a YT channel with this content.



Upvote
6

Downvote

Reply
reply

Award

Share
Share

Training_Designer_41
•
1y ago
My guess is that developers love frameworks, with the promise it’ll do all the work, so the demand… I do frameworks but often end up getting rid of them for most use-cases


Upvote
2

Downvote

Reply
reply

Award

Share
Share

u/grudev avatar
grudev
•
1y ago
I concur. 


Upvote
2

Downvote

Reply
reply

Award

Share
Share

Easy_Deal7178
•
10mo ago
Exactly right. Langchain for example was a great idea, but become the worst thing for creativity.


Upvote
2

Downvote

Reply
reply

Award

Share
Share

filter_ice
•
5mo ago
Its not just about doing it yourself. When you see in different angles to solve one problem you get much better understanding out of it.
Then maybe you can use your own solutions. Use libs if it makes your life easier.



Upvote
2

Downvote

Reply
reply

Award

Share
Share

noprompt
•
4mo ago
Nothing against using libraries if they add value. Its just that I don't think any of the libraries in the Lang* ecosystem are doing that. I agree in the value of seeing "different angles", but haven't seen them in the Lang* libraries. Further, doing it yourself _is_ about seeing those different angles.

Something to note here: the problems the Lang* ecosystem claim to address with respect to LLMs are still very new and poorly understood. It is highly likely the abstractions on offer are premature. Until someone produces a theory or has evidence for how systems which rely heavily on LLMs should be designed, I think its wise to remain skeptical.


Upvote
2

Downvote

Reply
reply

Award

Share
Share

u/JacketHistorical2321 avatar
JacketHistorical2321
•
1y ago
Can you point out a few things you found that were specifically counterproductive when you tried working with dspy?


Upvote
2

Downvote

Reply
reply

Award

Share
Share

ChanceCod1029
•
9mo ago
Good comment today found interesting stuff called RunLLM.. thinking do you have some idea how they get so good answers ? Im trying classical RAG but that’s far from this product 😬


Upvote
1

Downvote

Reply
reply

Award

Share
Share

u/AutomataManifold avatar
AutomataManifold
•
1y ago
I've been using DSPy for a bunch of things now. I find it pretty straightforward to use, especially once you understand how it composes prompts. It's very effective for composing complex chains of operations, but it's also handy for just doing simple prompts. 

It's a little better at using foundation models, since you sometimes have to finesse it a bit for some instruction formats. I've had more luck with Mistral than with Llama 3's format, so far. It is a bit optionated about the prompt format,  though they're making changes to the backend to give you more control over that.

If you're using it with a local model, you should try to use vLLM or Aphrodite or one of the other inference backends that does batching, because it's a lot easier when you're generating thousands of tokens per second. 



Upvote
12

Downvote

Reply
reply

Award

Share
Share

dimsumham
•
1y ago
How does it compose prompts? This is the biggest stumbling block for me trying to understand this as a newb.

Say you have a complex extraction task you're trying to set up. Do you provide it w the initial prompt? How does it get mutated?



Upvote
4

Downvote

Reply
reply

Award

Share
Share

u/AutomataManifold avatar
AutomataManifold
•
1y ago
It's a little easier if you have something that lets you see the final prompt (like a local inference server with verbose prompt reporting).

The key is the dspy.Signature classes. Here's an example from the docs:

class Emotion(dspy.Signature):
    """Classify emotion among sadness, joy, love, anger, fear, surprise."""
    
    sentence = dspy.InputField()
    sentiment = dspy.OutputField()

sentence = "i started feeling a little vulnerable when the giant spotlight started blinding me"  # from dair-ai/emotion

classify = dspy.Predict(Emotion)
classify(sentence=sentence)
The doc string is the basic prompt, and the input and output fields can take desc="your description here" arguments if you need to manually clarify.

The Signature class you write gets turned into a prompt with the docstring as the instructions and the InputFields and OutputFields as part of the formatting. The thing that is most surprising is that the variable names get turned into part of the prompt, so getting their names right is important. Then you give the Signature to a Module like dspy.Predict or dspy.ChainOfThought or whatever and you've got a function that you can call to get the LLM result.

There's some shortcuts you can do, like just using a string to define the entire Signature like this: 'sentence -> sentiment' but that's just a bonus.

If you want to do something fancier, you can write your own Modules and do extra stuff like using Suggest/Assert to enforce properties of the result. Or even just re-write it with ordinary Python.

The default prompts it writes work a little bit better with a foundation model rather than an instruct model, because most instruct models aren't trained on that exact format. But it can work with an instruct model.

Technically you can edit the exact formatting, but that's a bit advanced right now until they finish adding support for it. I set up a custom LLM API interface that intercepts the prompts. It's not necessary for Mistral, but it lets me put the final bit of the prompt in the assistant part rather than the user part of the instructions, which improves performance on some instruct formats. Once they have a little bit better support for that it'll make it much easier to make it conform to whatever custom instruct format you have, but the existing behavior is good enough that I don't need to worry too much about forcing JSON output or whatever.

The optimization is another step beyond that. There's several Optimizers; the basic ones just try different combinations of examples for a few-shot prompt, the more advanced ones use the LLM to rewrite the prompt text itself until you get the performance you want. In either case, it depends on having a dataset of good examples. The dataset can be small, particularly because you can use DSPy to generate more examples and bootstrap your way up bigger datasets. The key requirement is that you need some kind of metric to judge your output by. Which can be another, bigger LLM rating the results, letting you use a big model to train a smaller one.

You can keep going and edit the model weights, but just optimizing the prompt is often enough. Even without the Optimizers, you can get a long way with just Signatures and Modules. It's really useful to be able to look at which prompts are giving you issues and change them to ChainOfThought or whatever.



Upvote
32

Downvote

Reply
reply

Award

Share
Share

u/stegd avatar
stegd
•
1y ago
lol, this is better explanation than I have found in their documentation. thanks!


Upvote
6

Downvote

Reply
reply

Award

Share
Share

dimsumham
•
1y ago
This is EXTREMELY helpful. Thank you so much!


Upvote
4

Downvote

Reply
reply

Award

Share
Share

u/rohitspujari avatar
rohitspujari
•
10mo ago
Great explanation! How does it call APIs? Say you are doing a chain-of-thought and want to call an external API to get current weather as one of the intermediate steps. Do you have to write a separate module?



Upvote
1

Downvote

Reply
reply

Award

Share
Share

u/AutomataManifold avatar
AutomataManifold
•
10mo ago
You can include whatever arbitrary Python code you want. You can get fancy with it and use their retriever class, or just write whatever. 


Upvote
2

Downvote

Reply
reply

Award

Share
Share

Sunija_Dev
•
1y ago
Can you post example outputs? :3

As in

the task

your prompts/the adapted prompts from dspy

output with/without dspy (for comparison)



Upvote
2

Downvote

Reply
reply

Award

Share
Share

u/AutomataManifold avatar
AutomataManifold
•
1y ago
I can see what I can dig up; I didn't save the final verbose prompt outputs, since they aren't the level you're usually interacting with DSPy, because they're constructed dynamically.

If you want to see what the raw prompts actually look like, I recommend running it yourself and checking the verbose input to the inference engine. Or you can use inspect_history().

There's a pretty thorough explanation in the docs of what the core Signature functionality is doing.

They look something approximately like this:

Given the fields `input`, produce the fields `output`.

---

Follow the following format.

Input: ${input}
Output: ${output}

---

Input: Data example from dataset.
Output: Output example from dataset

Input: Current input data.
Output:
...but they're adding the ability to have more control over the backend generator. So don't get too hung up on the exact format just yet. I've got a hacked version that intercepts the call and changes it for Mistral, but that will soon be unnecessary.

The Optimizers and Modules change these in several ways. For example, you can custom write a Module to do additional processing, like a RAG lookup, or Chain-of-thought prompting, or a ReAct agent pipeline, or an Assert that evaluates the output.

The basic Optimizers just do multi-shot examples (running tests to figure out which examples get you the best results). That is often enough to significantly improve performance; if you frequently have trouble with your LLM adding stuff like "Here's the JSON you requested" or other formatting errors it'll solve that pretty quickly. I don't even ask for JSON most of the time, because it'll generate stuff that can be translated into JSON pretty easily, so I just let it generate field by field and do the JSON-ification myself.

The advanced Optimizers like MIPRO do more complicated training that involves generating new prompt versions based on your data.

A handy thing, if you're using a weaker model, is that you can write a Module to go field by field, verifying the results. This gives you more calls but gives you extra validation. You can also use Pydantic Types or custom-written per-field evaluation.

Ideally someone will implement the ability to combine DSPy with Outlines and we'll also have token-inference level enforcement, but that's not implemented yet; the core DSPy functionality is designed to work even if you don't have direct control of the LLM. If you do have direct control, you can do things like finetuning a small model on your specific task (so you can have a 770M model that outperforms a 13B model because it's custom-tuned for your exact problem).

All of this is just scratching the surface of what you can do with it once you start combining these different elements.



Upvote
7

Downvote

Reply
reply

Award

Share
Share

Sunija_Dev
•
1y ago
To be fair, answers like those are why I don't trust DSPy at all. :D

Every time I ask for actual examples, somebody explains a lengthy explanation of the concept, asks that I should just run it myself and sends 2000 links.

If there isn't one proper example, I'll just assume it doesn't work properly.

(Sorry if you're unrelated to the project and just tried to be helpful. :X)



Upvote
7

Downvote

Reply
reply

Award

Share
Share

u/AutomataManifold avatar
AutomataManifold
•
1y ago
I don't have anything to do with the project, I just use it sometimes.

I'm not quite sure what you're looking for. I don't use it to improve specific prompts, I use it to write the whole pipeline. The big benefit for me is that I no longer care about Guidance or JSON when I'm using DSPy. I just get the data I need in the format I need it in. So the killer feature for me is that I never ever have to care about dealing with "Here's the JSON you requested:" getting randomly prepended to my responses.



Upvote
8

Downvote

Reply
reply

Award

Share
Share

bikesniff
•
4mo ago
can you give an example of the output when you let it 'generate field by field'? ......like a list of key value pairs? or something close to yaml?

I guess im mostly interested in whether the output can be coerced to JSON without an additional call to another LLM or how reliably it can be done



Upvote
1

Downvote

Reply
reply

Award

Share
Share

u/AutomataManifold avatar
AutomataManifold
•
4mo ago
They've updated the format, so I don't have a good example at hand. However the DSPy documentation does: https://dspy.ai/tutorials/observability/

There's an inspect_history() function to view the exact calls in your own code.

That said, these days if you want structured generation, I'd say you should use Instructor or Outlines instead.



Upvote
1

Downvote

Reply
reply

Award

Share
Share

bikesniff
•
4mo ago
•
Edited 4mo ago
thanks for the insight

update: the linked page is really interesting in general


Upvote
1

Downvote

Reply
reply

Award

Share
Share

Sunija_Dev
•
1y ago
"Does DSSPy work?"

...was the first question I had. They have 0 actual examples online (task + adapted prompts + output with dspy + output without dspy for comparison). They also couldn't provide me some after asking for them three times on their Discord.

So my conclusion was, either A) Nice idea, doesn't work. B) Gives only a ~5% improvement, so you couldn't even notice it.

As much as I love the idea, everything that kinda works like autogpt seems to perform so bad, that it's not worth setting up the framework (atm).

Also, I'd love to be proven wrong by somebody posting dspy examples (as explained above).



Upvote
5

Downvote

Reply
reply

Award

Share
Share

AlphaRue
•
5mo ago
Stanford STORM uses dspy


Upvote
2

Downvote

Reply
reply

Award

Share
Share

u/afterprime_official avatar
afterprime_official
•
Promoted

Afterprime - Ranked #1 globally for lowest execution cost by ForexBenchmark
Learn More
afterprime.com
Thumbnail image: Afterprime - Ranked #1 globally for lowest execution cost by ForexBenchmark
reddit_reddit_01
•
1y ago
I have tried it for multi class multi label text classification. 

Seems to work. Definitely not mature enough though. 

I recently checked functions by OpenAI itselfm seems to be sufficient enough for my usecase. 

Only thing I was happy with was I could modularise my code better using DSPy.

And major concern for me is I don't get to know how many hits are being made during optimisation etc. And while optimisation is big one USP, I'm not able to utilise it. 


Upvote
5

Downvote

Reply
reply

Award

Share
Share

[deleted]
•
1y ago
I tried it with a local model but couldn’t get it to work. It seems like the right direction for a lot of applications. I’m completely on board with their idea, but I might wait for it to mature a bit before I try it again.



Upvote
4

Downvote

Reply
reply

Award

Share
Share

purple_sack_lunch
OP
•
1y ago
Seems like we are in the same position. Thx for your thoughts


Upvote
4

Downvote

Reply
reply

Award

Share
Share

u/samme013 avatar
samme013
•
1y ago
Main advantage is the optimizer aspect which requires some kind of dataset to evaluate against. If you t hink that would make sense for your usecase I would consider it otherwise would just stick to no framework + something for structured output like instructor.



Upvote
4

Downvote

Reply
reply

Award

Share
Share


[deleted]
•
1y ago
u/Fluffy-Play1251 avatar
Fluffy-Play1251
•
1y ago
I use dspy. I like the speed at which i can define inputs and outputs. My use case involves using a lot of llm calls to build up a response, and dspy gives me consistent results for not a lot of work.

If i were more invested i might roll mu own, but so far i like the ability to define the inputs and outputs to quickly structure llm systems.


Upvote
4

Downvote

Reply
reply

Award

Share
Share

u/nishan_e_hind avatar
nishan_e_hind
•
1y ago
shaky API, difficult to debug and after all the effort the prompts are not generalizable beyond the training/bootstrapped samples.

better to write out a ReAct pipeline by hand and find optimal prompts.


Upvote
4

Downvote

Reply
reply

Award

Share
Share

u/Swimming_Dig_3616 avatar
Swimming_Dig_3616
•
10mo ago
I have been using it for a week. My initial impressions were positive, and the concepts of signatures and modules are intriguing. However, after using it for a while, I am disappointed.

I am not sure if I used it correctly, but the generated (trained) prompt simply adds some examples. Firstly, this makes the prompt very long, and secondly, the selected examples do not cover all scenarios. It does not utilize the power of LLMs to abstract concepts. For example, in my case, I need the LLM to identify evidence for a certain medical disease. The LLM does not perform well with just a few examples; we actually need a prompt that can summarize the rules from all the examples and use these rules in the prompt.

For models less powerful than GPT-4, the quality is very poor. I wanted to use it to tune a good prompt for a cheaper model, but it failed. (GPT-4 can perform well with a simple handcrafted prompt.)

For me, a handcrafted prompt is much simpler to create and allows me to understand the deficiencies of the LLM. The ideas of chaining and multi-agent systems can also be implemented without being hindered by overly complex peripheral code.

I will stop experimenting with DSPy until it is proven to work.


Upvote
4

Downvote

Reply
reply

Award

Share
Share

maylad31
•
1y ago
I think as a concept it is good. But I guess the framework needs to be improved. I tried their signature optimizer, it works but it is not easy to tweak their prompts, i see people having issues if prompts are in a different language. Here is the code if it helps anyone get started: https://github.com/maylad31/dspy-phi3


Upvote
3

Downvote

Reply
reply

Award

Share
Share

u/LatestLurkingHandle avatar
LatestLurkingHandle
•
1y ago
This video provides a good overview https://youtu.be/6rN9ozzdT3A?feature=shared


Upvote
2

Downvote

Reply
reply

Award

Share
Share

u/LiYin2010 avatar
LiYin2010
•
9mo ago
Try AdalFlow: The “PyTorch” library to auto-prompt any LLM tasks.

It has the strongest architecture, as well as the best optimization.

https://github.com/SylphAI-Inc/AdalFlow

Comment Image

Upvote
2

Downvote

Reply
reply

Award

Share
Share

u/NoTranslator4364 avatar
NoTranslator4364
•
9mo ago
My firsst impression of it bad. Feels too formalized than practical. Soo much code to do a simpliest thing

also why fuck are the signatures just strings. That looks like a headache in the future.


Upvote
2

Downvote

Reply
reply

Award

Share
Share

u/Valuable_Can6223 avatar
Valuable_Can6223
•
9mo ago
I keep hearing this is better but from what I have seen I think it’s just better if you are a programmer. -



Upvote
2

Downvote

Reply
reply

Award

Share
Share

u/Previous_Ladder9278 avatar
Previous_Ladder9278
•
5mo ago
or use a low-code version of dspy: https://github.com/langwatch/langwatch


Upvote
2

Downvote

Reply
reply

Award

Share
Share

Basic-Pay-9535
•
1y ago
How do you do so without frameworks ?


Upvote
1

Downvote

Reply
reply

Award

Share
Share

u/timeisether avatar
timeisether
•
1y ago
What for is Dspy Program of Thought?



Upvote
1

Downvote

Reply
reply

Award

Share
Share

Muted_Estate890
•
1y ago
dspy.programOfThought is for getting an llm to write code or do math (using code) as per the docs and the helper functions that the module includes. It allows you to apply multiple iterations on an llm writing code so that the llm can improve its own code. Also it checks for errors in between iterations and passes in the error to the next iteration so the llm can resolve it. Its a cool concept!


Upvote
2

Downvote

Reply
reply

Award

Share
Share

konilse
•
9mo ago
the idea is very nice, but I don't think the added value of the framework is really that great in most of the use cases


Upvote
1

Downvote

Reply
reply

Award

Share
Share

GaggedTomato
•
9mo ago
The idea on paper is fantastic, as I spend hours prompt engineering to get all hallucinations out of my local SLM. But the current design has made it hard for me to implement some basic features I need for a production environment, like streaming and stopping a prompt on stopwords (which I did implement already without dspy). Trying to inherit from LM class and make my custom, seems to lead to all kind of unexpected errors in the sourcecode.

I hope it will get a bit more mature, like some have said here.


Upvote
1

Downvote

Reply
reply

Award

Share
Share

u/Doubleve75 avatar
Doubleve75
•
8mo ago
Dspy has been amazing, i know my company Giggso uses it extensively for most functions in conjunction with langgraph . I have not seen it do wonders with programofthought but chain is pretty consistent in results.


Upvote
1

Downvote

Reply
reply

Award

Share
Share

Acrobatic-Aerie-4468
•
4mo ago
DSPy is a "Prompt Optimisation and Automation Framework", not "Another Prompt Engineering Framework".

Community of developers have decided to make the lives of other developers who want to better interact with LLMs, and have written the framework that automates many of the prompting related work, in a reliable and predictable fashion.

I have extensively worked on DSPy, and supported many real life projects that involve DSPy. You can see the lectures here on Youtube for your reference.
https://www.youtube.com/playlist?list=PLbzjzOKeYPCqoCjk_rTuZA1Qobq5_D_hX


Upvote
1

Downvote

Reply
reply

Award

Share
Share

FunJumpy9129
•
24d ago
I just watched a video about dspy, it seems the most useful part is the Bayesian optimization, but I am still skeptical what is the space we can squeeze out of simply instruction or demonstrations.


Upvote
1

Downvote

Reply
reply

Award

Share
Share

u/RepulsiveDepartment8 avatar
RepulsiveDepartment8
•
1y ago
Do not waste your time on it. I tried. very bad. compile, what a joke. just some fancy stuff waste your time, do your own in few lines of code. loop with try catch. evaluate, a few prompt, will be much better. They even delete my critize github discussion. Dont know why so many people talk about it. maybe people like talk something fancy, no mater he understand it or not.


Upvote
1

Downvote

Reply
reply

Award

Share
Share

maddogxsk
•
1y ago
Llama 3.1
From all frameworks, Langchain is the most useful I got to use, for both own personal hobby projects and soon-to-launch into production from my job (I work as a solutions architect in a pretty large national bank from latAm)

Some people find it complicated and all, but the deal is it is a framework with a pretty large or huge toolset, it aims to be useful at any level and you can do both prompting as the classic old way mode, or get to know LCEL (which is what many find complicated), etc.

It is pretty hard to find a framework that handles every aspect of the technology at the level it does and the integrations does everything pretty easy, enough to set a .env and let the bound env vars defined in documentation for every integration do the hard job


Upvote
0

Downvote

Reply
reply

Award

Share
Share

Community Info Section
r/LocalLLaMA
Join
LocalLlama
Subreddit to discuss about Llama, the large language model created by Meta AI.
Created Mar 10, 2023
Public
474K
Members
753
 Online
Top 1%
Rank by size 
User flair
u/obscuramusis avatar
obscuramusis
Rules
1
Please search before asking
2
Off-Topic Posts
3
Low Effort Posts
4
Limit Self-Promotion
5
Follow Reddit's Content Policy
Moderators
Message Mods
u/Civil_Collection7267 avatar
u/Civil_Collection7267 
Alpaca
u/AutoModerator avatar
u/AutoModerator
u/PhilosophySmooth3389 avatar
u/PhilosophySmooth3389
View all moderators
Reddit Rules
Privacy Policy
User Agreement
Reddit, Inc. © 2025. All rights reserved.

Collapse Navigation
